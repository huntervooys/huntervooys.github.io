[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hunterr_2024",
    "section": "",
    "text": "Preface\nListing¬†1: greeting\n\n\nprint(\"Good Morning! ü§ó\")\n\n\n\n\n[1] \"Good Morning! ü§ó\"",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#this-book",
    "href": "index.html#this-book",
    "title": "hunterr_2024",
    "section": "This book",
    "text": "This book\nThis document you are reading now is called a Quarto book. We will talk more about Quarto later, but for now, all you need to know about Quarto books is that they:\n\nare relatively simple to construct,\ncan contain R code snippets and their outputs,\nare pretty enough for me to be temporarily satisfied.\n\n\nFeatures of This Book\n\n\n\n\n\n\ncallouts\n\n\n\n\n\nThis book let‚Äôs me put many of my least relevant tangents in these collapsible little notes.\n\n\n\nThe features of this book, which are achieved easily through Quarto, explain its format. Code snippets appear in this book, but only if you want them to. Each code snippet is collapsible by clicking the triangle next to ‚Äúshow code for this result‚Äù. If you really hate code, you can click the ‚Äú&lt;/&gt; Code‚Äù button on the top of each page and hide (or show) all the code at once. Speaking of the ‚Äú&lt;/&gt; Code‚Äù button, if you click ‚ÄúView Source‚Äù you can see all of the code that was used to create the page.\nQuarto also lets me show you ‚Äúpaged tables.‚Äù In the following code, I generate 1000 random numbers between -10 and 10, which I label x. Then, I apply the mystery_function to each number, producing y. What does mystery_function do?\n\n\nshow code for this result\ntable_1 &lt;- tibble(\n  x = runif(n = 1000, min = -10, max = 10),\n  y = mystery_function(x)\n)\ntable_1\n\n\n\n  \n\n\n\nQuarto also let‚Äôs me show you plots, like the one below. What do you think mystery_function does now?\n\n\nshow code for this result\nqplot(x = table_1$x, y = table_1$y) +\n  labs(x = \"x\", y = \"mystery_function(x)\")\n\n\n\n\n\n\n\n\n\nIn reality, mystery_function just squares x, so:\n\\[\n\\mathtt{mystery\\_function}(x) = x^2\n\\tag{1}\\]\nAll of the chunks of code have line numbers. For any chunk, a little clipboard appears when you hover over the code listing, and you can copy it by clicking on the clipboard. (This includes the code in the ‚ÄúView Source‚Äù pane, meaning you can copy the entire page, text and all.)\nQuarto also processes citations, so I can, for example, easily direct you towards my two fathers: Turing (1936) and Foucault (1978). It will also process internal links so that I can, for example, send you back to the top of the page: Listing¬†1.\nFinally, Quarto allows me to annotate code, which is helpful to explain how it works when a high-level of technical detail is needed. For an example of those annotations and when they might be helpful, look to Figure¬†2.1.\n\n\nTools\nR is an free and open-source statistical programming language.You use R by typing commands into an R console, which looks like this:\n\n\n\n\n\n\n\nRStudio\n\n\n\n\n\nRStudio is a graphical user interface (GUI) and interactive development environment (IDE) with which to use R. This means that RStudio contains an R console (lower left) and a variety of other tools (right), like a text editor (upper left). RStudio looks like this:\n\n\n\n\n\n\n\n\n\n\nTidyverse\n\n\n\n\n\nTidyverse is a collection of R packages that are designed to work well together and to acomplish data analysis tasks. The Tidyverse packages were developed by Hadley Wickham primarily, but also by teams of his collaborators.\nBecause Tidyverse is set of R pacakges, there is not picture to provide. Tidyverse exists as R functions that you can (and will) use in your code.\n\n\n\n\n\n\n\n\n\ngrammar of graphics\n\n\n\n\n\nThe grammar of graphics refers to another set of R packages, exemplified by ggplot2, which is included in the tidyverse. The grammar of graphics is the tool that most R programmers use to produce the ‚Äúproduction-quality graphics‚Äù for which R is known. Once you begin to produce plots with ggplot2, you are likely recognize that you have been seeing these plots for years.\n\n\n\nI believe that these tools will allow you to accomplish the vast majority of data analysis tasks, and that knowing how to use them will result in you understanding data analysis on a deeper level and being able to do it faster. That being said, these are not the only tools for data analysis. I encourage you to explore others, as well; particularly, if you intend to do a lot of work with text data, I would suggest python.\nBecause my audience does not intend to become ‚Äúprogrammers,‚Äù per se, I would also like to introduce you to the following tools, which allow R code to be integrated into readable documents instead of writing R code in separate text files with R code only (called scripts). R scripts have a place, to be clear, especially while you are learning. However, if you would like to share the analysis that you do with an R script, sharing that R script is not a good way to do that. When you send someone an R script, you are sending them a bunch of code, not an analysis\nThe current gold standard for the sort of work you are most likely to want to produce (reproducible research) is this - the document you are reading now. This is a document which includes code, the output of that code, and text explaining that code and providing context. This approach is called literate programming. The tools used to produce literate programming documents with R are:\n\nRMarkdown: a document preparation software based on R and a markup language called markdown that is mainly used to make static documents (like appendices to a journal article). RMarkdown uses a software called pandoc to turn the .rmd file into: a pdf (latex or a beamer presentation), a .html website, a word document, a power-point, and a lot of other formats you‚Äôre likely to never use.\nQuarto: a very similar, but more advanced and comprehensive software than RMarkdown. Most of the things you can do in Quarto are also possible in RMarkdown, like adding cross references (like this Equation¬†1), creating books (also like this), creating interactive data dashboards (which seems particularly trendy as of late), and creating blogs and websites. Quarto is made by the same company that makes RStudio.\nShiny: a software (written in R) that allows you to create interactive plots, which may be helpful if you are, say, trying to decide the optimal number of bins in a histogram. You could use shiny to create a histogram and a bins slider, so that you could easily see a variety of different bin sizes merely by moving a slider (instead of by writing, modifying, and rewriting code to achieve the same end).\n\nMy goal for you is to write code that is readable and to put that code inside documents that are actively fun and/or interesting to read. (That‚Äôs also, incidentally, my goal for me.)\nWe are also going to do statistics! There are two approaches to statistics that we‚Äôre going to adopt through these lessons, so I‚Äôd like to begin by elucidating the way in which these approaches are different. I am more familiar with the first approach (exploratory data analysis). Confirmatory data analysis uses many of the same tools (like hypothesis testing, which I will show you), but it uses them in a different and moer complicated way. The ultimate goal of both approaches is to predict the result of measurements.\n\nexploratory data analysis: If the purpose of a statistical model is to predict data, then a model that makes the most accurate prediction is the best model. The model creation process is iterative. Once you see the results of a model, you can use the results to modify the model itself. Generally, practitioners recognize that there are a variety of different types of models that could be used for any task. Thus, they usually construct a variety of different models and then compare them to select a final model. Practitioners will use numbers (in diagnostic and statistic tables) or visuals (like a residuals plot) when comparing models.\nconfirmatory data analysis: In the best case, at least according to the ‚ÄúOpen science‚Äù framework, the final statistical model will have been selected and preregistered before data is even collected. Statistics have to be rigorous to mean anything, a fact which the machine learning people (who do only exploratory work) ignore. They don‚Äôt check model assumptions using statistical tests like the Shapiro-Wilk normality test, and they are constantly ‚Äúp-hacking‚Äù and ‚ÄúHARKing‚Äù to forcibly extract findings from their data. Confirmatory data analysis rejects these practices, aiming instead for statistical models that are pre-specified (pre-registered), theoretically-based, and rigorous (whatever they take that to mean).\n\nI‚Äôm often somewhat flippant about the second approach, which suggest and attempts to discover Truth where I am skeptical it exists. In any case, the second approach is the only one that is taught in statistics courses. This is a mistake. Firstly, exploratory data analysis is much more commond. Secondly, it is easier to get started doing exploratory (rather than confirmatory) data analysis. Because both of these approaches adopt many of the same tools, it seems to me that starting with exploratory data analysis and trying to make that make sense is the most effective way to learn confirmatory data analysis (which will require additional effort and research that I can‚Äôt provide).\nThus, I intend to provide you with a strong understanding of data that you can directly apply to exploratory data analysis tasks. My hope is that this understanding will enable you to complete a diversity of tasks, including confirmatory data analysis, if that is of interest.\n\n\nLearning Objectives\nThere has to be some boring stuff because pedagogy. I have quite a few learning objectives for you, forming one big list, but I‚Äôll attempt to section them off so they are easier to read.\n\nR Learning Objectives\n\ndiscuss R as a language with a history: use knowledge about the history of R (and of scientific computing more generally) to describe what R is, what people ‚Äúsay‚Äù in this language, and why this language has the properties and characteristics that it does.\nR competence: read R expressions written by others (allowing the language to serve a communicative purpose), and write R expressions that are readable and align with best practices within the open source R community.\nR‚Äôs friends: Describe R‚Äôs relationship to RStudio, RMarkdown, Shiny, Quarto, and Tidyverse; and, describe what each of these tools is and why someone would use them.\n\nrun R code in several different ways: via the console, a script, and Quarto or RMarkdown documents.\n\ndescribe R‚Äôs data types and the use of each: strings, numerics (floating point ‚Äúdoubles‚Äù, integers, and complex numbers), logicals, datetimes, and factors\ndescribe R‚Äôs data structures and the use of each: including, vectors (1-dimensional arrays), matrices (2-dimensional arrays), arrays (more than 2-dimensional arrays), lists (key-value pairs), data frames, and tibbles\naccess R documentation, and read it effectively enough to solve a problem\n\n\n\nComputation Learning Objectives\n\ngenerate synthetic data: use simulation of simple events (like the rolling of dice or flipping of a coin) to gain visual intuition for the central limit theorem and the law of large numbers\nuse the Monte Carlo simulation framework to evaluate statistical tests (e.g., by determining what happens when assumptions are violated)\nprocess string data: convert strings to all upper or lower case, add prefixes or suffixes, splitting strings apart\nprocess numeric data: scale and center numeric data and write functions to accomplish non-standard transformations\nprocess language data: apply the principles of natural language processing to pre-process text data (by tokenizing and stemming text and describing both of those processes and why they are used)\n\n\n\nStatistics Learning Objectives\n\nnull hypothesis significance testing: use R to perform null-hypothesis significance tests, such as the one-sample, two-sample, and repeated measures t-test\nregression: use R to perform linear and logistic regressions, including regressions with polynomial terms\nmachine learning: use R to perform a more complicated machine learning task, likely by constructing a decision tree and a random forest classification model\ndimensionality reduction: perform principal component analysis and construct a latent semantic space, and explain why these two seemingly distinct methods are connected by singular value decomposition\n\n\n\nData Science Learning Objectives\n\nrectangular data: use the tidy data framework to read, write, and pre-process rectangular data in a consistent, efficient, and minimally complex manner.\n\nimport data from a variety of sources including: comma-separated values (.csv) files, excel spreadshees (.xlsx files), google sheets spreadsheets\n‚Äòtidy‚Äô data into the following format: one observation per row, one variable per column, one value per cell\nuse available tools that enable you to store data in a very consistent format with very little effort\n\nlanguage data: use R to pre-process, analyze and visualize text data\nvisualization: use R and the grammar of graphics (represented by ggplot2 and related packages) to visualize data and to share data visualizations with others\npublication: use RMarkdown or Quarto to conduct a linear or logistic regression, and to report and interpret the results of those tests\n\n\n\n\n\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem.‚Äù Journal of Math 58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_lore.html",
    "href": "01_lore.html",
    "title": "1¬† The Lore",
    "section": "",
    "text": "1.1 Early 19th Century: the birth of programming\nThe idea of a programmable computer is not difficult to understand. The primary goal is to make a machine that you can use for different tasks, depending on what program you feed to that machine. Programs are written in code, which today is stored as text files on a computer. The programmable machine also lives within the computer, so running the program is as simple as telling the machine part of the computer where the program is located; then, the machine part of the computer tries to read find the and read the program at the specified location, and the (hopefully) program runs.\nFolks in the 19th century did not have access to digital text files, and so they could not write their programs on them. How did they write programs, and which sort of programs did they write? These are the primary questions I hope to address in this very first section?",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#early-19th-century-the-birth-of-programming",
    "href": "01_lore.html#early-19th-century-the-birth-of-programming",
    "title": "1¬† The Lore",
    "section": "",
    "text": "1.1.1 Jacquard‚Äôs Loom: the punched card\nOne of the first programmable machines was Jacquard‚Äôs loom, which Jacquard patented in 1804. Jacquard was a weaver. Jacquard‚Äôs loom was a machine that could create a variety of different patterns, depending on which program was put into it?\nSo, Jacquard wrote programs to produce beautiful woven fabrics, but more important is how he wrote programs. Jacquard had no text files, so instead he developed a different form of machine input: the punched card. Each line on Jacquard‚Äôs punched cards contained information about a single row of the design. The cards could be fed sequentially into the loom to produce a large pattern.\n\n\n1.1.2 Babbage, Lovelace, and the Analytical Engine\nGeneral-purpose digital computers, the sort of computers that can run R, emerged as an idea in the early-to-mid 19th century. Up to that point, computers were either mechanical (mechanical computers are fascinating, by the way) or just humans.\nOne of the first to develop a design for a general-purpose computer was Charles Babbage, working in the early part of the 19th century. In the 1830‚Äôs he proposed a massively complicated, general-purpose, steam-powered computer, which he called the analytical engine. The computer was only capable of carrying out the four basic operations of arithmetic: addition, subtraction, multiplication, and division; it was designed to take input via punched card, just like Jacquard‚Äôs loom.\n\n\n\n\n\n\nLady Lovelace\n\n\n\n\n\nDuring the 1830‚Äôs and 1840‚Äôs, Lady Ada Lovelace communicated with Charles Babbage (and several others involved in similar work) with the intention to collaborate with him in studying the analytical engine. It was Lady Lovelace who wrote the first substantial computer program, whose purpose was to compute Fibonacci numbers (Tibees 2020). Her program, written in the iconic note G, used only the four simple arithmetic operations.\nLovelace was interested in discovering the capabilities of the analytical engine. Her program computing Fibonacci numbers was important because it used loops in computation. Lovelace, daughter of the poet Lord Byron, was also interested in non-mathematical applications for the machine. She suggested that a sufficiently mathematical theory of sound could enable to engine to compose complex and scientific symphonies (Tibees 2020). Isn‚Äôt that beautiful!",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#middle-and-late-19th-century",
    "href": "01_lore.html#middle-and-late-19th-century",
    "title": "1¬† The Lore",
    "section": "1.2 Middle and Late 19th century",
    "text": "1.2 Middle and Late 19th century\n\n1.2.1 1830-1870(ish)\nThe middle of the 19th century was a period of massive global shifts. Liberation from enslavement was spreading across the globe after the Haitian revolution left white men horrified at the peculiar institution, which was also losing economic utility (see Capitalism and Slavery by Eric Williams).\nAlso within that 40 year period was:\n\nabolition (as a matter of law, anyways): British Empire (1834), French Empire (1848), Russian Empire (1861), Dutch Empire (and Dutch East India Company; 1863-73), American Empire (1865), Portuguese Empire (1869)\nSamuel Colt‚Äôs invention of a revolver that can be mass-produced (1836?)\nthe development of the telegraph (1830s)\nthe trail of tears (starting 1836)\nthe revolutions of 1848 and the publication of the communist manifesto\nthe first woman‚Äôs rights convention in the U.S. (Seneca Falls Convention, 1848)\nthe discovery of the Bessemer Process which enables the mass-production of steel, paving the way for emerging steel tycoons (1855)\nDarwin published On the Origin of Species (1859)\nGatling‚Äôs invention of the machine gun (1861)\nMaxwell publishes his equations, proposing an incredibly successful theory of physics that understands electricity, magnetism, and light as essentially the same thing (1861)\nthe construction and openning of the Suez Canal (1860‚Äôs)\nMendel‚Äôs publication of his laws of genetic inheritance (1865)\nthe discovery of the cell and subsequent elaboration of cell theory (1865 and after)\nNobel‚Äôs invention of dynamite (1867)\nMarx‚Äô publication of the first volume of capital (1867)\nthe completion of the transcontinental railroad (U.S., 1869)\nMendeleev‚Äôs publication of the first periodic table (1869)\n\nIn this revisionist history of the computer (and ultimately of R), this period in history marked a transformation of power. The structure and organization of society was changing along with the flow of people, ideas, and commerce. Western, liberal democracies had to develop new technologies of population control in order to prevent all of these liberal changes from challenging their position of authority and power.\n\n\n1.2.2 Late 19th century\nWith the relative liberation of black bodies (and other bodies, as well) came a scientific imperative. Power continued to demand that these bodies be inferior, but evidence of inferiority was no longer to come from the conditions and dimensions of the body. Nay, the newly-available technologies of genetic inheritance and natural selection allowed a regime of a new flavor to take hold, one that cited hard science to support and justify the inequities in society. Inferiority was moving through the skin, into the body, and - importantly - into the mind.\nWilhelm Wundt opened the first psychology lab, and William James delivered the first psychology course and textbook. Galton, who was studying intelligence, popularized the idea of the median (Bakker and Gravemeijer 2006). Psychology and with it psychological statistics, was beginning to take shape to meet the new demands of the state: a theory and a technology that will find permanent, internal traits upon which to stratify society into haves and have-nots. The story of the emergence of psychological statistics is incomplete without mention of eugenics. The tools being developed were not neutral and scientific, but overtly political, aimed at achieving the goals of the state.\nAlso in the late 19th century was what Foucault called the implantation of perversions (Foucault 1978) - the creation of new symbolic threats to the body and to society as a whole. This operated through the invention of new characters that continue to exist within society today.\nFirstly, there was the medical specification of the homosexual (Townsend 2011). This began in 1864 with the work of Karl-Heinrich Ulrichs, who was gay himself. He specified men as either urnings or dionings. Urnings and Dionings are both male-bodied creatures, but the urning experiences the desires and character of a female (Townsend 2011). The dioning, by contrast, is normal. Discourse about the urning (renamed to the invert, and then to the homosexual) continued well into the 20th century, and the sissy (the archetype the invert represents) is, obviously, still with us.\nAlso within this time period, was the medical specification of the hysteric woman, which was initially the perogative of Jean-Martin Charcot.\nI‚Äôll mention just one more character that was invented in the later 19th century. For all of American history to this point, immigration law was about the process of naturalization - immigrants becoming citizens. From the beginning of the union, only white men of ‚Äúgood moral character‚Äù were allowed to become American citizens (Naturalization act of 1790?). There was little effort to actually prevent bodies from entering the country.\nUntil 1875. With the passage fo the Page Act of 1875, the United States declared its intention to keep undesirable bodies out of the country for the first time. Shortly thereafter, the ‚Äúillegal alien‚Äù was invented as a result of the Chinese Exclusion Act of 1882, which is the only American immigration law I am aware of that names a specific national group in its title.\nAll this to say that the nature and enforcement of undesirability were in massive flux in the late 19th century. The foreign element was moving within: the enslaved African could become a citizen and could vote, the invert or the hysteric could be hiding within anyone, and the state took up the power to deport bodies that did not belong. No longer was the anthropologist writing about the inferiority of foreign peoples (although to be clear, they absolutely were still doing that); the pschiatrist was now writing about our own inferiority.\nI consider the birth of statistics to be in this time period, which does not have pleasant implications for statistics as a field. There is a lot more to be said about the advent of statistics, and how statistics is designed to serve power (i.e., fulfill the demands of the state). However, I‚Äôm going to leave all of that unsaid and refocus on computation in general, and statistical computing in particular.\nThe late 19th century was also, notoriously, the era of massive trusts in the United States. These monstrous, monopolistic companies exploited both the consumer and the worker, but the United States did not yet have a legal mechanism for breaking them up. The most important monopoly for our purposes: the one that is most influential is the development of S and then R is the AT&T monopoly.\nAnother monopoly was also forming. Using Jacquard‚Äôs punched cards, an American man designed and patented a system to read punched cards. In 1890, this punched card system was used to complete the census, resulting in the 1890 census being completed two years quicker than the 1880 one. The company that developed this technology would go on to become IBM, which enjoyed monopoly status in the computing industry for several decades.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#early-19th-century-the-advent-of-computing",
    "href": "01_lore.html#early-19th-century-the-advent-of-computing",
    "title": "1¬† The Lore",
    "section": "1.3 Early 19th century: the advent of computing",
    "text": "1.3 Early 19th century: the advent of computing\nNear the end of the 19th century, a mathematician named David Hilbert decided that mathematics needed to be formalized. Up to that point, it had developed as myriad sub-disciplines that failed to cohere into a single, interconnected web of mathematics. Hilbert believed that it should, and his goal was to formalize this system. He believed that such a system (of mathematical axioms) needed to have three properties:\n\nto be consistent: it should not be possible to derive that a statement is both true and false\nto be complete: it should be possible to derive the truth of every true statement (or the falsity of its negation)\nto be decidable: there must be an algorithm that can identify all and only true statements in a finite number of steps.\n\n(The excitement about formalizing affected Hilbert, but by no means was he the first or the only to be caught up in this mess. Notoriously, Whitehead and Russel got spun up enough to publish a 126-page long proof that \\(1+1=2\\). I‚Äôm mostly attributing these three demands to Hilbert for sanity‚Äôs sake because I cannot stand to write out the sordid details. These three ‚Äúproperties‚Äù as I call them, are really inspired very loosely on any specific, cite-able Hilbert publication. He did publish a list of 23 questions, which refer to the properties I mention here, but understand this as a drastically over-simplified view of the mathematical debates unfolding at the time.)\nMathematics was not the only field to be heating up. There was growing speculation in physics that matter may not be as continuous as was previously assumed. In 1900, Max Planck published the first quantum theory in physics, which was aimed at modelling thermal radiation. Shortly thereafter, Albert Einstein published another quantum theory, this time aimed at modeling the the photoelectric effect. Both of these models used quantum stuff (i.e., minimal, discrete units of energy, creating measurements of energy that are always a multiple of the quantum unit), but the authors did not actually believe the world was quantum. Famously, Einstein‚Äôs theories of relativity both rely on space-time being continuous. They merely believed quantized math was the best way to explain non-quantum physical phenomena.\nNeils Bohr went the whole way, creating his model of the atom, with distinct, orbital electron shells. In the 1920‚Äôs quantum mechanics, as we know it today, came into existence. It did not make Einstein happy. Einstein wanted a deterministic world, where each cause has an specific, reliable effect. Quantum mechanics is not a deterministic theory of physics, but a probabilistic one. I take this diversion into the physical sciences not only to stress that this is a transition period within the physical sciences, but to temper my claim from the previous section. The ‚Äúdemands of power‚Äù did no less to supercharge the development of statistics and probability than did rapid changes in the way we understand and model the physical world.\nDuring my quantum mechanical tangent, G√∂del has proven that achieving the second property of Hilbert‚Äôs idealistic system is unlikely. In fact, G√∂del establishes that it is logically impossible that any formal mathematical system could be complete, as defined above.\nTo answer the question about whether mathematics is decidable, a new technology is needed. Before a mathematician can make formal claims about the capabilities or limitations of algorithms in general (as Hilbert demanded), she must first provide a rigorous definition of an algorithm. Two mathematicians took up this task, Alonzo Church who developed the lambda calculus, and Alan Turing who developed the Turing machine. Both men reached the same conclusion: mathematics cannot be decidable. It is logically impossible to make an algorithm (a Turing machine) that can identify all and only true statements (Turing 1936). There are, as it turns out, hard limits on the types of problems algorithms are able to solve (at least in a finite number of steps).\nThus, Turing half accidentally created the field of computer science while trying to answer a question about the foundations of mathematics. This is also an opportune time to introduce the term Turing-complete which refers to anything (model of computation, programming language, a book of instructions used by a human computer) that can simulate the a Turing machine. Any Turing-complete system is essentially equivalent to the original Turing machine described in (Turing 1936). The analytical engine is (theoretically, of course, it never got built) Turing-complete; Jacquard‚Äôs loom, by contrast, is not. Modern programming languages are, for the most part, Turing complete, meaning that any function you write in a modern programming language could be performed on the OG Turing machine from (Turing 1936).\nThe first electric, digital computer was not fully constructed until 1945. It was built by and for the U.S. military, who named the machine ENIAC. Thus, the first computations done on an electric, digital computer were intended to speed up the process of human and earthly destruction. ENIAC was a bunch of coordinated units that ran according to the placement of wires on the machine (Shustek 2016). The machine took IBM punched cards as input (remember the punched card monopolist from the end of the 19th century?).\nInitially, the wires on ENIAC had to be moved for each new problem (Shustek 2016). The process of re-configuring the machine for each new problem was tedious, but it was possible, and so ENIAC was Turing-complete. However, having to physically move wires prevented the machine from achieving the utility of a modern programmable computer.\nThis machine was very quickly modified in a way that dramatically changed its function. Instead of having to move wires, and then feed the machine (punched card) instructions based on the position of those wires, it would be much faster permanently code instructions (functions) into the machine. Then, the input of the machine could describe the sequence of functions. You could achieve looping by instructing ENIAC to perform a function repeatedly and conditional (if-statement) execution by instructing ENIAC to skip functions in the sequence.\nThis is the idea behind modern programming languages. Instructions for the computer, written in the computer‚Äôs language (ENIAC‚Äôs language was wires, the one we‚Äôll soon focus on is R) are stored within the machine. ‚ÄúProgramming‚Äù the machine involves telling it which instructions to perform and in which order. In 1948, the first ENIAC ‚Äúprogram‚Äù ran under this new computer architecture was a Monte Carlo simulation of neutron decay during nuclear fission (Shustek 2016).",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#att-bell-labs-and-s",
    "href": "01_lore.html#att-bell-labs-and-s",
    "title": "1¬† The Lore",
    "section": "1.4 AT&T, Bell Labs, and S",
    "text": "1.4 AT&T, Bell Labs, and S\nMonopolies suck, and AT&T did as well. Throughout the beginnning of the century, it gradually became clear that the benefits of a monopolist teleohpne provider were not going to materialize. In 1949, the U.S. Department of Justice sued AT&T for violating the anti-trust act, and the resulting 1956 consent decree prohibited AT&T from entering the computer business (Chang, Hen, and Kan n.d.).\nThis consent decree did not prevent the further degradation of AT&T‚Äôs service, nor did it prevent future anti-trust lawsuits. Throughout the 60‚Äôs and early 70‚Äôs, the U.S. government dogged AT&T with recurrent anti-trust lawsuits. In 1974, the Department of Justice began their final lawsuit against a monopoly AT&T. Over the course of the next decade, the government proved that AT&T was leveraging its monopoly power to predatory ends, annihilating potential competitors and pricing services far beyond the cost to provide them. This lawsuit ended in 1982 with the dissolution of AT&T into 7 Regional Bell Operating Companies (Chang, Hen, and Kan n.d.).\nBell Labs was probably the most important laboratory within AT&T. During the early 70‚Äôs, the researchers at the statistics research department within Bell Labs was using the programming language FORTRAN. FORTRAN is a general purpose, compiled programming language, developed by IBM (the punched card guys).\n(This isn‚Äôt super relevant, but I think it‚Äôs fun. Before 1968, computational statisticians had been using a algorithm called RANDU, which was a FORTRAN function that generated random numbers. In 1968, a mathematician proved that the allegedly random numbers actually all had to lie on a series of parallel hyper-planes, and we thus not actually random. wtf is a series of parallel hyper-planes? See below)\n\n\n\n\n\n\n\n\n\nFORTRAN, the name, stands for ‚Äúformula translation,‚Äù and it was primarily used for scientific computing, like computing weather models or doing computational physics - things that have to do with numbers, essentially. It is still used in these fields to some extent, although it is less popular for scientific computing than other, more recent programming languages, like R. FORTRAN is, computationally speaking, incredibly efficient, mostly by natively supporting parallel computation. For this reason, FORTRAN is still used to benchmark supercomputers. You can learn more about FORTRAN on its website.\nIn any case, in the 1970‚Äôs, the statistics research department at Bell Labs found FORTRAN to be somewhat insufficient, and they set out to develop a new language that would more fully suit their needs (Becker 1994).\n\n1.4.1 S\nS is a statistical computing language that was developed first at Bell Laboratories in the mid 1970s. At the time, statistics was undergoing a change. Previously, statistics had been developing as a set of methods - essentially algorithms that prescriptively described how to complete a statistical analysis from beginning to end. In the early 70‚Äôs, John Tukey was working at Bell Labs and at Princeton, and he was making a lot of noise about the problems with statistics. He popularized a different approach to statistics, establishing something of a binary between data analysis and statistics, just as I did between machine learning and statistics (in Section 1.2; Tukey (1972)).\nThe statistics research department was beginning to demand a tool that aligned with Tukey‚Äôs approach. FORTRAN, developed more than a decade before that demand was created at and by Bell Labs, did not measure up to the task. Instead, they decided to develop a new language, which they named S. Initially, there was a large focus on being able to import FORTRAN functions into S, so that there could be a smooth transition from FORTRAN to S within Bell Labs.\nS was built from the ground up to include graphics capabilities, and a structure that enabled and encouraged exploratory data analysis. The basic data structure in S is a vector of like-elements, which were used to make matrices and time-series; S also included lists (key-value maps) and the $ operator, which could be used to retrieve specific components of larger data structures (Becker 1994). It also included all of the arithmetic operators that you need in a desk calculator, making it useful for that purpose, as well.\nIn 1980, S was distributed outside of Bell Labs for the first time. Initially, it was distributed for a nominal fee and for educational use only, but by 1981 it was widely available (Becker 1994). After it began to be distributed, the developers added explicit looping (i.e., for loops), as well as the apply function, which could be used to loop over a vector while applying a function (Becker 1994). The developers also introduced the ‚Äúcategory‚Äù, which is now called the factor in R. Categories are vectors of data. They merge numerical and string data types - each entry in the vector is assigned a category label (so that you can read it), as well as a underlying integer (so that you can do math with categories).\nAlthough S was developed initially by statisticians, it clearly had utility as a data manipulation, graphics, and exploratory data analysis tool. In 1988, the developers released the ‚ÄúNew S,‚Äù renaming the software after some significant changes. The most significant feature of New S was the inclusion of first class functions, which are functions that you can assign to a name and and then refer to by that name. Functions are first class in that they are S objects, just like any vector or matrix. For the first time, S had depreciated functions, which R also has. Depreciated functions are functions for which there is a better alternative. They are generally still included in R and S distributions (so old code that uses depreciated functions can still run), but it‚Äôs best to avoid using them (and to use the better alternatives instead). By 1988, many of the FORTRAN functions from the initial development of S were rewritten in C, which is a general purpose programming languages on which New S is built (Becker 1994).\nIn 1991, the S development team expanded, and there was a focus on adding statistical software to the S language. Although S was developed by statisticians who intended to use it for statistics, the statistics are not inherent in S: ‚ÄúS is a computational language and environment for data analysis and graphics‚Äù (Becker 1994). As such, the developers added the formula class, which could be used to specify statistical models. The formula is marked by the ~ operator, with the dependent variable on the left and the independent variable(s) on the right (e.g., y ~ x + w + x*w).\nAlso in the 1991 release was the data.frame. Matrices are like vectors: they can only have one type of data. If you have a matrix that has even one number in it, then the entire matrix must be numeric, even if you want to use it to represent string data (like names and job titles) or categorical data (Becker 1994). So, a matrix is a combination of multiple vectors, all of the same type. A data.frame, by contrast, is a combination of vectors of any type. You can have a string vector (column) in the data frame representing job title, as well as a numeric vector representing income. As with matrices, you can use the $ operator to pick a vector out of the data frame (e.g., data$income picks out the income vector in the data frame called data).\nIt‚Äôs not really possible for a programming language to die. As we have seen with FORTRAN and S, new programming languages often use code from their older counterparts, especially at the beginning. Even though I can no longer find S on the internet and run it on my computer, a very large number of S functions continue to exist in R.\nI am able to find relatively scant documentation about this final period in the history of S, so the rest of this section is at least somewhat speculative (except claims that are cited, of course).\nWhat is the need for R if S exists? Well, well, well. Let‚Äôs talk about corporate fuckery, which both killed S and prevented it from dying. I have been making a much bigger deal over anti-trust law than the vast majority of those who introduce R to their students. To this point, as far as AT&T and Bell Labs are concerned, I have presented a world in which they are legally prohibited from selling computers (and presumably, computer software) as a result of the Consent Decree from 1956.\nUp to this point, no one was making money off of S. Although Bell was initially charging folks a nominal fee to use the software (Becker 1994), this practice ended quickly, meaning that the software was being distributed for free. As a result of the anti-trust, Bell Labs was not going to monetize this technology. Instead, one of their former employees had to do it.\nIn the 70s and 80s, the graphical user interface (GUI) was being born. This emerging technology came with a new generation of capitalists who had not been subject to extensive anti-trust, in which former trade union president Ronald Reagan did not believe - the capitalists who bring us Microsoft and Apple, who own outright the operating systems of about 85% of the worlds‚Äô computers (and many phones and other devices, as well).\nS wasn‚Äôt fated to become Windows; it was fated to become S-PLUS. S-PLUS is/was a statistical computing software with a graphical user interface. It was developed by a company owned by a former Bell Labs employee and University of Washington professor, R Douglas Martin. His work is primarily in econometrics, and he has extensively published about investment risks. Because of this, and because S-PLUS was and is mostly used by economists, a cynic might call it an application to be used for those who are unwilling or unable to learn how to code (similar in character to Microsoft‚Äôs SPSS). S-PLUS started circulating (for a fee) in about the year 1987, and it did include features that S did not (like generalized linear models).\nLet me just quickly recap, so I can make sure everyone is oriented in time - I‚Äôm discussing a lot of events that overlap and are not all well documented. In 1980, S was released to the public; in 1988, S had a significant update, becoming ‚ÄúNew S‚Äù; around 1987, a former employee of Bell Labs developed S-PLUS; in 1991, S had an update that focused on statistics.\nIn 1991, two statisticians quietly began work on the project (R) that would more-or-less kill S and S-PLUS.\nIn 1993, S and S-PLUS were reunited when Bell Labs sold S to the company that had developed S-PLUS. That company, in turn, immediately merged with a company called MathSoft. S-PLUS was only available on windows, and its relationship with Microsoft strengthened when features were added to connect S-PLUS to Excel and to SPSS.\nPart of the company (MathSoft) was sold, it got renamed (to Insightful), the exclusive license to distribute S turned into AT&T (i.e., Lucent, one of the companies that remained after AT&T) selling S so that it became the property of Insightful. Then Insightful got bought by a company called TIBCO, and then‚Ä¶\nI think you get the general idea. S and S-PLUS got bought, and sold, and licensed, and merged, and acquired to the point that it no longer really exists in any meaningful, public way. But by the 2000s, that didn‚Äôt matter.\n\n\n\n\n\n\nBakker, Arthur, and Koeno P. E. Gravemeijer. 2006. ‚ÄúAn Historical Phenomenology of Mean and Median.‚Äù Educational Studies in Mathematics 62 (2): 149‚Äì68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. ‚ÄúA Brief History of S.‚Äù In Computational Statistics, edited by Peter Dirschedl and R√ºdiger Ostermann, 81‚Äì110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. ‚ÄúCase Study 1: AT&T Divestiture.‚Äù Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nShustek, Leonard J. 2016. ‚ÄúProgramming the ENIAC: An Example of Why Computer History Is Hard.‚Äù May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. ‚ÄúThe Medicalization of ‚ÄòHomosexuality‚Äô.‚Äù Honors Capstone Projects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTukey, John W. 1972. ‚ÄúData Analysis, Computation and Mathematics.‚Äù Quarterly of Applied Mathematics 30 (1): 51‚Äì65. https://doi.org/10.1090/qam/99740.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem.‚Äù Journal of Math 58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "02_r_and_friends.html",
    "href": "02_r_and_friends.html",
    "title": "2¬† R and Friends",
    "section": "",
    "text": "2.1 R and Friends\nIn 1991, two professors in New Zealand began to develop R, a process which they documented in (Ihaka and Gentleman 1996). R is very similar to S; so similar, in fact, that it is frequently called a dialect of S. What is the difference between S and R? The creators of R describe it as having the syntax of S (meaning that most examples, including the following example, can be run in S or R) but the semantics of Scope (which is a programming language from the Lisp family).\nProbably the key difference between the two languages is the lexical scoping. Whenever you use R (or most other programming languages), you have to have something called a frame. Frames include things like functions and named variables. Each function creates its own frame. The frame for the function f in the example below (from Ihaka and Gentleman (1996)) contains the named variable y and the named function g. The named function g, as a function, creates its own frame (in which to store variables and functions). There is also something called a global frame which, in the following example, includes an assignment of the value 123 to the name y and the assignment of some function to the name f.\nAs you can see, in R, running the function f with 10 as an argument results in the function returning 100 (10 times 10). In S, this very same code would have resulted in function f returning the value 123. In S, when we define the function g, S uses the global frame as the basis for the function, including the assignment of the value 123 to y. R, by contrast, creates g with a locally-scoped frame, meaning that the frame for g includes the assignment of the value x * x to the variable y (assignments which are inherited from the parent frame). Thus, in S, the function g is evaluated as print(123), but the R function is evaluated as print(x * x) (the function f is responsible for substituting x to make print(10 * 10).\nUnlike Scheme, but like S, R uses lazy evaluation. In essence, this means that R does not run your code unless it absolutely has to. I‚Äôll use the example of Figure¬†2.1 to explain what this means. Lines 2 through 6 contain the declaration of function f (even though line 4 also contains the declaration of function g. When you run line 2, all of the lines down to line 6 (where the closing bracket, } is located) get stored in R‚Äôs memory next to the name f. However, R will not run the function f until you actually go to use it (i.e., until you make the function call in line 7). This is why we call R lazy, but what‚Äôs the big deal?\nIf you make a syntax error in your declaration of function f, R is going to have to tell you that you made a syntax error at some point. In a language that is not lazy, the language evaluates function f when you store it. Thus, a non-lazy language will send you a syntax error after you run the function declaration (i.e., after you run line 2, which also causes lines 3-6 to run). If Figure¬†2.1 were written in a non-lazy language, the syntax error would occur where the 1 annotation is. However, in R, the function is merely stored when you run lines 2-6. Function f does not actually run until you call it in line 7 (marked with a 3 in Figure¬†2.1). Laziness is a feature that R inherited from S, which is also lazy.\nThis is getting a bit technical. The two men who developed R are Ross Ihaka and Robert Gentleman. On a family tree posted on Ihaka‚Äôs personal website, he lists himself as the academic grandchild of John Tukey, then statistician at Bell Labs that popularized exploratory data analysis (the framework that created the need for S, which was also, if you‚Äôll recall developed at Bell Labs).\nIhaka is a now retired statistician from the University of Auckland. Gentleman is a bioinformatician who currently works at Harvard and 23andMe. Allegedly, Ihaka and Gentleman developped R for use in teaching statistics. That was part of both of their jobs as professors, after all. However, this doesn‚Äôt seem very plausible (why would the professors write their own statistical programming languages rather than using a well-documented one which would seem to be better for pedagogy), nor have I seen any specific evidence for it. That being said, in the years 1993-94, R was stuck at the University of Auckland, being used by them, probably their peers, and less probably their students, but the software was not yet being distributed, as S or S-PLUS was.\nIn 1995, one of their colleagues convinced them to licence use of the software as free software under a GNU general public license.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>R and Friends</span>"
    ]
  },
  {
    "objectID": "02_r_and_friends.html#r-and-friends",
    "href": "02_r_and_friends.html#r-and-friends",
    "title": "2¬† R and Friends",
    "section": "",
    "text": "show code for this result\ny &lt;- 123 # assign 123 to the name y\n1f &lt;- function(x) {\n  y &lt;- x * x # assign x times x to the name y\n2  g &lt;- function() print(y) # create new function and new scope\n  g() # return the output of function g\n}\n3f(x=10)\n\n\n\n1\n\nthe the beginning of the declaration of function f (between lines 2 and 6)\n\n2\n\nthe declaration of function g\n\n3\n\na function call for function f (with the argument x set to equal 10)\n\n\n\n\n\n\n\n\n[1] 100\n\n\n\n\nFigure¬†2.1: an example from Ihaka and Gentleman (1996)\n\n\n\n\n\n\n\n\n\n\n2.1.1 Free Software\nI just bolded the term free software; why? As it turns out, the term free software has a specific definition that extends far beyond the idea of ‚Äúsoftware that you don‚Äôt have to pay for.‚Äù So what is free software? Free software is characterized by the four freedoms (Foundation n.d.):\n\n\n\nFree Software Foundation‚Äôs Four Essential Freedoms\n\n\nThe idea of free software, and it‚Äôs formation in the four freedoms seen above, came to be popular in the mid-80‚Äôs after the Reagan government had made clear it‚Äôs stance (and the republican, and soon the democratic, party‚Äôs stance) on anti-trust enforcement. In the wake of the disruption to the computing (IBM) and telephone (AT&T) industries, the Reaganites declared that we were entering into an era of free, competitive trade while setting up a regulatory framework that would allow tech companies to consolidate power and market share ad infinitum, resulting in the current big 4(-ish): Apple, Alphabet (Google), Amazon, and Meta (and Microsoft, Nvidia, and potentially Tesla and like Netflix, depending on who you ask).\nIt is a good thing for us, then, that none of these companies own R, which the developers have promised will remain free software indefinitely.\n\n\n2.1.2 Open Sourcing and Crowd Sourcing\nThese days, it feels like only a real purist will call R free software. The more en vogue term is ‚Äúopen source.‚Äù The Free Software Foundation would like you to treat the terms as separate however (see this article).\nIn reality, the labels ‚Äúopen source‚Äù and ‚Äúfree software‚Äù are mostly synonymous in that they refer to many of the same software. As Freedoms 1 and 3 make clear, software must be open source before it can be free. The free software folks‚Äô biggest problem with ‚Äúopen source‚Äù is one of semantics, really. They claim that the ‚Äúopen source‚Äù movement argues too much about how free software is good for business and software development (i.e., because curious users can look for and find bugs). The free software people are not interested in these practical matters, focusing instead of the moral question of what sort of software is right and wrong. They correspondingly argue their case in the form of moral imperatives (the four freedoms).\nI am less interested in these theoretical questions, and more interested in explaining to you what the implication of free or open software is bound to be (at least in the case of R): crowd-sourced development.\nR itself provides you with basic statistical functionality. However, the vast majority of what is commonly called ‚ÄúR‚Äù is not actually part of the base distribution of R. Instead, most of the functionality is packaged within ‚Äúpackages,‚Äù which are you load into R with the library() function, as shown below:\n\n\nshow code for this result\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\nHadley Wickham\n\n\n\n\n\n\n\n\nFoundation, Free Software. n.d. ‚ÄúWhat Is Free Software? - GNU Project - Free Software Foundation.‚Äù Accessed May 9, 2024. https://www.gnu.org/philosophy/free-sw.html.\n\n\nIhaka, Ross, and Robert Gentleman. 1996. ‚ÄúR: A Language for Data Analysis and Graphics.‚Äù Journal of Computational and Graphical Statistics 5 (3): 299‚Äì314. https://doi.org/10.2307/1390807.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>R and Friends</span>"
    ]
  },
  {
    "objectID": "03_r_101.html",
    "href": "03_r_101.html",
    "title": "3¬† R 101",
    "section": "",
    "text": "3.1 Introduction\nThis tutorial is adapted from a fabulous youtube video by Very Normal. I recommend watching this video before starting this tutorial.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#data-types",
    "href": "03_r_101.html#data-types",
    "title": "3¬† R 101",
    "section": "3.2 data types",
    "text": "3.2 data types\n\n3.2.1 Numeric Data\n\n\nshow code for this result\na &lt;- 12 # numeric\nb &lt;- 5L # integer\nc &lt;- 12.5 # double (floating point)\n\nprint(a + b) # numeric \n\n\n[1] 17\n\n\nshow code for this result\nprint((a + b) / b)  # double (floating point)\n\n\n[1] 3.4\n\n\n\n\n3.2.2 Complex Numbers\nRealistically, you‚Äôre never going to use complex numbers, so you can safely skip this exercise. However, I think complex exponentiation is pretty, so you‚Äôd really be missing out.\n\n\nshow code for this result\n# change real and imaginary parts to see how the plot changes\nz &lt;- (0.8 + 1i)\n\n# take z to the power of 1, 2, ..., whatever the 'to' argument is\ncomplex_numbers &lt;- z ** seq(from = 1, \n                      to = 12, # change to plot more data \n                      by = 0.1) # change to plot more data\n\n# plot the complex numbers\nplot(complex_numbers, type = \"o\") # possible types: \"p\", \"l\", \"b\", \"c\", \"o\", \"h\", \"s\", \"S\", \"n\"\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 Characters\n\n\nshow code for this result\na &lt;- \"hello\"\nb &lt;- 'world'\n\nprint(paste(a, b))  # paste is a function that concatenates strings\n\n\n[1] \"hello world\"\n\n\nshow code for this result\nprint(paste0(a, b))\n\n\n[1] \"helloworld\"\n\n\n\n\n3.2.4 Logical Data\n\n\nshow code for this result\nTRUE & FALSE\n\n\n[1] FALSE\n\n\nshow code for this result\nTRUE | FALSE\n\n\n[1] TRUE\n\n\n\n\n3.2.5 factors\nfactors combine numbers and strings. They are useful for categorical data.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#data-structures",
    "href": "03_r_101.html#data-structures",
    "title": "3¬† R 101",
    "section": "3.3 Data Structures",
    "text": "3.3 Data Structures\n\nvectors: stores ordered data of the same type\n\nhave indexes that start at 1\n\nmatrix: stores data in 2 dimensions\narrays: higher dimensional matrices\nlists: key value pairs\n\nif you don‚Äôt put names in the list, R will index them with numbers\n\ndataframes: each row is an observation, each column is a characteristic\ntibbles: data frames with extra functionality",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#iteration",
    "href": "03_r_101.html#iteration",
    "title": "3¬† R 101",
    "section": "3.4 Iteration",
    "text": "3.4 Iteration\n\nfor loop: will run code a certain number of times\nwhile loop: will run code until a certain condition is no longer met\n\nuseful in optimization tasks (like sample size calculations)",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#control-flow",
    "href": "03_r_101.html#control-flow",
    "title": "3¬† R 101",
    "section": "3.5 Control Flow",
    "text": "3.5 Control Flow\nif (condition) {\n  code to run if condition is met\n}\nyou can add else statements, and even chain else statements, but it is easy to get confused",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#functions",
    "href": "03_r_101.html#functions",
    "title": "3¬† R 101",
    "section": "3.6 functions",
    "text": "3.6 functions\nfunction_name = function(inputs) {\n\n  a bunch of code you would like to reuse\n  the last line of code is the output of the function\n\n}",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#libraries",
    "href": "03_r_101.html#libraries",
    "title": "3¬† R 101",
    "section": "3.7 Libraries",
    "text": "3.7 Libraries\n\nhow to load and detach libraries\ndevtools - downloading from github\nCRAN",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#rstudio",
    "href": "03_r_101.html#rstudio",
    "title": "3¬† R 101",
    "section": "3.8 RStudio",
    "text": "3.8 RStudio",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#tidyverse",
    "href": "03_r_101.html#tidyverse",
    "title": "3¬† R 101",
    "section": "3.9 Tidyverse",
    "text": "3.9 Tidyverse\n\n3.9.1 readr\n\nread_csv and related funcions for reading data\nreadxl::read_xlsx for excel files\n\n\n\n3.9.2 tibble\n\nhow to make a tibble\nalmost all of the functions in the tidyverse input and output tibbles; we can pipe data\n\n\n\n3.9.3 dplyr\n\nmanipulation of data, especially useful for cleaning\nselect: select or remove\nfilter\nmutate: create new columns\n\n\n\n3.9.4 stringr\n\n\n3.9.5 lubridate\n\n\n3.9.6 forcats\n\n\n3.9.7 purr\n\nlist columns are super useful - lists can have different data types\nmap functions - output is a list column\n\n\n\n3.9.8 tidyr\n\npivot wider\npivot longer\n\n\n\n3.9.9 ggplot2\n\nmappings\ngeom - like geom_line or geom_point\nyou can add as many layers as you want\nthemes",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "04_visualization.html",
    "href": "04_visualization.html",
    "title": "4¬† Visualization",
    "section": "",
    "text": "4.1 Plotting Basics\nThe first step to making any plot is calling the ggplot function. If you look at the help page for the ggplot function (by typing ?ggplot into an R console) , you will find that it takes 2 arguments: data and mapping. Ggplot expects that the data argument is going to be a data frame with tidy data in.\nshow code for this result\nggplot(data = time_use)\nAs you can see, ggplot creates an empty plot. Next, we will add a mapping argument. The mapping will tell ggplot which aesthetics (like the x-axis, y-axis, color, shape, etc.) will be represented by which variables in the data. In this case, we want the time_spent variable on the x-axis and the women_to_men variable on the y-axis, as they appear in the reference plot. When we call the ggplot while providing a data and a mapping argument, R will create a blank plot with axes.\nshow code for this result\nggplot(data = time_use, mapping = aes(x = time_spent, y = women_to_men))\nAs with any function in R, we do not need to write all of the arguments on a single line. We can separate the arguments onto different lines, as long as we make sure each line ends with a comma or the end parenthesis (i.e., the end of the function call). You may find that the following two code snippets are more readable than the one above, even though all three would produce the same output.\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men))\nshow code for this result\nggplot(\n  data = time_use,\n  mapping = aes(\n    x = time_spent,\n    y = women_to_men\n  )\n)\nTo R, it does not matter whether you type this function call out in 1 line or in 7; it is the same arguments being passed to the same function and thus produces the same result.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "04_visualization.html#plotting-basics",
    "href": "04_visualization.html#plotting-basics",
    "title": "4¬† Visualization",
    "section": "",
    "text": "4.1.1 Geoms\nOur plot is missing something: can you spot it? There is no data on our plot! We represent data in a ggplot by using a geom function. Almost all of these functions start with geom_ (like geom_bar or geom_smooth) or stat_ (like stat_count). You can see a fuller list of the geom functions available on the ggplot2 cheat sheet.\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nWe add a geom by literally adding it (with the + operator) to the ggplot function call. The + always has to go at the end of the line. You can separate the functions by as many empty or commented lines as you‚Äôd like. You can also do this within function calls. So, this code will run:\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  \n  \n  # this is a comment\n  \n  geom_point()\n\n\n\n\n\n\n\n\n\nas will this code:\n\n\nshow code for this result\nggplot(data = time_use,\n       \n       # this is a comment\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nand if you wanted to be really verbose, you could even do something like this:\n\n\nshow code for this result\n# create ggplot\nggplot(\n  \n  # add data to plot\n  data = time_use,\n  \n  # create mapping\n  mapping = aes(\n    # map x-axis\n    x = time_spent,\n    # map y-axis\n    y = women_to_men\n  )\n) +\n  \n  # add scatterplot\n  geom_point()\n\n\n\n\n\n\n\n\n\nWhat is the difference between a mapping and a style? A mapping connects one aesthetic to a variable, but a style just sets the aesthetic. For example, styling our scatter plot might mean turning all the points blue, whereas a mapping would match each activity to a color based on a scale. This is a graph that uses color as a style:\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\n\n\nand this is a plot that uses color as a map:\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = activity))\n\n\n\n\n\n\n\n\n\nYou can put mappings in the ggplot function, or in any geom function. In the above code, the mapping for x and y is in the ggplot function, and the mapping for color is in the geom_point function.\nEvery geom will inherit the mapping from the ggplot function. If we added another geom to the plot, we could see this. In the plot below, I added the geom_smooth, and - as you can see - it inherits the x and the y aesthetic from the ggplot function, but it does not inherit the color argument from the geom_point function.\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = activity)) +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIf you wanted R to make differently colored smooth lines in this plot, you could add a color aesthetic to the geom_smooth function, or you could move the color aesthetic from the geom_point function to the ggplot function, thereby allowing the geom_smooth function to inherit a color aesthetic.\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, \n                     y = women_to_men,\n                     color = activity)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet‚Äôs do a little investigating with just the geom_smooth function.\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe used the color aesthetic to make separate lines above, but we didn‚Äôt need to. We can also use the group or linetype arguments to create separate lines\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(group = activity))\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(linetype = activity))\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can change the data supplied to geom_smooth to draw only one line:\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = continent)) +\n  geom_smooth(data = filter(time_use, activity == \"Unpaid work\"),\n    mapping = aes(group = activity))\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou should use the help page (?geom_smooth) to read more about the aesthetics and arguments the function can take. I will only highlight two more: method, and se. I frequently find myself favoring a linear regression line (rather than the default which uses the LOESS smoothing function (a type of local, polynomial regression). You can get a straight line by changing the method argument to ‚Äúlm‚Äù. The se argument can be set to T or TRUE or F or FALSE, and it controls whether the plot includes an gray error area.\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(group = activity), \n              method = \"lm\",\n              se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOn a completely separate note, you use a very similar process of setting the data and mapping arguments to make a bar chart. Ggplot makes a distinction between a bar chart and a column chart (even though they can appear to be identical). A bar chart has a single, discrete aesthetic mapping (like one that maps x to continent).\n\n\nshow code for this result\nggplot(data = distinct(time_use, country, continent),\n       mapping = aes(x = continent)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nA column chart takes 2 mappings: a discrete x aesthetic (like activity) and a continuous y aesthetic (like time_spent).\n\n\nshow code for this result\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                color = activity)\n) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nBar charts are a great example of the difference between the color and the fill aesthetics. Points and lines have only colors, but bars and columns (and other geoms, like density plots) have fills, as well. Let‚Äôs fix the last plot.\n\n\nshow code for this result\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                fill = activity)\n) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nThis plot does not provide a helpful comparison of the time use in OECD countries across various continents because all of the columns have a different height. In reality, we would like all of the columns to be the same height so we can compare proportions. We can do this using the position argument\n\n\nshow code for this result\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                fill = activity)\n) +\n  geom_col(position = \"fill\")\n\n\n\n\n\n\n\n\n\nThis is a better plot to view the differences in OECD time use across continents. However, there is (again) very limited data for several continents which are either missed (i.e., most of Africa, Oceania, South America, and Asia) or have only very few countries (i.e., North America)\n\n\n4.1.2 Faceting\nUsing just the ggplot function and a few geom functions, we can get damn near the reference plots we started with.\n\n\nshow code for this result\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFaceting is tricky to explain in words, but it‚Äôs very easy to understand if you can see it.\n\n\nshow code for this result\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere are two faceting functions: facet_wrap and facet_grid. Both have a scales argument, which is set as ‚Äúfixed‚Äù by default. You can change it to:\n\n‚Äúfree_x‚Äù to allow the x axes to have different limits in the different plots\n‚Äúfree_y‚Äù to do the same for the y axes\n‚Äúfree‚Äù to have both axes take different limits for each separate plot\n\nIn this case, we‚Äôll just set the scales argument to \"free\" so that all of the data aren‚Äôt packed so tightly.\n\n\nshow code for this result\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2,\n             scales = \"free\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nfacet_grid is helpful if you would like to facet on two (discrete!) variables at once (like activity and continent in the plot below).\n\n\nshow code for this result\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 0.5,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_grid(continent ~ activity, \n             scales = \"free\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWith so few data, such extensive faceting is neither informative nor helpful, so we‚Äôll stick with just one facet: activity.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "04_visualization.html#plot-styling",
    "href": "04_visualization.html#plot-styling",
    "title": "4¬† Visualization",
    "section": "4.2 Plot Styling",
    "text": "4.2 Plot Styling\nOur current plot is beginning to look very similar to the reference plot we were attempting to recreate. We have written 23 lines so far. To save space, I am going to save the plot that we currently have under the name p. When we call p, R will make the plot. We can also add things to p, just as we added them to the ggplot function.\n\n\nshow code for this result\n# save ggplot object under the name \"p\"\np &lt;- ggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2,\n             scales = \"free\")\n\np\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n4.2.1 Labels\nBy default, ggplot does not create a title or subtitle for the plot, and it uses the names of the variables for the axes and scales (i.e., color scale in our plot). The labs function allows you to update various text elements in the plot.\n\n\nshow code for this result\np &lt;- p + labs(title = \"Gender Parity in Time Spent in OECD Nations\",\n         subtitle = \"ATTN: all plots on different axes\",\n         x = \"time spent (minutes per day)\",\n         y = \"ratio of women's time spent to men's\")\n\np\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAdding labels to the outside of your plot is as simple as that. You could also use labels as a geom using the geom_text function. It takes an additional aesthetic, label that we have not yet seen.\n\n\nshow code for this result\nggplot(data = filter(time_use, activity == \"Unpaid work\"),\n       mapping = aes(x = time_spent,\n                     y = women_to_men,\n                     color = continent)) +\n  geom_text(mapping = aes(label = country))\n\n\n\n\n\n\n\n\n\n\n\n4.2.2 Scales and Coordinates\nOur plot is different from the reference plot in which colors it uses. The reference plot uses black, orange, blue, green, and yellow; but, ours uses red, beige, green, blue, and purple. What gives? The reference plot uses a different color scale than does our plot. The color scale is the part of the plot that matches each continent to a unique color. The reference plot uses a color scale from the ggthemes package that is colorblind friendly.\n\n\nshow code for this result\np &lt;- p + ggthemes::scale_color_colorblind()\n\np\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nscales also control the x and y axes, specifically continuous scales. We can use a scale function to set the limits or breaks for the axes. In our case, this allows us to demonstrate how much larger the gender disparity appears to be for unpaid work rather than for any of the other (measured) uses of time.\n\n\nshow code for this result\np + \n  scale_y_continuous(limits = c(0, 7), breaks = c(1, 3.5, 7))\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nwe can use scale functions to add transformations, as well. There are functions like scale_x_sqrt and scale_x_log10 that put the data on square root or log axes. Our time use data is not a great example of the utility of these types of axes. Instead, look at this data about coffee production:\n\n\nshow code for this result\ncoffee_plot &lt;- ggplot(data = coffee,\n       mapping = aes(x = pounds,\n                     y = pop_2019)) +\n  geom_text(mapping = aes(label = country)) +\n  labs(x = \"pounds of coffee produced in 2019\",\n       y = \"population in 2019\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\ncoffee_plot\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe coffee data is dominated by small producers - with many populations and productions below 10 million people and pounds. The relationship between the variables is obscured by the clumping of data, which is caused by the massive outliers of Brazil (massive production) and India (massive population). A log-log plot (with log axes on the x and y) reveals a different perspective on this data. You must be careful, however, because log axes are not incredibly easy for most people to read, and the essentially everyone (including those that can read log-log scales) expects the scales on a plot to be linear unless explicitly warned otherwise.\n\n\nshow code for this result\ncoffee_plot + \n  scale_x_log10() +\n  scale_y_log10()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere are many more transformations the scale functions can perform (like a boxcox, or logit transform), and you can see a fuller list in the documentation of the scale_x_continuous function (or any of the continuous scale functions). Okay; bye, coffee data!\nWe can finally use scales functions to control the labels on the axes. I find this is particularly helpful if you have percentages on one axis. We have a proportion, which can be represented as a percentage.\n\n\nshow code for this result\np &lt;- p + scale_y_continuous(labels = scales::percent_format())\n\np\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n4.2.3 Themes\nThe plot we have still doesn‚Äôt look quite like the reference plot we aimed toward. This can be explained by a difference in theme. The reference plot uses the minimal theme, whereas ours uses the default theme. We can ‚Äúfix‚Äù that by using the theme_minimal function.\n\n\nshow code for this result\np + theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can see the other default themes by seeing the help page for theme_minimal (i.e., ?theme_minimal, which is also the help page for the other built-in ggplot themes). My other favorite is theme_classic.\n\n\nshow code for this result\np + theme_classic()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI also use theme_void, but typically only when making pie charts. theme_void removes most of the elements in the plot, including the x and y axes and gridlines.\n\n\nshow code for this result\np + theme_void()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nshow code for this result\np &lt;- p + theme_minimal()\n\n\n\n\n4.2.4 Guides, Legends, and Text\nAlthough it makes no sense not to have it in this case, there are some cases in which you would like to remove the legend, which appears to the right of the plot by default. There are two ways to do this, using guides or using theme.\nguides is a function that is occasionally helpful for specifying which type of scale variables should be mapped to. You could set the guide to the color and/or shape arguments to ‚Äúnone‚Äù to remove one or both aspects of the legend.\n\n\nshow code for this result\np + guides(\n  color = \"none\"\n)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou could also remove the legend, instead of removing variables from the legend. You would do this using the legend.position argument in the theme function.\n\n\nshow code for this result\np + theme(legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou could also move the legend around using the legend.position argument.\n\n\nshow code for this result\np &lt;- p + theme(legend.position = \"top\")\n\np\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFinally, you can use the theme function to make really specific changes to the plot, like changing the angle of the numbers of the y-axis\n\n\nshow code for this result\np + theme(axis.text.x = element_text(angle = -30),\n          axis.text.y = element_text(angle = 30))\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023. ‚ÄúData Visualization.‚Äù In R for Data Science, 2nd ed. https://r4ds.hadley.nz/data-visualize.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. ‚ÄúData Visualization.‚Äù In R for Data Science, 1st ed. https://r4ds.had.co.nz/data-visualisation.html.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "0R_references.html",
    "href": "0R_references.html",
    "title": "References",
    "section": "",
    "text": "Bakker, Arthur, and Koeno P. E. Gravemeijer. 2006. ‚ÄúAn\nHistorical Phenomenology of Mean and\nMedian.‚Äù Educational Studies in Mathematics\n62 (2): 149‚Äì68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. ‚ÄúA Brief History of\nS.‚Äù In Computational\nStatistics, edited by Peter Dirschedl and R√ºdiger\nOstermann, 81‚Äì110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. ‚ÄúCase\nStudy 1: AT&T\nDivestiture.‚Äù Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of\nSexuality. Vol. 1. 3 vols. Random House.\n\n\nFoundation, Free Software. n.d. ‚ÄúWhat Is Free\nSoftware? - GNU Project - Free Software\nFoundation.‚Äù Accessed May 9, 2024. https://www.gnu.org/philosophy/free-sw.html.\n\n\nIhaka, Ross, and Robert Gentleman. 1996. ‚ÄúR: A\nLanguage for Data Analysis and\nGraphics.‚Äù Journal of Computational and\nGraphical Statistics 5 (3): 299‚Äì314. https://doi.org/10.2307/1390807.\n\n\nShustek, Leonard J. 2016. ‚ÄúProgramming the ENIAC:\nAn Example of Why Computer History Is\nHard.‚Äù May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. ‚ÄúThe Medicalization of\n‚ÄòHomosexuality‚Äô.‚Äù Honors Capstone\nProjects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTukey, John W. 1972. ‚ÄúData Analysis, Computation and\nMathematics.‚Äù Quarterly of Applied Mathematics 30 (1):\n51‚Äì65. https://doi.org/10.1090/qam/99740.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to\nthe Entscheidungsproblem.‚Äù Journal of Math\n58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023.\n‚ÄúData Visualization.‚Äù In R for Data\nScience, 2nd ed. https://r4ds.hadley.nz/data-visualize.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. ‚ÄúData\nVisualization.‚Äù In R for Data Science, 1st\ned. https://r4ds.had.co.nz/data-visualisation.html.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "0A_resources.html",
    "href": "0A_resources.html",
    "title": "Appendix A ‚Äî Getting Help",
    "section": "",
    "text": "The contents of this appendix are not here!\n\ngetting help in R (? and ??)\nthe two types of help pages, and the structure of each\n\ndataset pages\nfunction pages\n\nCheatsheets\ndocumentation online\n\nrmarkdown\nquarto:\n\nquarto guides\nquarto reference\n\nrstudio user guide\ntidymodels (for machine learning, mostly)\ngoogle and stack exchange\n\nThere is a general guide to getting R help, and it includes a suggestion I will forward: use Google and include the term ‚ÄúR‚Äù in the search",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Getting Help</span>"
    ]
  }
]