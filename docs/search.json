[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hunterr_2024",
    "section": "",
    "text": "Preface\nListing¬†1: greeting\n\n\nprint(\"Good Morning! ü§ó\")\n\n\n\n\n[1] \"Good Morning! ü§ó\"",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#this-book",
    "href": "index.html#this-book",
    "title": "hunterr_2024",
    "section": "This book",
    "text": "This book\nThis document you are reading now is called a Quarto book. We will talk more about Quarto later, but for now, all you need to know about Quarto books is that they:\n\nare relatively simple to construct,\ncan contain R code snippets and their outputs,\nare pretty enough for me to be temporarily satisfied.\n\n\nFeatures of This Book\n\n\n\n\n\n\ncallouts\n\n\n\n\n\nThis book let‚Äôs me put many of my least relevant tangents in these collapsible little notes.\n\n\n\nThe features of this book, which are achieved easily through Quarto, explain its format. Code snippets appear in this book, but only if you want them to. Each code snippet is collapsible by clicking the triangle next to ‚Äúshow code for this result‚Äù. If you really hate code, you can click the ‚Äú&lt;/&gt; Code‚Äù button on the top of each page and hide (or show) all the code at once. Speaking of the ‚Äú&lt;/&gt; Code‚Äù button, if you click ‚ÄúView Source‚Äù you can see all of the code that was used to create the page.\nQuarto also lets me show you ‚Äúpaged tables.‚Äù In the following code, I generate 1000 random numbers between -10 and 10, which I label x. Then, I apply the mystery_function to each number, producing y. What does mystery_function do?\n\ntable_1 &lt;- tibble(\n  x = runif(n = 1000, min = -10, max = 10),\n  y = mystery_function(x)\n)\ntable_1\n\n\n  \n\n\n\nQuarto also let‚Äôs me show you plots, like the one below. What do you think mystery_function does now?\n\nqplot(x = table_1$x, y = table_1$y) +\n  labs(x = \"x\", y = \"mystery_function(x)\")\n\n\n\n\n\n\n\n\nIn reality, mystery_function just squares x, so:\n\\[\n\\mathtt{mystery\\_function}(x) = x^2\n\\tag{1}\\]\nAll of the chunks of code have line numbers. For any chunk, a little clipboard appears when you hover over the code listing, and you can copy it by clicking on the clipboard. (This includes the code in the ‚ÄúView Source‚Äù pane, meaning you can copy the entire page, text and all.)\nQuarto also processes citations, so I can, for example, easily direct you towards my two fathers: Turing (1936) and Foucault (1978). It will also process internal links so that I can, for example, send you back to the top of the page: Listing¬†1.\nFinally, Quarto allows me to annotate code, which is helpful to explain how it works when a high-level of technical detail is needed. For an example of those annotations and when they might be helpful, look to Figure¬†2.1.\n\n\nTools\nR is an free and open-source statistical programming language.You use R by typing commands into an R console, which looks like this:\n\n\n\n\n\n\n\nRStudio\n\n\n\n\n\nRStudio is a graphical user interface (GUI) and interactive development environment (IDE) with which to use R. This means that RStudio contains an R console (lower left) and a variety of other tools (right), like a text editor (upper left). RStudio looks like this:\n\n\n\n\n\n\n\n\n\n\nTidyverse\n\n\n\n\n\nTidyverse is a collection of R packages that are designed to work well together and to acomplish data analysis tasks. The Tidyverse packages were developed by Hadley Wickham primarily, but also by teams of his collaborators.\nBecause Tidyverse is set of R pacakges, there is not picture to provide. Tidyverse exists as R functions that you can (and will) use in your code.\n\n\n\n\n\n\n\n\n\ngrammar of graphics\n\n\n\n\n\nThe grammar of graphics refers to another set of R packages, exemplified by ggplot2, which is included in the tidyverse. The grammar of graphics is the tool that most R programmers use to produce the ‚Äúproduction-quality graphics‚Äù for which R is known. Once you begin to produce plots with ggplot2, you are likely recognize that you have been seeing these plots for years.\n\n\n\nI believe that these tools will allow you to accomplish the vast majority of data analysis tasks, and that knowing how to use them will result in you understanding data analysis on a deeper level and being able to do it faster. That being said, these are not the only tools for data analysis. I encourage you to explore others, as well; particularly, if you intend to do a lot of work with text data, I would suggest python.\nBecause my audience does not intend to become ‚Äúprogrammers,‚Äù per se, I would also like to introduce you to the following tools, which allow R code to be integrated into readable documents instead of writing R code in separate text files with R code only (called scripts). R scripts have a place, to be clear, especially while you are learning. However, if you would like to share the analysis that you do with an R script, sharing that R script is not a good way to do that. When you send someone an R script, you are sending them a bunch of code, not an analysis\nThe current gold standard for the sort of work you are most likely to want to produce (reproducible research) is this - the document you are reading now. This is a document which includes code, the output of that code, and text explaining that code and providing context. This approach is called literate programming. The tools used to produce literate programming documents with R are:\n\nRMarkdown: a document preparation software based on R and a markup language called markdown that is mainly used to make static documents (like appendices to a journal article). RMarkdown uses a software called pandoc to turn the .rmd file into: a pdf (latex or a beamer presentation), a .html website, a word document, a power-point, and a lot of other formats you‚Äôre likely to never use.\nQuarto: a very similar, but more advanced and comprehensive software than RMarkdown. Most of the things you can do in Quarto are also possible in RMarkdown, like adding cross references (like this Equation¬†1), creating books (also like this), creating interactive data dashboards (which seems particularly trendy as of late), and creating blogs and websites. Quarto is made by the same company that makes RStudio.\nShiny: a software (written in R) that allows you to create interactive plots, which may be helpful if you are, say, trying to decide the optimal number of bins in a histogram. You could use shiny to create a histogram and a bins slider, so that you could easily see a variety of different bin sizes merely by moving a slider (instead of by writing, modifying, and rewriting code to achieve the same end).\n\nMy goal for you is to write code that is readable and to put that code inside documents that are actively fun and/or interesting to read. (That‚Äôs also, incidentally, my goal for me.)\nWe are also going to do statistics! There are two approaches to statistics that we‚Äôre going to adopt through these lessons, so I‚Äôd like to begin by elucidating the way in which these approaches are different. I am more familiar with the first approach (exploratory data analysis). Confirmatory data analysis uses many of the same tools (like hypothesis testing, which I will show you), but it uses them in a different and moer complicated way. The ultimate goal of both approaches is to predict the result of measurements.\n\nexploratory data analysis: If the purpose of a statistical model is to predict data, then a model that makes the most accurate prediction is the best model. The model creation process is iterative. Once you see the results of a model, you can use the results to modify the model itself. Generally, practitioners recognize that there are a variety of different types of models that could be used for any task. Thus, they usually construct a variety of different models and then compare them to select a final model. Practitioners will use numbers (in diagnostic and statistic tables) or visuals (like a residuals plot) when comparing models.\nconfirmatory data analysis: In the best case, at least according to the ‚ÄúOpen science‚Äù framework, the final statistical model will have been selected and preregistered before data is even collected. Statistics have to be rigorous to mean anything, a fact which the machine learning people (who do only exploratory work) ignore. They don‚Äôt check model assumptions using statistical tests like the Shapiro-Wilk normality test, and they are constantly ‚Äúp-hacking‚Äù and ‚ÄúHARKing‚Äù to forcibly extract findings from their data. Confirmatory data analysis rejects these practices, aiming instead for statistical models that are pre-specified (pre-registered), theoretically-based, and rigorous (whatever they take that to mean).\n\nI‚Äôm often somewhat flippant about the second approach, which suggest and attempts to discover Truth where I am skeptical it exists. In any case, the second approach is the only one that is taught in statistics courses. This is a mistake. Firstly, exploratory data analysis is much more commond. Secondly, it is easier to get started doing exploratory (rather than confirmatory) data analysis. Because both of these approaches adopt many of the same tools, it seems to me that starting with exploratory data analysis and trying to make that make sense is the most effective way to learn confirmatory data analysis (which will require additional effort and research that I can‚Äôt provide).\nThus, I intend to provide you with a strong understanding of data that you can directly apply to exploratory data analysis tasks. My hope is that this understanding will enable you to complete a diversity of tasks, including confirmatory data analysis, if that is of interest.\n\n\nLearning Objectives\nThere has to be some boring stuff because pedagogy. I have quite a few learning objectives for you, forming one big list, but I‚Äôll attempt to section them off so they are easier to read.\n\nR Learning Objectives\n\ndiscuss R as a language with a history: use knowledge about the history of R (and of scientific computing more generally) to describe what R is, what people ‚Äúsay‚Äù in this language, and why this language has the properties and characteristics that it does.\nR competence: read R expressions written by others (allowing the language to serve a communicative purpose), and write R expressions that are readable and align with best practices within the open source R community.\nR‚Äôs friends: Describe R‚Äôs relationship to RStudio, RMarkdown, Shiny, Quarto, and Tidyverse; and, describe what each of these tools is and why someone would use them.\n\nrun R code in several different ways: via the console, a script, and Quarto or RMarkdown documents.\n\ndescribe R‚Äôs data types and the use of each: strings, numerics (floating point ‚Äúdoubles‚Äù, integers, and complex numbers), logicals, datetimes, and factors\ndescribe R‚Äôs data structures and the use of each: including, vectors (1-dimensional arrays), matrices (2-dimensional arrays), arrays (more than 2-dimensional arrays), lists (key-value pairs), data frames, and tibbles\naccess R documentation, and read it effectively enough to solve a problem\n\n\n\nComputation Learning Objectives\n\ngenerate synthetic data: use simulation of simple events (like the rolling of dice or flipping of a coin) to gain visual intuition for the central limit theorem and the law of large numbers\nuse the Monte Carlo simulation framework to evaluate statistical tests (e.g., by determining what happens when assumptions are violated)\nprocess string data: convert strings to all upper or lower case, add prefixes or suffixes, splitting strings apart\nprocess numeric data: scale and center numeric data and write functions to accomplish non-standard transformations\nprocess language data: apply the principles of natural language processing to pre-process text data (by tokenizing and stemming text and describing both of those processes and why they are used)\n\n\n\nStatistics Learning Objectives\n\nnull hypothesis significance testing: use R to perform null-hypothesis significance tests, such as the one-sample, two-sample, and repeated measures t-test\nregression: use R to perform linear and logistic regressions, including regressions with polynomial terms\nmachine learning: use R to perform a more complicated machine learning task, likely by constructing a decision tree and a random forest classification model\ndimensionality reduction: perform principal component analysis and construct a latent semantic space, and explain why these two seemingly distinct methods are connected by singular value decomposition\n\n\n\nData Science Learning Objectives\n\nrectangular data: use the tidy data framework to read, write, and pre-process rectangular data in a consistent, efficient, and minimally complex manner.\n\nimport data from a variety of sources including: comma-separated values (.csv) files, excel spreadshees (.xlsx files), google sheets spreadsheets\n‚Äòtidy‚Äô data into the following format: one observation per row, one variable per column, one value per cell\nuse available tools that enable you to store data in a very consistent format with very little effort\n\nlanguage data: use R to pre-process, analyze and visualize text data\nvisualization: use R and the grammar of graphics (represented by ggplot2 and related packages) to visualize data and to share data visualizations with others\npublication: use RMarkdown or Quarto to conduct a linear or logistic regression, and to report and interpret the results of those tests\n\n\n\n\n\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem.‚Äù Journal of Math 58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_lore.html",
    "href": "01_lore.html",
    "title": "1¬† The Lore",
    "section": "",
    "text": "1.1 Early 19th Century: the birth of programming\nThe idea of a programmable computer is not difficult to understand. The primary goal is to make a machine that you can use for different tasks, depending on what program you feed to that machine. Programs are written in code, which today is stored as text files on a computer. The programmable machine also lives within the computer, so running the program is as simple as telling the machine part of the computer where the program is located; then, the machine part of the computer tries to read find the and read the program at the specified location, and the (hopefully) program runs.\nFolks in the 19th century did not have access to digital text files, and so they could not write their programs on them. How did they write programs, and which sort of programs did they write? These are the primary questions I hope to address in this very first section?",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#early-19th-century-the-birth-of-programming",
    "href": "01_lore.html#early-19th-century-the-birth-of-programming",
    "title": "1¬† The Lore",
    "section": "",
    "text": "1.1.1 Jacquard‚Äôs Loom: the punched card\nOne of the first programmable machines was Jacquard‚Äôs loom, which Jacquard patented in 1804. Jacquard was a weaver. Jacquard‚Äôs loom was a machine that could create a variety of different patterns, depending on which program was put into it?\nSo, Jacquard wrote programs to produce beautiful woven fabrics, but more important is how he wrote programs. Jacquard had no text files, so instead he developed a different form of machine input: the punched card. Each line on Jacquard‚Äôs punched cards contained information about a single row of the design. The cards could be fed sequentially into the loom to produce a large pattern.\n\n\n1.1.2 Babbage, Lovelace, and the Analytical Engine\nGeneral-purpose digital computers, the sort of computers that can run R, emerged as an idea in the early-to-mid 19th century. Up to that point, computers were either mechanical (mechanical computers are fascinating, by the way) or just humans.\nOne of the first to develop a design for a general-purpose computer was Charles Babbage, working in the early part of the 19th century. In the 1830‚Äôs he proposed a massively complicated, general-purpose, steam-powered computer, which he called the analytical engine. The computer was only capable of carrying out the four basic operations of arithmetic: addition, subtraction, multiplication, and division; it was designed to take input via punched card, just like Jacquard‚Äôs loom.\n\n\n\n\n\n\nLady Lovelace\n\n\n\n\n\nDuring the 1830‚Äôs and 1840‚Äôs, Lady Ada Lovelace communicated with Charles Babbage (and several others involved in similar work) with the intention to collaborate with him in studying the analytical engine. It was Lady Lovelace who wrote the first substantial computer program, whose purpose was to compute Fibonacci numbers (Tibees 2020). Her program, written in the iconic note G, used only the four simple arithmetic operations.\nLovelace was interested in discovering the capabilities of the analytical engine. Her program computing Fibonacci numbers was important because it used loops in computation. Lovelace, daughter of the poet Lord Byron, was also interested in non-mathematical applications for the machine. She suggested that a sufficiently mathematical theory of sound could enable to engine to compose complex and scientific symphonies (Tibees 2020). Isn‚Äôt that beautiful!",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#middle-and-late-19th-century",
    "href": "01_lore.html#middle-and-late-19th-century",
    "title": "1¬† The Lore",
    "section": "1.2 Middle and Late 19th century",
    "text": "1.2 Middle and Late 19th century\n\n1.2.1 1830-1870(ish)\nThe middle of the 19th century was a period of massive global shifts. Liberation from enslavement was spreading across the globe after the Haitian revolution left white men horrified at the peculiar institution, which was also losing economic utility (see Capitalism and Slavery by Eric Williams).\nAlso within that 40 year period was:\n\nabolition (as a matter of law, anyways): British Empire (1834), French Empire (1848), Russian Empire (1861), Dutch Empire (and Dutch East India Company; 1863-73), American Empire (1865), Portuguese Empire (1869)\nSamuel Colt‚Äôs invention of a revolver that can be mass-produced (1836?)\nthe development of the telegraph (1830s)\nthe trail of tears (starting 1836)\nthe revolutions of 1848 and the publication of the communist manifesto\nthe first woman‚Äôs rights convention in the U.S. (Seneca Falls Convention, 1848)\nthe discovery of the Bessemer Process which enables the mass-production of steel, paving the way for emerging steel tycoons (1855)\nDarwin published On the Origin of Species (1859)\nGatling‚Äôs invention of the machine gun (1861)\nMaxwell publishes his equations, proposing an incredibly successful theory of physics that understands electricity, magnetism, and light as essentially the same thing (1861)\nthe construction and openning of the Suez Canal (1860‚Äôs)\nMendel‚Äôs publication of his laws of genetic inheritance (1865)\nthe discovery of the cell and subsequent elaboration of cell theory (1865 and after)\nNobel‚Äôs invention of dynamite (1867)\nMarx‚Äô publication of the first volume of capital (1867)\nthe completion of the transcontinental railroad (U.S., 1869)\nMendeleev‚Äôs publication of the first periodic table (1869)\n\nIn this revisionist history of the computer (and ultimately of R), this period in history marked a transformation of power. The structure and organization of society was changing along with the flow of people, ideas, and commerce. Western, liberal democracies had to develop new technologies of population control in order to prevent all of these liberal changes from challenging their position of authority and power.\n\n\n1.2.2 Late 19th century\nWith the relative liberation of black bodies (and other bodies, as well) came a scientific imperative. Power continued to demand that these bodies be inferior, but evidence of inferiority was no longer to come from the conditions and dimensions of the body. Nay, the newly-available technologies of genetic inheritance and natural selection allowed a regime of a new flavor to take hold, one that cited hard science to support and justify the inequities in society. Inferiority was moving through the skin, into the body, and - importantly - into the mind.\nWilhelm Wundt opened the first psychology lab, and William James delivered the first psychology course and textbook. Galton, who was studying intelligence, popularized the idea of the median (Bakker and Gravemeijer 2006). Psychology and with it psychological statistics, was beginning to take shape to meet the new demands of the state: a theory and a technology that will find permanent, internal traits upon which to stratify society into haves and have-nots. The story of the emergence of psychological statistics is incomplete without mention of eugenics. The tools being developed were not neutral and scientific, but overtly political, aimed at achieving the goals of the state.\nAlso in the late 19th century was what Foucault called the implantation of perversions (Foucault 1978) - the creation of new symbolic threats to the body and to society as a whole. This operated through the invention of new characters that continue to exist within society today.\nFirstly, there was the medical specification of the homosexual (Townsend 2011). This began in 1864 with the work of Karl-Heinrich Ulrichs, who was gay himself. He specified men as either urnings or dionings. Urnings and Dionings are both male-bodied creatures, but the urning experiences the desires and character of a female (Townsend 2011). The dioning, by contrast, is normal. Discourse about the urning (renamed to the invert, and then to the homosexual) continued well into the 20th century, and the sissy (the archetype the invert represents) is, obviously, still with us.\nAlso within this time period, was the medical specification of the hysteric woman, which was initially the perogative of Jean-Martin Charcot.\nI‚Äôll mention just one more character that was invented in the later 19th century. For all of American history to this point, immigration law was about the process of naturalization - immigrants becoming citizens. From the beginning of the union, only white men of ‚Äúgood moral character‚Äù were allowed to become American citizens (Naturalization act of 1790?). There was little effort to actually prevent bodies from entering the country.\nUntil 1875. With the passage fo the Page Act of 1875, the United States declared its intention to keep undesirable bodies out of the country for the first time. Shortly thereafter, the ‚Äúillegal alien‚Äù was invented as a result of the Chinese Exclusion Act of 1882, which is the only American immigration law I am aware of that names a specific national group in its title.\nAll this to say that the nature and enforcement of undesirability were in massive flux in the late 19th century. The foreign element was moving within: the enslaved African could become a citizen and could vote, the invert or the hysteric could be hiding within anyone, and the state took up the power to deport bodies that did not belong. No longer was the anthropologist writing about the inferiority of foreign peoples (although to be clear, they absolutely were still doing that); the pschiatrist was now writing about our own inferiority.\nI consider the birth of statistics to be in this time period, which does not have pleasant implications for statistics as a field. There is a lot more to be said about the advent of statistics, and how statistics is designed to serve power (i.e., fulfill the demands of the state). However, I‚Äôm going to leave all of that unsaid and refocus on computation in general, and statistical computing in particular.\nThe late 19th century was also, notoriously, the era of massive trusts in the United States. These monstrous, monopolistic companies exploited both the consumer and the worker, but the United States did not yet have a legal mechanism for breaking them up. The most important monopoly for our purposes: the one that is most influential is the development of S and then R is the AT&T monopoly.\nAnother monopoly was also forming. Using Jacquard‚Äôs punched cards, an American man designed and patented a system to read punched cards. In 1890, this punched card system was used to complete the census, resulting in the 1890 census being completed two years quicker than the 1880 one. The company that developed this technology would go on to become IBM, which enjoyed monopoly status in the computing industry for several decades.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#sec-early-19th-century-the-advent-of-computing",
    "href": "01_lore.html#sec-early-19th-century-the-advent-of-computing",
    "title": "1¬† The Lore",
    "section": "1.3 Early 19th century: the advent of computing",
    "text": "1.3 Early 19th century: the advent of computing\nNear the end of the 19th century, a mathematician named David Hilbert decided that mathematics needed to be formalized. Up to that point, it had developed as myriad sub-disciplines that failed to cohere into a single, interconnected web of mathematics. Hilbert believed that it should, and his goal was to formalize this system. He believed that such a system (of mathematical axioms) needed to have three properties:\n\nto be consistent: it should not be possible to derive that a statement is both true and false\nto be complete: it should be possible to derive the truth of every true statement (or the falsity of its negation)\nto be decidable: there must be an algorithm that can identify all and only true statements in a finite number of steps.\n\n(The excitement about formalizing affected Hilbert, but by no means was he the first or the only to be caught up in this mess. Notoriously, Whitehead and Russel got spun up enough to publish a 126-page long proof that \\(1+1=2\\). I‚Äôm mostly attributing these three demands to Hilbert for sanity‚Äôs sake because I cannot stand to write out the sordid details. These three ‚Äúproperties‚Äù as I call them, are really inspired very loosely on any specific, cite-able Hilbert publication. He did publish a list of 23 questions, which refer to the properties I mention here, but understand this as a drastically over-simplified view of the mathematical debates unfolding at the time.)\nMathematics was not the only field to be heating up. There was growing speculation in physics that matter may not be as continuous as was previously assumed. In 1900, Max Planck published the first quantum theory in physics, which was aimed at modelling thermal radiation. Shortly thereafter, Albert Einstein published another quantum theory, this time aimed at modeling the the photoelectric effect. Both of these models used quantum stuff (i.e., minimal, discrete units of energy, creating measurements of energy that are always a multiple of the quantum unit), but the authors did not actually believe the world was quantum. Famously, Einstein‚Äôs theories of relativity both rely on space-time being continuous. They merely believed quantized math was the best way to explain non-quantum physical phenomena.\nNeils Bohr went the whole way, creating his model of the atom, with distinct, orbital electron shells. In the 1920‚Äôs quantum mechanics, as we know it today, came into existence. It did not make Einstein happy. Einstein wanted a deterministic world, where each cause has an specific, reliable effect. Quantum mechanics is not a deterministic theory of physics, but a probabilistic one. I take this diversion into the physical sciences not only to stress that this is a transition period within the physical sciences, but to temper my claim from the previous section. The ‚Äúdemands of power‚Äù did no less to supercharge the development of statistics and probability than did rapid changes in the way we understand and model the physical world.\nDuring my quantum mechanical tangent, G√∂del has proven that achieving the second property of Hilbert‚Äôs idealistic system is unlikely. In fact, G√∂del establishes that it is logically impossible that any formal mathematical system could be complete, as defined above.\nTo answer the question about whether mathematics is decidable, a new technology is needed. Before a mathematician can make formal claims about the capabilities or limitations of algorithms in general (as Hilbert demanded), she must first provide a rigorous definition of an algorithm. Two mathematicians took up this task, Alonzo Church who developed the lambda calculus, and Alan Turing who developed the Turing machine. Both men reached the same conclusion: mathematics cannot be decidable. It is logically impossible to make an algorithm (a Turing machine) that can identify all and only true statements (Turing 1936). There are, as it turns out, hard limits on the types of problems algorithms are able to solve (at least in a finite number of steps).\nThus, Turing half accidentally created the field of computer science while trying to answer a question about the foundations of mathematics. This is also an opportune time to introduce the term Turing-complete which refers to anything (model of computation, programming language, a book of instructions used by a human computer) that can simulate the a Turing machine. Any Turing-complete system is essentially equivalent to the original Turing machine described in (Turing 1936). The analytical engine is (theoretically, of course, it never got built) Turing-complete; Jacquard‚Äôs loom, by contrast, is not. Modern programming languages are, for the most part, Turing complete, meaning that any function you write in a modern programming language could be performed on the OG Turing machine from (Turing 1936).\nThe first electric, digital computer was not fully constructed until 1945. It was built by and for the U.S. military, who named the machine ENIAC. Thus, the first computations done on an electric, digital computer were intended to speed up the process of human and earthly destruction. ENIAC was a bunch of coordinated units that ran according to the placement of wires on the machine (Shustek 2016). The machine took IBM punched cards as input (remember the punched card monopolist from the end of the 19th century?).\nInitially, the wires on ENIAC had to be moved for each new problem (Shustek 2016). The process of re-configuring the machine for each new problem was tedious, but it was possible, and so ENIAC was Turing-complete. However, having to physically move wires prevented the machine from achieving the utility of a modern programmable computer.\nThis machine was very quickly modified in a way that dramatically changed its function. Instead of having to move wires, and then feed the machine (punched card) instructions based on the position of those wires, it would be much faster permanently code instructions (functions) into the machine. Then, the input of the machine could describe the sequence of functions. You could achieve looping by instructing ENIAC to perform a function repeatedly and conditional (if-statement) execution by instructing ENIAC to skip functions in the sequence.\nThis is the idea behind modern programming languages. Instructions for the computer, written in the computer‚Äôs language (ENIAC‚Äôs language was wires, the one we‚Äôll soon focus on is R) are stored within the machine. ‚ÄúProgramming‚Äù the machine involves telling it which instructions to perform and in which order. In 1948, the first ENIAC ‚Äúprogram‚Äù ran under this new computer architecture was a Monte Carlo simulation of neutron decay during nuclear fission (Shustek 2016).",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#att-bell-labs-and-s",
    "href": "01_lore.html#att-bell-labs-and-s",
    "title": "1¬† The Lore",
    "section": "1.4 AT&T, Bell Labs, and S",
    "text": "1.4 AT&T, Bell Labs, and S\nMonopolies suck, and AT&T did as well. Throughout the beginnning of the century, it gradually became clear that the benefits of a monopolist teleohpne provider were not going to materialize. In 1949, the U.S. Department of Justice sued AT&T for violating the anti-trust act, and the resulting 1956 consent decree prohibited AT&T from entering the computer business (Chang, Hen, and Kan n.d.).\nThis consent decree did not prevent the further degradation of AT&T‚Äôs service, nor did it prevent future anti-trust lawsuits. Throughout the 60‚Äôs and early 70‚Äôs, the U.S. government dogged AT&T with recurrent anti-trust lawsuits. In 1974, the Department of Justice began their final lawsuit against a monopoly AT&T. Over the course of the next decade, the government proved that AT&T was leveraging its monopoly power to predatory ends, annihilating potential competitors and pricing services far beyond the cost to provide them. This lawsuit ended in 1982 with the dissolution of AT&T into 7 Regional Bell Operating Companies (Chang, Hen, and Kan n.d.).\nBell Labs was probably the most important laboratory within AT&T. During the early 70‚Äôs, the researchers at the statistics research department within Bell Labs was using the programming language FORTRAN. FORTRAN is a general purpose, compiled programming language, developed by IBM (the punched card guys).\n(This isn‚Äôt super relevant, but I think it‚Äôs fun. Before 1968, computational statisticians had been using a algorithm called RANDU, which was a FORTRAN function that generated random numbers. In 1968, a mathematician proved that the allegedly random numbers actually all had to lie on a series of parallel hyper-planes, and we thus not actually random. wtf is a series of parallel hyper-planes? See below)\n\n\n\n\n\n\n\n\n\nFORTRAN, the name, stands for ‚Äúformula translation,‚Äù and it was primarily used for scientific computing, like computing weather models or doing computational physics - things that have to do with numbers, essentially. It is still used in these fields to some extent, although it is less popular for scientific computing than other, more recent programming languages, like R. FORTRAN is, computationally speaking, incredibly efficient, mostly by natively supporting parallel computation. For this reason, FORTRAN is still used to benchmark supercomputers. You can learn more about FORTRAN on its website.\nIn any case, in the 1970‚Äôs, the statistics research department at Bell Labs found FORTRAN to be somewhat insufficient, and they set out to develop a new language that would more fully suit their needs (Becker 1994).\n\n1.4.1 S\nS is a statistical computing language that was developed first at Bell Laboratories in the mid 1970s. At the time, statistics was undergoing a change. Previously, statistics had been developing as a set of methods - essentially algorithms that prescriptively described how to complete a statistical analysis from beginning to end. In the early 70‚Äôs, John Tukey was working at Bell Labs and at Princeton, and he was making a lot of noise about the problems with statistics. He popularized a different approach to statistics, establishing something of a binary between data analysis and statistics, just as I did between machine learning and statistics (in Section 1.2; Tukey (1972)).\nThe statistics research department was beginning to demand a tool that aligned with Tukey‚Äôs approach. FORTRAN, developed more than a decade before that demand was created at and by Bell Labs, did not measure up to the task. Instead, they decided to develop a new language, which they named S. Initially, there was a large focus on being able to import FORTRAN functions into S, so that there could be a smooth transition from FORTRAN to S within Bell Labs.\nS was built from the ground up to include graphics capabilities, and a structure that enabled and encouraged exploratory data analysis. The basic data structure in S is a vector of like-elements, which were used to make matrices and time-series; S also included lists (key-value maps) and the $ operator, which could be used to retrieve specific components of larger data structures (Becker 1994). It also included all of the arithmetic operators that you need in a desk calculator, making it useful for that purpose, as well.\nIn 1980, S was distributed outside of Bell Labs for the first time. Initially, it was distributed for a nominal fee and for educational use only, but by 1981 it was widely available (Becker 1994). After it began to be distributed, the developers added explicit looping (i.e., for loops), as well as the apply function, which could be used to loop over a vector while applying a function (Becker 1994). The developers also introduced the ‚Äúcategory‚Äù, which is now called the factor in R. Categories are vectors of data. They merge numerical and string data types - each entry in the vector is assigned a category label (so that you can read it), as well as a underlying integer (so that you can do math with categories).\nAlthough S was developed initially by statisticians, it clearly had utility as a data manipulation, graphics, and exploratory data analysis tool. In 1988, the developers released the ‚ÄúNew S,‚Äù renaming the software after some significant changes. The most significant feature of New S was the inclusion of first class functions, which are functions that you can assign to a name and and then refer to by that name. Functions are first class in that they are S objects, just like any vector or matrix. For the first time, S had depreciated functions, which R also has. Depreciated functions are functions for which there is a better alternative. They are generally still included in R and S distributions (so old code that uses depreciated functions can still run), but it‚Äôs best to avoid using them (and to use the better alternatives instead). By 1988, many of the FORTRAN functions from the initial development of S were rewritten in C, which is a general purpose programming languages on which New S is built (Becker 1994).\nIn 1991, the S development team expanded, and there was a focus on adding statistical software to the S language. Although S was developed by statisticians who intended to use it for statistics, the statistics are not inherent in S: ‚ÄúS is a computational language and environment for data analysis and graphics‚Äù (Becker 1994). As such, the developers added the formula class, which could be used to specify statistical models. The formula is marked by the ~ operator, with the dependent variable on the left and the independent variable(s) on the right (e.g., y ~ x + w + x*w).\nAlso in the 1991 release was the data.frame. Matrices are like vectors: they can only have one type of data. If you have a matrix that has even one number in it, then the entire matrix must be numeric, even if you want to use it to represent string data (like names and job titles) or categorical data (Becker 1994). So, a matrix is a combination of multiple vectors, all of the same type. A data.frame, by contrast, is a combination of vectors of any type. You can have a string vector (column) in the data frame representing job title, as well as a numeric vector representing income. As with matrices, you can use the $ operator to pick a vector out of the data frame (e.g., data$income picks out the income vector in the data frame called data).\nIt‚Äôs not really possible for a programming language to die. As we have seen with FORTRAN and S, new programming languages often use code from their older counterparts, especially at the beginning. Even though I can no longer find S on the internet and run it on my computer, a very large number of S functions continue to exist in R.\nI am able to find relatively scant documentation about this final period in the history of S, so the rest of this section is at least somewhat speculative (except claims that are cited, of course).\nWhat is the need for R if S exists? Well, well, well. Let‚Äôs talk about corporate fuckery, which both killed S and prevented it from dying. I have been making a much bigger deal over anti-trust law than the vast majority of those who introduce R to their students. To this point, as far as AT&T and Bell Labs are concerned, I have presented a world in which they are legally prohibited from selling computers (and presumably, computer software) as a result of the Consent Decree from 1956.\nUp to this point, no one was making money off of S. Although Bell was initially charging folks a nominal fee to use the software (Becker 1994), this practice ended quickly, meaning that the software was being distributed for free. As a result of the anti-trust, Bell Labs was not going to monetize this technology. Instead, one of their former employees had to do it.\nIn the 70s and 80s, the graphical user interface (GUI) was being born. This emerging technology came with a new generation of capitalists who had not been subject to extensive anti-trust, in which former trade union president Ronald Reagan did not believe - the capitalists who bring us Microsoft and Apple, who own outright the operating systems of about 85% of the worlds‚Äô computers (and many phones and other devices, as well).\nS wasn‚Äôt fated to become Windows; it was fated to become S-PLUS. S-PLUS is/was a statistical computing software with a graphical user interface. It was developed by a company owned by a former Bell Labs employee and University of Washington professor, R Douglas Martin. His work is primarily in econometrics, and he has extensively published about investment risks. Because of this, and because S-PLUS was and is mostly used by economists, a cynic might call it an application to be used for those who are unwilling or unable to learn how to code (similar in character to Microsoft‚Äôs SPSS). S-PLUS started circulating (for a fee) in about the year 1987, and it did include features that S did not (like generalized linear models).\nLet me just quickly recap, so I can make sure everyone is oriented in time - I‚Äôm discussing a lot of events that overlap and are not all well documented. In 1980, S was released to the public; in 1988, S had a significant update, becoming ‚ÄúNew S‚Äù; around 1987, a former employee of Bell Labs developed S-PLUS; in 1991, S had an update that focused on statistics.\nIn 1991, two statisticians quietly began work on the project (R) that would more-or-less kill S and S-PLUS.\nIn 1993, S and S-PLUS were reunited when Bell Labs sold S to the company that had developed S-PLUS. That company, in turn, immediately merged with a company called MathSoft. S-PLUS was only available on windows, and its relationship with Microsoft strengthened when features were added to connect S-PLUS to Excel and to SPSS.\nPart of the company (MathSoft) was sold, it got renamed (to Insightful), the exclusive license to distribute S turned into AT&T (i.e., Lucent, one of the companies that remained after AT&T) selling S so that it became the property of Insightful. Then Insightful got bought by a company called TIBCO, and then‚Ä¶\nI think you get the general idea. S and S-PLUS got bought, and sold, and licensed, and merged, and acquired to the point that it no longer really exists in any meaningful, public way. But by the 2000s, that didn‚Äôt matter.\n\n\n\n\n\n\nBakker, Arthur, and Koeno P. E. Gravemeijer. 2006. ‚ÄúAn Historical Phenomenology of Mean and Median.‚Äù Educational Studies in Mathematics 62 (2): 149‚Äì68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. ‚ÄúA Brief History of S.‚Äù In Computational Statistics, edited by Peter Dirschedl and R√ºdiger Ostermann, 81‚Äì110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. ‚ÄúCase Study 1: AT&T Divestiture.‚Äù Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nShustek, Leonard J. 2016. ‚ÄúProgramming the ENIAC: An Example of Why Computer History Is Hard.‚Äù May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. ‚ÄúThe Medicalization of ‚ÄòHomosexuality‚Äô.‚Äù Honors Capstone Projects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTukey, John W. 1972. ‚ÄúData Analysis, Computation and Mathematics.‚Äù Quarterly of Applied Mathematics 30 (1): 51‚Äì65. https://doi.org/10.1090/qam/99740.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem.‚Äù Journal of Math 58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "02_r_and_friends.html",
    "href": "02_r_and_friends.html",
    "title": "2¬† Modern R",
    "section": "",
    "text": "2.1 Initial Development of R (1990s)\nIn 1991, two professors in New Zealand began to develop R, a process which they documented in (Ihaka and Gentleman 1996). R is very similar to S; so similar, in fact, that it is frequently called a dialect of S. What is the difference between S and R? The creators of R describe it as having the syntax of S (meaning that most examples, including the following example, can be run in S or R) but the semantics of Scope (which is a programming language from the Lisp family).\nProbably the key difference between the two languages is the lexical scoping. Whenever you use R (or most other programming languages), you have to have something called a frame. Frames include things like functions and named variables. Each function creates its own frame. The frame for the function f in the example below (from Ihaka and Gentleman (1996)) contains the named variable y and the named function g. The named function g, as a function, creates its own frame (in which to store variables and functions). There is also something called a global frame which, in the following example, includes an assignment of the value 123 to the name y and the assignment of some function to the name f.\nAs you can see, in R, running the function f with 10 as an argument results in the function returning 100 (10 times 10). In S, this very same code would have resulted in function f returning the value 123. In S, when we define the function g, S uses the global frame as the basis for the function, including the assignment of the value 123 to y. R, by contrast, creates g with a locally-scoped frame, meaning that the frame for g includes the assignment of the value x * x to the variable y (assignments which are inherited from the parent frame). Thus, in S, the function g is evaluated as print(123), but the R function is evaluated as print(x * x) (the function f is responsible for substituting x to make print(10 * 10).\nUnlike Scheme, but like S, R uses lazy evaluation. In essence, this means that R does not run your code unless it absolutely has to. I‚Äôll use the example of Figure¬†2.1 to explain what this means. Lines 2 through 6 contain the declaration of function f (even though line 4 also contains the declaration of function g. When you run line 2, all of the lines down to line 6 (where the closing bracket, } is located) get stored in R‚Äôs memory next to the name f. However, R will not run the function f until you actually go to use it (i.e., until you make the function call in line 7). This is why we call R lazy, but what‚Äôs the big deal?\nIf you make a syntax error in your declaration of function f, R is going to have to tell you that you made a syntax error at some point. In a language that is not lazy, the language evaluates function f when you store it. Thus, a non-lazy language will send you a syntax error after you run the function declaration (i.e., after you run line 2, which also causes lines 3-6 to run). If Figure¬†2.1 were written in a non-lazy language, the syntax error would occur where the 1 annotation is. However, in R, the function is merely stored when you run lines 2-6. Function f does not actually run until you call it in line 7 (marked with a 3 in Figure¬†2.1). Laziness is a feature that R inherited from S, which is also lazy.\nThis is getting a bit technical. The two men who developed R are Ross Ihaka and Robert Gentleman. On a family tree posted on Ihaka‚Äôs personal website, he lists himself as the academic grandchild of John Tukey, then statistician at Bell Labs that popularized exploratory data analysis (the framework that created the need for S, which was also, if you‚Äôll recall developed at Bell Labs).\nIhaka is a now retired statistician from the University of Auckland. Gentleman is a bioinformatician who currently works at Harvard and 23andMe. Allegedly, Ihaka and Gentleman developped R for use in teaching statistics. That was part of both of their jobs as professors, after all. However, this doesn‚Äôt seem very plausible (why would the professors write their own statistical programming languages rather than using a well-documented one which would seem to be better for pedagogy), nor have I seen any specific evidence for it. That being said, in the years 1993-94, R was stuck at the University of Auckland, being used by them, probably their peers, and less probably their students, but the software was not yet being distributed, as S or S-PLUS was.\nIn 1995, one of their colleagues convinced them to licence use of the software as free software under a GNU general public license.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Modern R</span>"
    ]
  },
  {
    "objectID": "02_r_and_friends.html#initial-development-of-r-1990s",
    "href": "02_r_and_friends.html#initial-development-of-r-1990s",
    "title": "2¬† Modern R",
    "section": "",
    "text": "y &lt;- 123 # assign 123 to the name y\n1f &lt;- function(x) {\n  y &lt;- x * x # assign x times x to the name y\n2  g &lt;- function() print(y) # create new function and new scope\n  g() # return the output of function g\n}\n3f(x=10)\n\n\n1\n\nthe the beginning of the declaration of function f (between lines 2 and 6)\n\n2\n\nthe declaration of function g\n\n3\n\na function call for function f (with the argument x set to equal 10)\n\n\n\n\n\n\n\n\n[1] 100\n\n\n\n\nFigure¬†2.1: an example from Ihaka and Gentleman (1996)\n\n\n\n\n\n\n\n\n\n\n2.1.1 Free Software\nI just bolded the term free software; why? As it turns out, the term free software has a specific definition that extends far beyond the idea of ‚Äúsoftware that you don‚Äôt have to pay for.‚Äù So what is free software? Free software is characterized by the four freedoms (Foundation n.d.):\n\n\n\nFree Software Foundation‚Äôs Four Essential Freedoms\n\n\nThe idea of free software, and it‚Äôs formation in the four freedoms seen above, came to be popular in the mid-80‚Äôs after the Reagan government had made clear it‚Äôs stance (and the republican, and soon the democratic, party‚Äôs stance) on anti-trust enforcement. In the wake of the disruption to the computing (IBM) and telephone (AT&T) industries, the Reaganites declared that we were entering into an era of free, competitive trade while setting up a regulatory framework that would allow tech companies to consolidate power and market share ad infinitum, resulting in the current big 4(-ish): Apple, Alphabet (Google), Amazon, and Meta (and Microsoft, Nvidia, and potentially Tesla and like Netflix, depending on who you ask).\nIt is a good thing for us, then, that none of these companies own R, which the developers have promised will remain free software indefinitely.\n\n\n2.1.2 Open Sourcing and Crowd Sourcing\nThese days, it feels like only a real purist will call R free software. The more en vogue term is ‚Äúopen source.‚Äù The Free Software Foundation would like you to treat the terms as separate however (see this article).\nIn reality, the labels ‚Äúopen source‚Äù and ‚Äúfree software‚Äù are mostly synonymous in that they refer to many of the same software. As Freedoms 1 and 3 make clear, software must be open source before it can be free. The free software folks‚Äô biggest problem with ‚Äúopen source‚Äù is one of semantics, really. They claim that the ‚Äúopen source‚Äù movement argues too much about how free software is good for business and software development (i.e., because curious users can look for and find bugs). The free software people are not interested in these practical matters, focusing instead of the moral question of what sort of software is right and wrong. They correspondingly argue their case in the form of moral imperatives (the four freedoms).\nI am less interested in these theoretical questions, and more interested in explaining to you what the implication of free or open software is bound to be (at least in the case of R): crowd-sourced development.\nR itself provides you with basic statistical functionality. However, the vast majority of what is commonly called ‚ÄúR‚Äù is not actually part of the base distribution of R. Instead, most of the functionality is packaged within ‚Äúpackages,‚Äù which are you load into R with the library() function, as shown below:\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe tidyverse is really a ‚Äúmeta‚Äù-package (as is the tidymodels package, by the way). This means that it is a package that contains a bunch of other packages. What, then, is a package? A package is a collection of functions that someone has written and made available online for your (free) use. An example is the ggplot2 package, which is included in tidyverse. We‚Äôll be using it later.\n\n2.1.2.1 Where are packages and how many are there?\nI can answer the first question, which will reveal why an accurate answer for the second is impossible. CRAN is the Comprehensive R Archive Network. CRAN stores a repository of several thousand R packages. When you use the function install.packages(), you are dowloading packages from CRAN. However, there are also R packages that are not on CRAN.\nThe Bioconductor project is another repository of R packages (and also just a regular project, started by R co-creator Robert Gentleman). Bioconductor stores packages for a specific purpose, and that purpose is not ours, so we can safely ignore it. Packages can be (and frequently are) stored on GitHub, or similar online repositories. The devtools package has the functions you need to get packages from these other repositories (i.e., devtools::install_github(); and also to install packages stored on your computer).\nFinally, there are a multitude of packages that are stored on people‚Äôs personal websites, and other non-repository internet locations. Installing (or counting) these packages is trickier than the others, but if all else fails, you can download the package from the internet and install it using devtools::install_local().\n\n\n\n2.1.3 Wickham‚Äôs Tidyverse\nLet‚Äôs return to the tidyverse package, or more precisely, the packages within. They are:\n\nggplot2 for data visualization\ndplyr for data manipulation\ntidyr for data tidying\nreadr for data import\npurrr for functional programming\ntibble for tibbles (a modern version of data frames)\nstringr for string manipulation\nforcats for factor manipulation\n\nTidyverse also includes a number of non-core packages, such as haven (for importing SPSS, Stata, and SAS files), lubridate (for data manipulation), magrittr (for piping), readxl (for importing Excel files), modelr (for some, limited modeling tasks), reprex (for producing reproducible examples), rvest (for scraping web pages), and xml2 (for reading XML files). All of these packages (all 16 of them) were written (or co-written) by Hadley Wickham. Hadley also wrote blob, which is not in the list because I don‚Äôt know it does. In the tidyverse, there are also two packages which Wickham did not write, those being jsonlite (for reading JSON files), and glue (not sure what this one does, either).\nWhen you run library(tidyverse), as I did above, you load all eight of the core tidyverse packages. The others will not load, however. If you want to use a non-core package, you will have to load it separately or refer to it in the form package::function(). Let‚Äôs take an example from the lubridate package, which is not in the core tidyverse. lubridate has a function called now(), which returns the current date and time. Trying to use this package without loading lubridate is equivalent to telling R to use a function without telling it that or where it exists. If you want to use this function, you will have to load the lubridate package with library(package), or refer to it in the form package::function(). Both of these methods will allow R to find locate and run the now() functions.\n\n# R won't throw an error if we specify that now is from the lubridate package\nlubridate::now()\n\n[1] \"2024-05-25 09:15:42 EDT\"\n\n# we could also load lubridate and just use now normally\nlibrary(lubridate)\nnow()\n\n[1] \"2024-05-25 09:15:42 EDT\"\n\n\nIf you wanted to unload a package (for some reason), you could use the unloadNamespace() function. (A namespace is just the collection of functions in the package, each of which is associated with a name, like now or even unloadNamespace).\n\n\n\nHadley Wickham\n\n\nThis is my friend Hadley Wickham. He is a god from New Zealand. He got his bachelors in biology at the University of Auckland (which you may recall as the home university of Ross Ihaka), where he also got a masters in statistics; then, Iowa State University gave him a PhD in 2008. His PhD thesis was called ‚ÄúPractical tools for exploring data and models,‚Äù but I would probably read some of his other work (particularly R for data science) first/instead. He lives in Texas with his husband and dogs.\nHadley is an adjunct professor at Rice University (allegedly), as well as Stanford (where he seems to have taught last in 2019 or 2020) and the University of Auckland (where he is an honorary professor). He is the chief scientist at Posit - the company that makes RStudio. Why do I worship this man? What has he done to deserve it?\nHadley first stepped onto the scene with ggplot2 (and a few others) in (and around) 2008. We‚Äôll use ggplot2 soon enough, but I want to explain why it‚Äôs important first. The graphical capabilities of R are quite good on their own, but using the base R functions is quite complicated. This is because the base plotting functions use a pen-on-paper approach, meaning that you have to specify each aspect on the plot individually with very few defaults. Although the base plotting functions are complicated enough for me to not know how to use them, ggplot2 is much easier because it is based on the grammar of graphics (and because the interface Hadley designed works really well).\nStatisticians (and mathematicians) are really into the idea of abstraction. Other scientists use this tool, as well, but usually with much less vigor than the mathematician. She would like to talk about a graphic, but not any particular graphic. She wants to talk about the abstract idea of a graphic. What are its component parts? How do they fit together? What are the different sorts of plots and how do we tell them apart? What elements of a plot are required to communicate the information within? This sort of thing.\nThis is the sort of questioning that leads to the grammar of graphics, which is a book by Leland Wilkinson. Wilkinson sets up a framework and an abstraction of the concept of ‚Äúgraphic‚Äù that is general enough to describe essentially any plot that has or will ever appear in a scientific journal.\n(Leland Wilkinson was an American psychologist, statistician, and computer scientist. He got a two degrees at Harvard, including a second bachelors in sacred divinity, and then received a PhD in psychology at Yale. I‚Äôm not going to say a lot about this man, but he has had a large influence on psychological statistics, to such an extent that he was the primary author of the APA‚Äôs guidelines and explanations for statistical methods in psychology journals (Wilkinson 1999).)\nIn any case, Hadley Wickham read Wilkinson‚Äôs book and built ggplot2 according to the grammar of graphics framework (gg stands for grammar of graphics). This was necessary because Wilkinson‚Äôs book is not software. It addresses theoretical questions, but not practical ones - how can I actually make this plot? ggplot2 addresses these practical questions: you make the plot with ggplot2. To the greatest extent possible, the plot is specified by default and in accordance with the grammar of graphics framework. The plot is still fully customizable (like the base R graphics), but unlike with the base graphics, you can produce a very fine ‚Äúfinished‚Äù looking plot with very little code (and most every line of code will have a single, specific, and easy-to-understand purpose).\n\n\n2.1.4 Tidy Data\nOne of Wickham‚Äôs first acts, the software package we now call ggplot2, was explosive, changing permanently the way the majority of R users make plots, and gaining definitively the attention of the open-source R community who were elated to make use of this new tool. As it turns out, this would be the first, not the last, time that Hadley Wickham caused such an explosion by introducing an innovative and complete technology to the community.\nCentral to Wickham‚Äôs work, at least since 2007 with the release of the reshape package, has been the structure of data. There are a lot of ways that you can put the same information in a table, and it is not easy to verbally describe the ways in which these schemes are different (at least not in any generalizable, abstract way). The case here is very similar to the case with graphics. Because ‚Äúdata‚Äù as such is probably most important to computer scientists, they have for literal decades been proposing abstractions and definitions according to which we should understand their use of that term. In the 1960s and 1970s the new technology in this regard was the relational database, which should be to some extent familiar to anyone who has ever used the organization and note-taking app Notion, which makes heavy use of these sorts of databases. With large datasets, especially in the 70‚Äôs, using a relational structure to organize the data could speed up computation by reducing the amount of information that needs to be stored in most cases.\nThe relational database is not super important for our purposes. All you need to know is that this is a way of storing usually large amounts of data in separate, inter-linked tables. The complexity of this scheme is relating the multiple tables to each other, and this task becomes nearly impossible if you do not have a consistent ‚Äúphilosophy of data‚Äù that allows you to consistently structure data and to know how it is structured without having to look at it. Using such a consistent approach to relational databases has two primary effects. First, programming is quicker because, after all of the data is correctly formatted, you save time having to look to check how individual tables are structured. Secondly, if all of the data that you‚Äôre ever going to use has to be in the same format, then you can very easily use the same code to complete the same task with different data sets. Had all of these separate data sets been formatted differently, there is nary a chance you would ever be able to reuse code, at least not without some level of modification. Computer scientists and mathematicians can further elaborate on the benefits of a ‚Äúphilosophy of data,‚Äù but none of the alleged benefits will ever be as important to me as is saving time.\nThat brings us to this 2014 paper in which Wickham publishes his ‚Äúphilosophy of data‚Äù: (Wickham 2014). Wickham calls this framework the tidy data framework, and it has 3 relatively simple rules. (It may be useful to note that, as with ggplot2, the tidy data framework and the software that uses it are based on ideas that existed before Hadley Wickham. In this case, the tidy data framework is derived from the third normal form of relational databases, as set out by Cobb, who also invented the relational database.)\n\nEach variable must have its own column\nEach observation must have its own row\nEach value must have it‚Äôs own cell\n\nThese three rules are from Wickham‚Äôs textbook chapter on tidy data instead of the paper (which is less accessible and older; Wickham and Grolemund (2017)). Take a look at the following examples of the same data in three different formats:\n\ntable1\n\n\n  \n\n\ntable2\n\n\n  \n\n\ntable3\n\n\n  \n\n\n\nThe first table is in tidy format because all the variables (country, year, cases, and population) have a column, each row is an observation of a specific country in a specific year, and each value has it‚Äôs own cell. This is how you want your data to look because this is how Hadley Wickham expects it to look. The second and third tables, by contrast, are not tidy. table2 is untidy because the type column contains two variables (cases and population) as values in cell rather than where the should be (as the names of the columns). table3 is untidy because it breaks the third rule, containing both the cases and the population in one column, separated by a ‚Äú/‚Äù (the rate column). A far more ideal solution, if the rate variable is needed, would be to do something like this in which that variable has its own column.\n\ntable1 |&gt; \n  mutate(rate = cases / population)\n\n\n  \n\n\n\nThe tidy data framework is the one that I use, and it‚Äôs the one that I‚Äôm going to teach you. In principle, there is nothing speical about this framework. Any ol‚Äô ‚Äúphilosophy of data‚Äù will do, so long as it is consistent. That is in principle. In practice, it is not the case that ‚Äúany ol‚Äô philosophy of data will do.‚Äù A large variety of the tools that we will be using were developed by Hadley Wickham. These tools are designed to work specifically with data that is in tidy format. In practice, then, it is very important that you use this framework because any other framework would require you to develop your own set of tools, which is a lot of work that Hadley Wickham has already done for you.\n\n\n\n\n\n\nFoundation, Free Software. n.d. ‚ÄúWhat Is Free Software? - GNU Project - Free Software Foundation.‚Äù Accessed May 9, 2024. https://www.gnu.org/philosophy/free-sw.html.\n\n\nIhaka, Ross, and Robert Gentleman. 1996. ‚ÄúR: A Language for Data Analysis and Graphics.‚Äù Journal of Computational and Graphical Statistics 5 (3): 299‚Äì314. https://doi.org/10.2307/1390807.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù Journal of Statistical Software 59 (September): 1‚Äì23. https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, and Garet Grolemund. 2017. ‚ÄúTidy Data.‚Äù In R for Data Science, 1st ed. https://r4ds.had.co.nz/tidy-data.html.\n\n\nWilkinson, Leland. 1999. ‚ÄúStatistical Methods in Psychology Journals.‚Äù American Psychologist.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Modern R</span>"
    ]
  },
  {
    "objectID": "03_r_101.html",
    "href": "03_r_101.html",
    "title": "3¬† R 101",
    "section": "",
    "text": "3.1 Names and Assignment\nBy far the most common thing you will do in R is assigning values to names. This is done with the assignment operator &lt;-. You can also use = for assignment, but it is less common in R. The &lt;- operator assigns the value on the right to the name on the left. Whenever you want to retrieve the value, you can use the name.\nthe_name &lt;- 10L\nprint(the_name)\n\n[1] 10\nThere are restrictions to the names that objects can have in R. Names cannot start with a number. Names are case-sensitive, meaning that X is not the same thing as x.\nx &lt;- 10\nX &lt;- 20\nprint(x)\n\n[1] 10\nNames can contain letters, numbers, periods, and underscores. They cannot contain spaces.\nTo the greatest extent possible, you should try to use names that are informative and short. This makes your code easier to write (because of the short name) and easier to read (because of the informative name).\nThat is what you need to write R code that will run. But what about writing R code that is readable? This is where the tidyverse style guide comes in. The tidyverse style guide is a set of conventions that the tidyverse team has developed to make R code more readable. The style guide is not a set of rules that you must follow, but it is a set of conventions that you should follow if you want your code to be readable by others.\nThe tidyverse style guide has a lot of rules, but here are a few of the most important naming conventions:\nFinally, the last thing you want to avoid is overwriting the name of a function. If you assign a value to T, you will no longer be able to use it as a shortcut for TRUE. If you assign a value to mean, you will no longer be able to use that function to calculate the mean of a vector. You really need to be cautious about this, which I understand may be difficult as you learn. It‚Äôs a good habit to develop, and it will save you from having to debug this sort of error, which is often a difficult and time-consuming process.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#names-and-assignment",
    "href": "03_r_101.html#names-and-assignment",
    "title": "3¬† R 101",
    "section": "",
    "text": "2020_data is not a valid name because it starts with a number\ndata 2020 is not a valid name because it contains a space\nx2020_data is a valid name\ndata.2020 and data_2020 are valid names.\n\n\n\n\n\nnames should only contain lowercase letters, numbers, and underscores\nvariable names should generally be nominal\nfunction names should generally be verbal",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#data-types",
    "href": "03_r_101.html#data-types",
    "title": "3¬† R 101",
    "section": "3.2 data types",
    "text": "3.2 data types\nclass: the function that you use when you want to know what type of data is stored in an R object.\n\n3.2.1 Numbers\nIntegers are specified with an L:\n\nint &lt;- 5L\nclass(int)\n\n[1] \"integer\"\n\nprint(int)\n\n[1] 5\n\n\nwithout the L, the integer will be interpreted as numeric. Numeric data are numbers that have a decimal point (in this case .0, which R does not print):\n\nnum &lt;- 5\nclass(num)\n\n[1] \"numeric\"\n\nprint(num)\n\n[1] 5\n\n\nNumeric is a super-ordinate category that contains doubles and integers. Numeric data is stored as doubles (i.e., numbers that have decimal points). You can see how a value is stored on the machine (i.e., to see that numeric values are stored as doubles, always) using the typeof function.\n\ntypeof(int)\n\n[1] \"integer\"\n\ntypeof(num)\n\n[1] \"double\"\n\n\nAs you can see, the class numeric are stored as doubles (i.e, using 64 bits), whether the value being stored is an integer or not. Values with the class integer are stored as integers (i.e., using only 32 bits).\n\n\n\n\n\n\nDouble Precision?\n\n\n\n\n\nComputers store numbers in sequences of 0s and 1s. A single position in this sequence, which could be occupied by either a 0 or a 1 is called a bit. A number stored in this way is called a binary number. Here is an example of an 8-bit binary number:\n00101101\nDecoding this number is a straightforward matter. Each position in the sequence represents a power of 2. The rightmost position represents \\(2^0\\), and the leftmost position (in an 8 bit number) represents \\(2^7\\). The number encoded is a weighted sum of the powers of 2. 00101101 is decoded as:\n\\[\n\\text{number} = \\sum_{i=0}^{7} \\text{bit}_i \\times 2^i\n\\]\nOmitting the 0s, 00101101 works out to be:\n\\[\n(1 \\times 2^2) + (1 \\times 2^4) + (1 \\times 2^5) + (1 \\times 2^7) = 4 + 16 + 32 + 128 = 180\n\\]\nYou might notice a couple of things about this scheme of an 8-bit number that I have put forth. This scheme can only represent integers (a result of using integer multiplication with powers of 2, which are always integers). You may also have noticed that one could represent 0 in this format, but never -1. All of the integers decoded in this manner will be positive.\nYou might use a sign bit at the beginning of the sequence to fix this second problem. When the sign bit is 1, the integer is negative; when it is 0, the integer is positive. The new decoding scheme using a sign bit would be represented like this:\n\\[\n\\text{number} = (-1)^{bit_0} + \\sum_{i=0}^6 bit_{i+1} \\times 2^i\n\\]\nAgain ignoring the 0s (except for the sign bit), 00101101 works out to be:\n\\[\n(-1)^0 + (1 \\times 2^1) + (1 \\times 2^3) + (1 \\times 2^4) + (1 \\times 2^6) = 1 + 2 + 8 + 16 + 64 = 91\n\\]\nWe could now easily represent -91 by changing the sign bit: 10101101.\nThis scheme still can‚Äôt represent non-integers, and our 8-bits are quite limiting. The largest and smallest numbers we can represent are positive and negative 127 (01111111 and 11111111, respectively). The limiting factor is, of course, the limited number of bits we have (only 8), and the limited amount of information those 8 bits can encode (see information theory for a formalized, quantitative way to think about information in bits - developed by Claude Shannon at Bell Labs).\nFor historical reasons, a 32 bit number is called a single precision number. A 32 bit number encoded as described above is a 32 bit integer. In R, integers are stored as 32 bit numbers. This means that the range of integers R can store goes from -2,147,483,647 (-2.1 billion; 11111111111111111111111111111111) to 2,147,483,647 (2.1 billion; 01111111111111111111111111111111)1.\nDouble precision floating point numbers (doubles) are 64 bit numbers with decimal points. The two schemes I outline can only encode integers. Encoding decimal values requires a more complicated scheme that I have no intention of fully explaining (as I have for 8 and 32 bit integers). Here is a Wikipedia page if you care all that much.\nBriefly, the leftmost bit is still a sign bit, interpreted in the same we we interpreted it above. The next 11 bits represent an exponent (encoded in binary). 11 bits can encode integers from 0 to 2047. To achieve negative exponents (needed for numbers between 0 and 1 and therefore numbers between -1 and 0), the exponent is subtracted from 1023. Therefore, the range of values the exponent can take covers all the integers from -1023 to 1024. The rightmost 52 bits represent a sum of fractions that is computed as a weighted sum, very similarly to how the integers were calculated from 7 bits above. The only difference is that each bit represents multiplication by a negative power of 2. This results in a weighted sum of fractions, where only the negative powers of 2 corresponding to positions with 1s in them are included in this sum.\nIn any case, the real referent of double precision floating point number is the number of bits the computer uses to store the number. Single precision floating point numbers (i.e., numbers with decimals) are stored according to a similar sign bit, exponent, fraction scheme, only with 32 bits. The benefit to having more bits is having more precision. There are, however, computational costs, and in many cases (especially if you are only using a 64 bit computer, as you are), using more than 64 bits exponentially increases the amount of time the computer will take to complete a single computation.\n\n\n\n\n\n\n\n\n\nComplex Numbers\n\n\n\n\n\nThe other type of numeric data in R is complex numbers, which are so very cool and beautiful, but which you are never realistically going to use and which I will not, therefore, bore you with the details of. However, complex numbers are numbers with a real part (which is multiplied by \\(1\\)) and an imaginary part (which is multiplied by \\(i = \\sqrt{-1}\\)). The value of the complex number is the sum of the real and imaginary parts. You can store a complex number like this:\n\nz &lt;- 0.8 + 1i\nclass(z)\n\n[1] \"complex\"\n\n\nI‚Äôll just quickly show you the seq function so you can see the effect of complex exponentiation. We can use the seq function to get a sequence of numbers from from to to, by increments of by.\n\nexps &lt;- seq(from = 1, to = 12, by = 0.2)\nexps\n\n [1]  1.0  1.2  1.4  1.6  1.8  2.0  2.2  2.4  2.6  2.8  3.0  3.2  3.4  3.6  3.8\n[16]  4.0  4.2  4.4  4.6  4.8  5.0  5.2  5.4  5.6  5.8  6.0  6.2  6.4  6.6  6.8\n[31]  7.0  7.2  7.4  7.6  7.8  8.0  8.2  8.4  8.6  8.8  9.0  9.2  9.4  9.6  9.8\n[46] 10.0 10.2 10.4 10.6 10.8 11.0 11.2 11.4 11.6 11.8 12.0\n\n\nWe can then raise the complex number z to each of these exponents to see the effect of complex exponentiation.\n\nzs &lt;- tibble(exp = seq(from = 1, \n                       to = 17.5, \n                       by = 0.1),\n             z = z ** exp)\nzs\n\n\n  \n\n\n\nNow we can look at the pretty spiral that complex exponentiation results in:\n\n\n\n\n\n\n\n\n\nIsn‚Äôt that just gorgeous? Complex numbers can also spiral inwards:\n\nz &lt;- 0.15 + 0.85i\n\n\n\n\n\n\n\n\n\n\nI drew a unit circle in that plot for a reason. As you can see in the plot below, the first complex number we used (\\(0.8 + 1i\\)) corresponds to a point outside of the unit circle on the complex plane. The point corresponding to \\(z = 0.15 + 0.85i\\), by contrast, lies within the unit circle. This is why the the first complex number spirals outwards, while the spiral of the second complex number spirals inwards. The unit circle is the boundary between the two types of spirals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.1.1 Arithmetic\nR has a number of arithmetic operators, many of which you will be familiar with.\nThe unary operators are - and +, which negate and do nothing, respectively. You would use - when saving a negative number.\nThese are five binary operators that are used frequently in the arithmetic common in public schools:\n\n+: addition\n-: subtraction\n*: multiplication\n/: division\n^: exponentiation2\n\nExponentiation and division offer two useful examples to demonstrate Inf and -Inf values. Anything divided by infinity is zero, which is more of a convention in mathematics than a thing that you calculate out (which would take a log time). This division by infinity resulting in zero rule is coded into R:\n\n10 / Inf\n\n[1] 0\n\n\nR also recognizes that anything to a negative infinite power is zero, and that anything (including an infinite value) to the zeroth power is equal to 1.\n\n10 ^ -Inf\n\n[1] 0\n\nInf ^ 0\n\n[1] 1\n\n\nIn addition to addition, subtraction, multiplication, division, and exponentiation, R has a few other arithmetic operators that you may not be as familiar with:\n\n%%: modulus (remainder of division)\n%/%: integer division (division that rounds down to the nearest whole number)\n\nThese functions revolve around division. Take the number 73. When you divide 73 by 10, you get 7 with a remainder of 3. In this case, 7 is the integer divisor of 73 divided by 10, and 3 is the modulus of 73 divided by 10.\n\n73 %/% 10\n\n[1] 7\n\n73 %% 10\n\n[1] 3\n\n\nThese functions are part of modular arithmetic, which winds up being incredibly important to computer science and cryptography, as well as to many fields of mathematics (like number theory).\n\n\n\n\n\n\nRounding, Truncation, etc.\n\n\n\n\n\nR has 5 function used to round numbers:\n\nceiling(): rounds to greatest and nearest integer (will increase absolute value of a positive number and decrease the absolute value of a negative number)\nfloor(): rounds to least and nearest integer (will decrease the absolute value of a positive number and increase the absolute value of a negative number)\ntrunc(): truncates the decimal part of a number\nround(): rounds to the nearest integer (or to a specified number of decimal places)\nsignif(): rounds to a specified number of significant digits3\n\n\nx &lt;- 5.7\ny &lt;- -5.7\n\nceiling(x)\n\n[1] 6\n\nceiling(y)\n\n[1] -5\n\nfloor(x)\n\n[1] 5\n\nfloor(y)\n\n[1] -6\n\ntrunc(y)\n\n[1] -5\n\nx &lt;- 13950.28738\nround(x, digits = 3)\n\n[1] 13950.29\n\nsignif(x, digits = 3)\n\n[1] 14000\n\n\n\n\n\n\n\n\n\n\n\nRelational Logic\n\n\n\n\n\nRelational logic is used to compare two values. The result of a relational logic operation is a logical value, either TRUE or FALSE. The relational operators in R are:\n\n‚Äú==‚Äù: is the thing on the left identical to the thing on the right?\n‚Äú!=‚Äù: not identical\n‚Äú&lt;‚Äù: less than\n‚Äú&lt;=‚Äù: less than or equal to\n‚Äú&gt;‚Äù: greater than\n‚Äú&gt;=‚Äù: greater than or equal to\n\nThat might seem relatively straightforward. I raise you the following complication:\n\na &lt;- 0.1\nb &lt;- 0.3 - 0.2\nprint(c(a, b))\n\n[1] 0.1 0.1\n\na == b\n\n[1] FALSE\n\n\nThat‚Äôs really odd, isn‚Äôt it? The way R stores the two numbers (see the first callout about double precision) means that these two numbers are different by a very small amount. How much, you ask?\n\na - b\n\n[1] 2.775558e-17\n\n\nAs I said, a very small amount. This is why you should never use == to compare floating point numbers. Instead, you should use the all.equal() function, which will compare two numbers to a specified tolerance.\n\nall.equal(a, b)\n\n[1] TRUE\n\n\n\n\n\n\n\n\n3.2.2 Characters\nCharacters are fairly self explanatory. You can tell R that something is a character by putting it in quotes.\n\na &lt;- \"Good morning!\"\nclass(a)\n\n[1] \"character\"\n\nprint(a)\n\n[1] \"Good morning!\"\n\n\nYou can also convert something from another class to a string. When you do so, R will begin to print it in quotes:\n\nx &lt;- 124L\nclass(x)\n\n[1] \"integer\"\n\nprint(x)\n\n[1] 124\n\nx &lt;- as.character(x)\nclass(x)\n\n[1] \"character\"\n\nprint(x)\n\n[1] \"124\"\n\n\nSee another example of the quotes with logical data:\n\nx &lt;- TRUE\nclass(x)\n\n[1] \"logical\"\n\nprint(as.character(x))\n\n[1] \"TRUE\"\n\n\nThere is one type of data that does not encapsulate in quotes when it is converted to a character. This is the NA or missing value, which R will essentially always represent as NA, regardless of it‚Äôs class.\n\ny &lt;- NA\nclass(y)\n\n[1] \"logical\"\n\ny &lt;- as.double(y)\nclass(y)\n\n[1] \"numeric\"\n\nprint(as.character(y))\n\n[1] NA\n\n\n\n\n3.2.3 Logical Data\nLogical data is also simple to understand. Neither TRUE nor FALSE are unfamiliar to the reader.\n\nTRUE & FALSE\n\n[1] FALSE\n\nTRUE | FALSE\n\n[1] TRUE\n\n\nIt‚Äôs useful to note what happens when you turn logical data into numeric data. TRUE becomes 1 and FALSE becomes 0.\n\nvals &lt;- c(TRUE, FALSE)\nprint(as.numeric(vals))\n\n[1] 1 0\n\n\nYou can also write TRUE as simply T and FALSE as F.\n\n\n\n\n\n\nTip¬†3.1: Logical Arithmetic\n\n\n\n\n\nThe : operator is used to create a sequence of numbers, starting at the number on the right, and continuing by adding 1 until the number on the left is reached.\n\n0.1:5.75\n\n[1] 0.1 1.1 2.1 3.1 4.1 5.1\n\nx &lt;- 0:6\n\nIf we wanted to find only the numbers that are less than or equal to 4, we could use the &lt; operator (remember this list includes 0 at the beginning).\n\nx &lt;= 4\n\n[1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n\n\nIf we wanted to see how many of the numbers are less than or equal to 4, we could use the sum() function.\n\nsum(x &lt;= 4)\n\n[1] 5\n\n\nIf you just wanted to see if any or all of the numbers are less than four, you could use the any() or all() functions.\n\nany(x &lt;= 4)\n\n[1] TRUE\n\nall(x &lt;= 4)\n\n[1] FALSE\n\n\n\n\n\n\n\n3.2.4 Factors\nThere is an excellent chapter about factors in R for Data Science.\nFactors combine integers and strings, and they are useful for categorical data (which is exceptionally common in psychology research). One categorical variable is the day of the week. I am going to store all of the days of the week in a vector called days.\n\ndays &lt;- c(\"monday\", \n          \"tuesday\", \n          \"wednesday\", \n          \"thursday\", \n          \"friday\", \n          \"saturday\", \n          \"sunday\")\n\nHere is a vector of days of the week. We‚Äôll use it as our data.\n\n# draw a random sample of 100 days of the week\ndata &lt;- sample(days, 100, replace = TRUE)\n# print the first 15 days\nhead(data, n=15)\n\n [1] \"sunday\"    \"thursday\"  \"wednesday\" \"monday\"    \"saturday\"  \"saturday\" \n [7] \"saturday\"  \"saturday\"  \"monday\"    \"wednesday\" \"saturday\"  \"saturday\" \n[13] \"thursday\"  \"monday\"    \"tuesday\"  \n\n\nUsing a factor, rather than a character, is useful for sorting purposes. Sorting our character days results in a list that is in the wrong order.\n\nsort(days)\n\n[1] \"friday\"    \"monday\"    \"saturday\"  \"sunday\"    \"thursday\"  \"tuesday\"  \n[7] \"wednesday\"\n\n\nDays of the week is a good candidate for a factor because there are a fixed number of known days of the week. We can convert the vector days to a factor by using the factor() function.\n\ny &lt;- factor(data)\nhead(y, n=15)\n\n [1] sunday    thursday  wednesday monday    saturday  saturday  saturday \n [8] saturday  monday    wednesday saturday  saturday  thursday  monday   \n[15] tuesday  \nLevels: friday monday saturday sunday thursday tuesday wednesday\n\n\nAs you can see, failing to supply factor levels results in the levels being sorted in the same meaningless way as before. We can fix this by supplying the levels in the order we want them to appear.\n\ndata &lt;- factor(data, levels = days)\nhead(data, n=15)\n\n [1] sunday    thursday  wednesday monday    saturday  saturday  saturday \n [8] saturday  monday    wednesday saturday  saturday  thursday  monday   \n[15] tuesday  \nLevels: monday tuesday wednesday thursday friday saturday sunday\n\n\n\nnlevels(data)\n\n[1] 7\n\nlevels(data)\n\n[1] \"monday\"    \"tuesday\"   \"wednesday\" \"thursday\"  \"friday\"    \"saturday\" \n[7] \"sunday\"   \n\n# if you want to see the numeric backbone of the factor\nas.vector(unclass(data)) |&gt; head(n=15)\n\n [1] 7 4 3 1 6 6 6 6 1 3 6 6 4 1 2\n\n\nYou may want sunday to be the first day of the week. You can do this by releveling the factor\n\ndata &lt;- fct_relevel(data, \"sunday\")\nhead(data, n=15)\n\n [1] sunday    thursday  wednesday monday    saturday  saturday  saturday \n [8] saturday  monday    wednesday saturday  saturday  thursday  monday   \n[15] tuesday  \nLevels: sunday monday tuesday wednesday thursday friday saturday\n\n\nYou can also reverse factors.\n\ndata &lt;- fct_rev(data)\nhead(data, n=15)\n\n [1] sunday    thursday  wednesday monday    saturday  saturday  saturday \n [8] saturday  monday    wednesday saturday  saturday  thursday  monday   \n[15] tuesday  \nLevels: saturday friday thursday wednesday tuesday monday sunday\n\n\nSometimes you will want to change the labels for a factor. You can accomplish this with fct_recode. The new labels are on the left, and the old labels are on the right. Notice that the order of the labels in fct_recode does not change the ordering of the factors levels.\n\ndata &lt;- fct_recode(data,\n                  \"mon\" = \"monday\",\n                  \"tue\" = \"tuesday\",\n                  \"wed\" = \"wednesday\",\n                  \"thu\" = \"thursday\",\n                  \"fri\" = \"friday\",\n                  \"sat\" = \"saturday\",\n                  \"sun\" = \"sunday\",)\nhead(data, n=15)\n\n [1] sun thu wed mon sat sat sat sat mon wed sat sat thu mon tue\nLevels: sat fri thu wed tue mon sun\n\n\nYou might also want to collapse the levels of a factor into a set of super-ordinate labels, like ‚Äúworkday‚Äù and ‚Äúweekend‚Äù. You can do this with fct_collapse.\n\ndata1 &lt;- fct_collapse(data,\n                    workday = c(\"mon\", \"tue\", \"wed\", \"thu\", \"fri\"),\n                    weekend = c(\"sat\", \"sun\"))\nhead(data1, n=15)\n\n [1] weekend workday workday workday weekend weekend weekend weekend workday\n[10] workday weekend weekend workday workday workday\nLevels: weekend workday\n\n\nThe fct_lump function will keep the most common n levels and lump the others into a new level called ‚ÄúOther‚Äù. This is particularly helpful if you have categorical data with a lot of very small categories.\n\ndata1 &lt;- fct_lump(data, n = 3)\nhead(data1, n=15)\n\n [1] sun   thu   Other mon   sat   sat   sat   sat   mon   Other sat   sat  \n[13] thu   mon   tue  \nLevels: sat thu tue mon sun Other\n\n\n‚ÄúOther‚Äù looks strange here because its the only level with an upper case letter. As with any of the functions I show you, you can look at the help page for the function to see what arguments it takes. The fct_lump function takes an argument (other_level) that allows you to specify the name of the new level.\n\ndata1 &lt;- fct_lump(data, n = 3, other_level = \"???\")\nhead(data1, n=15)\n\n [1] sun thu ??? mon sat sat sat sat mon ??? sat sat thu mon tue\nLevels: sat thu tue mon sun ???\n\n\n\n\n\n\n\n\nGenerating Levels\n\n\n\n\n\nSometimes you need to generate repetative factor levels (e.g., for a repeated measures design, or for a truth table). Our truth table will have 4 propositions: R, P, D, and R. Therefore our truth table will have 81 rows (because each proposition can either be true, false, or gay and we have four propositions; \\(3 ^4 = 81\\)).\nThe gl function generates a factor with up to n levels, each repeated k times.\n\ngl(n = 3, k = 2)\n\n[1] 1 1 2 2 3 3\nLevels: 1 2 3\n\n\nThe real clincher comes in the form of the labels argument. This allows you to specify the labels for each level.\n\noutcomes &lt;- c(\"true\", \"false\", \"gay\")\ngl(n = 3, k = 3, labels = outcomes)\n\n[1] true  true  true  false false false gay   gay   gay  \nLevels: true false gay\n\n\nWe can also (for our truth table) use the length argument to make sure that all of our vectors are the 81 elements long that we need them to be. This is what I need for my truth table.\n\nR &lt;- gl(n = 3, k = 27, labels = outcomes)\np &lt;- gl(n = 3, k = 9, length = 81, labels = outcomes)\nd &lt;- gl(n = 3, k = 3, length = 81, labels = outcomes)\nr &lt;- gl(n = 3, k = 1, length = 81, labels = outcomes)\n\ntibble(R, p, d, r)\n\n\n  \n\n\n\n\n\n\n\n\n3.2.5 Coersion and Checking\nHere is a list of the data types you‚Äôve seen thus far:\n\ninteger\nnumeric\ncomplex\ncharacter\nlogical\nfactor\n\nSometimes you want to check that the class of a variable is what you expect if to be (especially before you try to do something that only one type can do, like division). You can use the is.* functions to check the class of a variable (replacing * with the appropriate type).\n\nx &lt;- \"zzz\"\nis.integer(x)\n\n[1] FALSE\n\nis.character(x)\n\n[1] TRUE\n\n\nYou can also coerce one type of data into another, although this is sometimes risky. Coercing something into a character is usually safe, but coercing something into a type like numeric or logical can cause errors. Often R will create NA values and produce a warning, as shown below.\n\nx &lt;- \"zzz\"\nas.numeric(x)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\nx &lt;- \"123\"\nas.numeric(x)\n\n[1] 123",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#data-structures",
    "href": "03_r_101.html#data-structures",
    "title": "3¬† R 101",
    "section": "3.3 Data Structures",
    "text": "3.3 Data Structures\n\n3.3.1 Vectors\nA vector is a one dimensional array of elements of the same type (e.g., all characters, all numerics). In R, essentially everything is a vector if you analyze it at a fine enough level of detail. You can create a vector with the c() function4.\n\nc(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\n\n\n\nGenerating Sequences\n\n\n\nA far easier way to generate the vector above would be to use the : operator, which is described in Tip¬†3.1.\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nLook at the help page of the seq() function by typing ?seq in the console. seq takes arguments from, to, by, and along, which I will highlight here. We could recreate our vector with the following code:\n\nseq(from = 1, to = 10, by = 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nYou could provide only a from and a by and a length to get the same sequence\n\nseq(from = 1, by = 1, length = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nYou could also go backwards:\n\nseq(from = 10, to = 1, by = -1)\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nIf you want a sequence that is the same length as another vector, you could use the along argument.\n\nx &lt;- c(198, 3, -13, 0, 178, 20)\nseq(from = 0, to = 1, along = x)\n\n[1] 0.0 0.2 0.4 0.6 0.8 1.0\n\n\nThe rep() function is used to repeat a value a certain number of times.\n\nrep(1, times = 10)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n\nYou don‚Äôt just have to repeat single values. You can repeat vectors as well.\n\nrep(1:3, times = 3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\n\nThe each argument determines how many time each element in the vector is repeated each time there is a repetition.\n\nrep(1:3, each = 3)\n\n[1] 1 1 1 2 2 2 3 3 3\n\n\nThere is no reason not to use the each and times arguments at the same time.\n\nrep(1:4, each = 2, times = 3)\n\n [1] 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4\n\n\nYou can also specify how many times each element is repeated in each repetition. I specify cat to repeat five times, dog once, and fish twice.\n\npets &lt;- c(\"cat\", \"dog\", \"fish\")\nrep(pets, c(5, 1, 2))\n\n[1] \"cat\"  \"cat\"  \"cat\"  \"cat\"  \"cat\"  \"dog\"  \"fish\" \"fish\"\n\n\n\n\nYou can name elements in a vector by using the names() function. This is particularly useful for counts.\n\ncounts &lt;- c(101, 28, 57, 4, 2)\nnames(counts) &lt;- c(\"A\", \"B\", \"C\", \"D\", \"F\")\n\ncounts\n\n  A   B   C   D   F \n101  28  57   4   2 \n\n\n\n\n\n\n\n\nSubscripting\n\n\n\n\n\nYou can subset a vector by using square brackets. You can subset by index, with the index starting at 1 (not 0).\n\nx &lt;- 1:10\nx[4]\n\n[1] 4\n\n\nWe can also subset based on another vector, selecting whichever indexes you like.\n\nx[c(2, 4, 8)]\n\n[1] 2 4 8\n\n\nYou can use negative indexes to exclude certain elements.\n\nx[-2]\n\n[1]  1  3  4  5  6  7  8  9 10\n\nx[-c(2, 4, 8, 3, 1)]\n\n[1]  5  6  7  9 10\n\n\nYou can also subset based on logicals. This is particularly useful for filtering data.\n\nx[x &gt; 5]\n\n[1]  6  7  8  9 10\n\n\n\n\n\nvectors are very typical in data science and statistics. Take, for example, the heights of all the characters in Star Wars (in cm).\n\nheights &lt;- dplyr::starwars$height\nnames(heights) &lt;- dplyr::starwars$name\nhead(heights, n = 30)\n\n       Luke Skywalker                 C-3PO                 R2-D2 \n                  172                   167                    96 \n          Darth Vader           Leia Organa             Owen Lars \n                  202                   150                   178 \n   Beru Whitesun Lars                 R5-D4     Biggs Darklighter \n                  165                    97                   183 \n       Obi-Wan Kenobi      Anakin Skywalker        Wilhuff Tarkin \n                  182                   188                   180 \n            Chewbacca              Han Solo                Greedo \n                  228                   180                   173 \nJabba Desilijic Tiure        Wedge Antilles      Jek Tono Porkins \n                  175                   170                   180 \n                 Yoda             Palpatine             Boba Fett \n                   66                   170                   183 \n                IG-88                 Bossk      Lando Calrissian \n                  200                   190                   177 \n                Lobot                Ackbar            Mon Mothma \n                  175                   180                   150 \n         Arvel Crynyd Wicket Systri Warrick             Nien Nunb \n                   NA                    88                   160 \n\n\nWhere in the vector is the greatest height and where is the lowest? Let‚Äôs sort the vector and then view the first 3 and last 3 elements in the sorted version.\n\n# sort the vector\nheights2 &lt;- sort(heights,\n                decreasing = TRUE)\n\nThere is a problem here, even if it‚Äôs not immediately apparent. Sorting, by default, will remove all the NA values from the vector. We can check that elements were removed by comparing the length of the sorted and the unsorted vectors; the sorted vector is smaller.\n\n# prints number of elements in vector (number of characters)\nlength(heights)\n\n[1] 87\n\nlength(heights2) &lt; length(heights)\n\n[1] TRUE\n\n\nLet‚Äôs try sorting again, but this time we will tell R what to do with the NA values using the na.last argument. By setting it to TRUE, we put all the NAs at the end of the vector.\n\nheights2 &lt;- sort(heights,\n                  decreasing = TRUE,\n                  na.last = TRUE)\n\n# verify that heights2 is the same length as the original data\nlength(heights2) == length(heights)\n\n[1] TRUE\n\n# print first 5 heights\nhead(heights2,\n     n = 3)\n\nYarael Poof     Tarfful     Lama Su \n        264         234         229 \n\n\nThe tallest character is Yarael Poof (with a height of 264 cm), and\n\n# should be all NA values (which we put at the end)\ntail(heights2,\n     n = 3)\n\n   Poe Dameron            BB8 Captain Phasma \n            NA             NA             NA \n\n# we can use subscriptting to filter out NAs\ntail(heights2[!is.na(heights2)],\n     n = 3)\n\nWicket Systri Warrick          Ratts Tyerel                  Yoda \n                   88                    79                    66 \n\n\nYoda has the smallest height (66 cm). We could also have achieved these conclusions using the min or max functions, but there‚Äôs a problem. The result we get is not Yoda and Yarael Poof; it‚Äôs NA.\n\nmax(heights)\n\n[1] NA\n\nmax(heights)\n\n[1] NA\n\n\nMany vector functions have a na.rm argument which tells R to do the computation while ignoring all missing values. Setting this argument to TRUE is typically required to produce a numerical result (e.g., R won‚Äôt compute a mean for a vector with an NA value). In the case of min and max\n\nmax(heights, na.rm = TRUE)\n\n[1] 264\n\nmin(heights, na.rm = TRUE)\n\n[1] 66\n\n\nA large range of descriptive statistics are available with built-in functions.\n\n### centrality statistics\nsum(heights, na.rm = TRUE)\n\n[1] 14143\n\nmean(heights, na.rm = TRUE)\n\n[1] 174.6049\n\nmedian(heights, na.rm = TRUE)\n\n[1] 180\n\n# get the first, second (median), and third quartiles\nquantile(heights, \n         probs = seq(from = 0.25,\n                     to = 0.75,\n                     by = 0.25),\n         na.rm = TRUE)\n\n25% 50% 75% \n167 180 191 \n\n### spread statistics\n# range\nrange(heights, na.rm = TRUE)\n\n[1]  66 264\n\n# variance\nvar(heights, na.rm = TRUE)\n\n[1] 1209.242\n\n# standard deviation\nsd(heights, na.rm = TRUE)\n\n[1] 34.77416\n\n\n\n3.3.1.1 Correlation, Variance, and Covariance\nThese functions are incredibly important (at least in principle) to a lot of frequentist statistics. They take 2 numeric vectors, an x and y; or just a single matrix or data frame.\n\n# x and y are perfectly negatively correlated (r=-1)\nx &lt;- 1:5\ny &lt;- 5:1\n\n# computing the covariance, removing na values\nvar(x,\n    y,\n    na.rm = TRUE)\n\n[1] -2.5\n\n# this is an equivalent computation of covariance\ncov(x,\n    y,\n    use = \"na.or.complete\",\n    method = \"pearson\")\n\n[1] -2.5\n\n# you could also use a different method\ncov(x,\n    y,\n    use = \"na.or.complete\",\n    method = \"kendall\")\n\n[1] -20\n\n\nCo-variances are typically standardized into correlations before they are interpreted. You can use the cor function to get a correlation coefficient calculated according to the methods outlined by \"pearson\", \"kendall\", and \"spearman\". We will calculate Pearson‚Äôs \\(r\\), which is standardized number (between -1, indicating perfect negative correlation, and 1, indicating perfect positive correlation).\n\ncor(x,\n    y,\n    use = \"na.or.complete\",\n    method = \"pearson\")\n\n[1] -1\n\n\n\n\n3.3.1.2 Random Sampling\nR has a function called sample that is useful for generating a random sample of data, provided with some probabilities. For example, if you wanted to simulate the roll of a die, you could take a random sample (of size 1) from the sequence of numbers from 1 to 6:\n\nsample(1:6, size = 1)\n\n[1] 2\n\n\nOften you will want a sample with a size larger than 1. You can use the replace argument to allow for repeated sampling, as would be the case in 3 repeated rolls of a die.\n\nsample(1:6, size = 3, replace = TRUE)\n\n[1] 3 3 3\n\n\nYou can also use the prob argument to specify the probability of each outcome. For example, if you wanted to simulate a weighted die, you could use the following probabilities:\n\nsample(1:6, size = 3, replace = TRUE, prob = c(0.1, 0.1, 0.1, 0.1, 0.1, 0.5))\n\n[1] 1 1 6\n\n\nR also allows you to generate random numbers from a variety of distributions - this is the r* series of functions (where * is the name of a distribution - see ?distributions for more information). For example, to generate 5 random numbers from a normal distribution with a mean of 0 and a standard deviation of 1, you could use the following code:\n\nrnorm(5, mean = 0, sd = 1)\n\n[1]  0.7731497 -1.0539902  0.7413780 -1.1913741 -0.3632255\n\n\nEach distribution also has an associated d, p, and q function. The d function gives the density of the distribution at a given point, the p function gives the cumulative distribution function of the distribution at a given point, and the q function gives the quantile of the distribution at a given probability.\nThe difference between d and p is easiest to show graphically.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI showed the quantile function above. the q series distribution functions are used to calculate quantiles. For example, to calculate the 0.95 quantile of the standard normal distribution, you could use the following code:\n\nqnorm(0.95, mean = 0, sd = 1)\n\n[1] 1.644854\n\n\nIf we wanted specific quantiles, we should specify them as the first argument. Here I select a variety of probabilities that show the empirical rule - about 68% of the data should be within 1 standard deviation of the mean, 95% within 2, and 99.7% within 3.\n\ntibble(quantile = c(0.0015, 0.025, 0.05, 0.16, 0.84, 0.95, 0.975, 0.9985),\n       z = qnorm(quantile, mean = 0, sd = 1))\n\n\n  \n\n\n\n\n\n\n3.3.2 Lists\nLists are a very flexible data structure in R. The most simple lists can be though of as vectors with obligatorily named elements (elements in vectors can be named, but not obligatorily so). You can create a list with the list function, and access elements with the $ operator.\n\n# creating a list\nmy_list &lt;- list(a = 1, b = 2, c = 3)\nmy_list$b\n\n[1] 2\n\n\nWhat‚Äôs neat about lists, compared to vectors, is that they can store multiple types of data (which vectors cannot). For example, you could store a vector, a matrix, and a data frame in a single list.\n\n# creating a list with a string, a vector, a matrix, and a list\nmy_list &lt;- list(a = \"hello\", b = 2:-1, c = matrix(1:4, nrow = 2), d = my_list)\nmy_list\n\n$a\n[1] \"hello\"\n\n$b\n[1]  2  1  0 -1\n\n$c\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n$d\n$d$a\n[1] 1\n\n$d$b\n[1] 2\n\n$d$c\n[1] 3\n\n\nYou can access elements of a list with the $ operator, or with double square brackets [[ ]]. The double square brackets are useful when you want to access elements programmatically.\n\n# accessing elements of a list\nmy_list$a\n\n[1] \"hello\"\n\nmy_list[[\"a\"]]\n\n[1] \"hello\"\n\n\n\n\n3.3.3 Data Frames and Tibbles\nData frames are the most common data structure in R. A data frame is nothing more than a collection of vectors of the same length. This might initially seem quite a strange definition I have just provided, but stay with me. The data.frame is the closest correspondent to a spreadsheet. It stores data in rectangular form (i.e., in a table with rows and columns). The tidy data framework tells us that each of the columns in the table must represent a single variable and each row a single observation. The variables are each represented by a vector of values. The entire data frame is constructed by putting the vector for each variable side-by-side. Like this:\n\na  &lt;- c(1, 2, 3)\nb  &lt;- c(\"a\", \"b\", \"c\")\ndata.frame(col_one = a, col_two = b)\n\n\n  \n\n\n\nHadley Wickham‚Äôs response to the data.frame is the tibble, which essentially the same exact data structure. The biggest difference between a data.frame and a tibble is the way that they print in an R console - tibbles are more human-readable (unfortunately, I cannot demonstrate this difference here). Similarly to above, we could create a tibble using the tibble function.\n\ntibble(col_one = a, col_two = b)\n\n\n  \n\n\n\n\n\n3.3.4 Other Data Structures\nThere are two more base R data structures that I want to show you, but we will make much less use of them. For our purposes, we will primarily be using tibbles.\nThe first is a matrix. A matrix is a two-dimensional array that stores data of the same type (almost always numeric). You can create a matrix with the matrix function. Matrices are frequently used in linear algebra, and we will be using them for that purpose later (when and if we get to latent semantic analysis, which heavily employs matrices as a data structure). Tibbles are more flexible than matrices because they can store data of different types. For lots of reasons, however, computers are really fast at doing linear algebra, so matrix operations can often be more efficient than tibble operations.\n\n# numeric matrix\nmatrix(1:6, nrow = 2)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# character matrix\nmatrix(letters[1:9], ncol = 3)\n\n     [,1] [,2] [,3]\n[1,] \"a\"  \"d\"  \"g\" \n[2,] \"b\"  \"e\"  \"h\" \n[3,] \"c\"  \"f\"  \"i\" \n\n\nFinally, and this is the worst, there is a data type in R that is just called an array. You may think: wait a minute, didn‚Äôt I just describe a vector as a one-dimensional array and a matrix as an array with two-dimensions? Of course, you are correct. Someone made the bat shit decision to name a data structure array even though there are two other data structures that are also arrays and which are used a thousand times more. So, then, what is an array?\nAn array is a three-or-more dimensional array that stores data of the same type. I very seldom make use of arrays, but occasionally they are helpful (like matrices, operations on arrays tend to be optimized to shit). This is a three dimensional array. As you can see, R prints each layer of the 3x3x3 array separately in a sequence of 2-dimensional matrices. This is because it is not possible to print all three dimensions at the same time.\n\narray(1:27, dim=c(3, 3, 3))\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   19   22   25\n[2,]   20   23   26\n[3,]   21   24   27",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#programming-basics",
    "href": "03_r_101.html#programming-basics",
    "title": "3¬† R 101",
    "section": "3.4 Programming Basics",
    "text": "3.4 Programming Basics\n\n3.4.1 Functions\nI just showed you a lot of functions, but you can (and surely at some point will) make your own. This is a super simple doubling function that highlights the syntax. In the first line, I name the function (double_me), then I tell R that it is a function and that it takes one argument, number. When you call it, the function returns the result of the last line of code in the function.\n\ndouble_me &lt;- function(number) {\n  number * 2\n}\ndouble_me(4)\n\n[1] 8\n\n\nFunctions can also take optional arguments, which must always come after the obligatory arguments.\n\ngreet_me &lt;- function(name, time = \"morning\") {\n  paste0(\"Good \", time, \", \", name, \"!\")\n}\ngreet_me(name = \"Hunter\")\n\n[1] \"Good morning, Hunter!\"\n\ngreet_me(name = \"Hunter\", time = \"evening\")\n\n[1] \"Good evening, Hunter!\"\n\n\nI‚Äôll show you one more function that computes a factorial. A factorial is the product of all positive integers up to a given number. For example, the factorial of 5 is \\(5 * 4 * 3 * 2 * 1 = 120\\). The best and easiest way to approach this function is with recursion. This means that you write a function that calls itself. The idea with the factorial works like this: if \\(5! = 5 * 4 * 3 * 2 * 1\\) and \\(4! = 4 * 3 * 2 * 1\\), then \\(5! = 5 * 4!\\), or-more generally-\\(n! = n * (n-1)!\\). This is the recursive part; there is a factorial on both sides of the equation. Recursion can be tricky. You must always remember to include a base case (in this instance, when n = 1) that tells the function to stop calling itself. If you don‚Äôt, the function will call itself infinitely, which is less than ideal and unlikely to result in a correct computation. This function features return statements which explicitly tell R what value to return. This is needed because the return value is not the last line of code in the function.\n\nfactorial &lt;- function(n) {\n  print(paste(\"calculating factorial of\", n))\n  if (n == 1) {\n    return(1)\n  } else {\n    return(n * factorial(n - 1))\n  }\n}\nfactorial(5)\n\n[1] \"calculating factorial of 5\"\n[1] \"calculating factorial of 4\"\n[1] \"calculating factorial of 3\"\n[1] \"calculating factorial of 2\"\n[1] \"calculating factorial of 1\"\n\n\n[1] 120\n\n\n\n\n3.4.2 Control Flow\nControl flow is the order in which the code is executed. As we saw in the first chapter, programming relies on the ability to repeatedly run the same code and also to conditionally run some code. R has a few control flow structures that I‚Äôm going to review: the for loop, the while loop, the repeat loop, and the if statement.\nLet‚Äôs do the loops, first. A for loop is useful when you want to run the same code a certain number of times. In the for loop, we have to give R a variable (i in this case) as well as a sequence (the vector 1:5 in this case). The code inside the curly braces will be run for each value of i in the sequence.\n\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nWhile loops run a certain portion of code repeatedly until a condition is no longer met. The condition is what goes in the parentheses. Typically while loops will change the value of a variable inside the loop, so that the condition is eventually no longer met. This is performed by the line i &lt;- i + 1\n\ni &lt;- 1\nwhile(i &lt;= 5) {\n  print(i)\n  i &lt;- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nRepeat loops are a little trickier and less common than for or while loops. They run a portion of code indefinitely until a break statement is encountered. It is far too easy, if you‚Äôre not careful, to create an infinite loop if you forget the break statement. The break statment ends the loop. In this case, the loop will run until i is greater than 5.\n\ni &lt;- 1\nrepeat {\n  print(i)\n  i &lt;- i + 1\n  if (i &gt; 5) {\n    break\n  }\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nThe if statement is used to conditionally run code. If the condition in the parentheses is met, the code inside the curly braces will run. If the condition is not met, the code will not run. One example of an if statement in located in the above repeat loop. The condition there is i &gt; 5, and when the condition is met, the break statement is executed, stopping the loop.\n\nif (TRUE) {\n  print(\"This will print\")\n}\n\n[1] \"This will print\"\n\nif (FALSE) {\n  print(\"This will not print\")\n}\n\nYou can also add an else statement to an if statement. This code will run if the condition is not met.\n\ni &lt;- 6\nif (i &lt;= 5) {\n  print(\"i is less than or equal to 5\")\n} else {\n  print(\"i is greater than 5\")\n}\n\n[1] \"i is greater than 5\"",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#footnotes",
    "href": "03_r_101.html#footnotes",
    "title": "3¬† R 101",
    "section": "",
    "text": "Editors note: This is a simplified telling of how R stores integers in memory. For simplicity, the binary digit is interpreted as if the first (leftmost) bit corresponded to \\(2^0\\). IRL, R most likely reads the bits backwards such that the last (rightmost) bit corresponds to \\(2^0\\). This is just one example, but there are numerous complications that I simply do not mention.‚Ü©Ô∏é\nYou can use the operator ** in place of ^, and I frequently do. R translates ** into ^ before evaluating any expression.‚Ü©Ô∏é\nmostly a science/engineering thing; see Wikipedia page for review.‚Ü©Ô∏é\nc() stands for concatenate.‚Ü©Ô∏é",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "04_visualization.html",
    "href": "04_visualization.html",
    "title": "4¬† Visualization",
    "section": "",
    "text": "4.1 Plotting Basics\nThe first step to making any plot is calling the ggplot function. If you look at the help page for the ggplot function (by typing ?ggplot into an R console) , you will find that it takes 2 arguments: data and mapping. Ggplot expects that the data argument is going to be a data frame with tidy data in.\nggplot(data = time_use)\nAs you can see, ggplot creates an empty plot. Next, we will add a mapping argument. The mapping will tell ggplot which aesthetics (like the x-axis, y-axis, color, shape, etc.) will be represented by which variables in the data. In this case, we want the time_spent variable on the x-axis and the women_to_men variable on the y-axis, as they appear in the reference plot. When we call the ggplot while providing a data and a mapping argument, R will create a blank plot with axes.\nggplot(data = time_use, mapping = aes(x = time_spent, y = women_to_men))\nAs with any function in R, we do not need to write all of the arguments on a single line. We can separate the arguments onto different lines, as long as we make sure each line ends with a comma or the end parenthesis (i.e., the end of the function call). You may find that the following two code snippets are more readable than the one above, even though all three would produce the same output.\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men))\nggplot(\n  data = time_use,\n  mapping = aes(\n    x = time_spent,\n    y = women_to_men\n  )\n)\nTo R, it does not matter whether you type this function call out in 1 line or in 7; it is the same arguments being passed to the same function and thus produces the same result.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "04_visualization.html#plotting-basics",
    "href": "04_visualization.html#plotting-basics",
    "title": "4¬† Visualization",
    "section": "",
    "text": "4.1.1 Geoms\nOur plot is missing something: can you spot it? There is no data on our plot! We represent data in a ggplot by using a geom function. Almost all of these functions start with geom_ (like geom_bar or geom_smooth) or stat_ (like stat_count). You can see a fuller list of the geom functions available on the ggplot2 cheat sheet.\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe add a geom by literally adding it (with the + operator) to the ggplot function call. The + always has to go at the end of the line. You can separate the functions by as many empty or commented lines as you‚Äôd like. You can also do this within function calls. So, this code will run:\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  \n  \n  # this is a comment\n  \n  geom_point()\n\n\n\n\n\n\n\n\nas will this code:\n\nggplot(data = time_use,\n       \n       # this is a comment\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point()\n\n\n\n\n\n\n\n\nand if you wanted to be really verbose, you could even do something like this:\n\n# create ggplot\nggplot(\n  \n  # add data to plot\n  data = time_use,\n  \n  # create mapping\n  mapping = aes(\n    # map x-axis\n    x = time_spent,\n    # map y-axis\n    y = women_to_men\n  )\n) +\n  \n  # add scatterplot\n  geom_point()\n\n\n\n\n\n\n\n\nWhat is the difference between a mapping and a style? A mapping connects one aesthetic to a variable, but a style just sets the aesthetic. For example, styling our scatter plot might mean turning all the points blue, whereas a mapping would match each activity to a color based on a scale. This is a graph that uses color as a style:\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\n\nand this is a plot that uses color as a map:\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = activity))\n\n\n\n\n\n\n\n\nYou can put mappings in the ggplot function, or in any geom function. In the above code, the mapping for x and y is in the ggplot function, and the mapping for color is in the geom_point function.\nEvery geom will inherit the mapping from the ggplot function. If we added another geom to the plot, we could see this. In the plot below, I added the geom_smooth, and - as you can see - it inherits the x and the y aesthetic from the ggplot function, but it does not inherit the color argument from the geom_point function.\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = activity)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIf you wanted R to make differently colored smooth lines in this plot, you could add a color aesthetic to the geom_smooth function, or you could move the color aesthetic from the geom_point function to the ggplot function, thereby allowing the geom_smooth function to inherit a color aesthetic.\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, \n                     y = women_to_men,\n                     color = activity)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet‚Äôs do a little investigating with just the geom_smooth function.\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe used the color aesthetic to make separate lines above, but we didn‚Äôt need to. We can also use the group or linetype arguments to create separate lines\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(group = activity))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(linetype = activity))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can change the data supplied to geom_smooth to draw only one line:\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = continent)) +\n  geom_smooth(data = filter(time_use, activity == \"Unpaid work\"),\n    mapping = aes(group = activity))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou should use the help page (?geom_smooth) to read more about the aesthetics and arguments the function can take. I will only highlight two more: method, and se. I frequently find myself favoring a linear regression line (rather than the default which uses the LOESS smoothing function (a type of local, polynomial regression). You can get a straight line by changing the method argument to ‚Äúlm‚Äù. The se argument can be set to T or TRUE or F or FALSE, and it controls whether the plot includes an gray error area.\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(group = activity), \n              method = \"lm\",\n              se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOn a completely separate note, you use a very similar process of setting the data and mapping arguments to make a bar chart. Ggplot makes a distinction between a bar chart and a column chart (even though they can appear to be identical). A bar chart has a single, discrete aesthetic mapping (like one that maps x to continent).\n\nggplot(data = distinct(time_use, country, continent),\n       mapping = aes(x = continent)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nA column chart takes 2 mappings: a discrete x aesthetic (like activity) and a continuous y aesthetic (like time_spent).\n\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                color = activity)\n) +\n  geom_col()\n\n\n\n\n\n\n\n\nBar charts are a great example of the difference between the color and the fill aesthetics. Points and lines have only colors, but bars and columns (and other geoms, like density plots) have fills, as well. Let‚Äôs fix the last plot.\n\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                fill = activity)\n) +\n  geom_col()\n\n\n\n\n\n\n\n\nThis plot does not provide a helpful comparison of the time use in OECD countries across various continents because all of the columns have a different height. In reality, we would like all of the columns to be the same height so we can compare proportions. We can do this using the position argument\n\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                fill = activity)\n) +\n  geom_col(position = \"fill\")\n\n\n\n\n\n\n\n\nThis is a better plot to view the differences in OECD time use across continents. However, there is (again) very limited data for several continents which are either missed (i.e., most of Africa, Oceania, South America, and Asia) or have only very few countries (i.e., North America)\n\n\n4.1.2 Faceting\nUsing just the ggplot function and a few geom functions, we can get damn near the reference plots we started with.\n\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFaceting is tricky to explain in words, but it‚Äôs very easy to understand if you can see it.\n\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere are two faceting functions: facet_wrap and facet_grid. Both have a scales argument, which is set as ‚Äúfixed‚Äù by default. You can change it to:\n\n‚Äúfree_x‚Äù to allow the x axes to have different limits in the different plots\n‚Äúfree_y‚Äù to do the same for the y axes\n‚Äúfree‚Äù to have both axes take different limits for each separate plot\n\nIn this case, we‚Äôll just set the scales argument to \"free\" so that all of the data aren‚Äôt packed so tightly.\n\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2,\n             scales = \"free\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nfacet_grid is helpful if you would like to facet on two (discrete!) variables at once (like activity and continent in the plot below).\n\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 0.5,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_grid(continent ~ activity, \n             scales = \"free\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWith so few data, such extensive faceting is neither informative nor helpful, so we‚Äôll stick with just one facet: activity.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "04_visualization.html#plot-styling",
    "href": "04_visualization.html#plot-styling",
    "title": "4¬† Visualization",
    "section": "4.2 Plot Styling",
    "text": "4.2 Plot Styling\nOur current plot is beginning to look very similar to the reference plot we were attempting to recreate. We have written 23 lines so far. To save space, I am going to save the plot that we currently have under the name p. When we call p, R will make the plot. We can also add things to p, just as we added them to the ggplot function.\n\n# save ggplot object under the name \"p\"\np &lt;- ggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2,\n             scales = \"free\")\n\np\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n4.2.1 Labels\nBy default, ggplot does not create a title or subtitle for the plot, and it uses the names of the variables for the axes and scales (i.e., color scale in our plot). The labs function allows you to update various text elements in the plot.\n\np &lt;- p + labs(title = \"Gender Parity in Time Spent in OECD Nations\",\n         subtitle = \"ATTN: all plots on different axes\",\n         x = \"time spent (minutes per day)\",\n         y = \"ratio of women's time spent to men's\")\n\np\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAdding labels to the outside of your plot is as simple as that. You could also use labels as a geom using the geom_text function. It takes an additional aesthetic, label that we have not yet seen.\n\nggplot(data = filter(time_use, activity == \"Unpaid work\"),\n       mapping = aes(x = time_spent,\n                     y = women_to_men,\n                     color = continent)) +\n  geom_text(mapping = aes(label = country))\n\n\n\n\n\n\n\n\n\n\n4.2.2 Scales and Coordinates\nOur plot is different from the reference plot in which colors it uses. The reference plot uses black, orange, blue, green, and yellow; but, ours uses red, beige, green, blue, and purple. What gives? The reference plot uses a different color scale than does our plot. The color scale is the part of the plot that matches each continent to a unique color. The reference plot uses a color scale from the ggthemes package that is colorblind friendly.\n\np &lt;- p + ggthemes::scale_color_colorblind()\n\np\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nscales also control the x and y axes, specifically continuous scales. We can use a scale function to set the limits or breaks for the axes. In our case, this allows us to demonstrate how much larger the gender disparity appears to be for unpaid work rather than for any of the other (measured) uses of time.\n\np + \n  scale_y_continuous(limits = c(0, 7), breaks = c(1, 3.5, 7))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nwe can use scale functions to add transformations, as well. There are functions like scale_x_sqrt and scale_x_log10 that put the data on square root or log axes. (You can also use the transform argument in the scale functions to add a large variety of transformations.) Our time use data is not a great example of the utility of these types of axes. Instead, look at this data about coffee production:\n\ncoffee_plot &lt;- ggplot(data = coffee,\n       mapping = aes(x = pounds,\n                     y = pop_2019)) +\n  geom_text(mapping = aes(label = country)) +\n  labs(x = \"pounds of coffee produced in 2019\",\n       y = \"population in 2019\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\ncoffee_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe coffee data is dominated by small producers - with many populations and productions below 10 million people and pounds. The relationship between the variables is obscured by the clumping of data, which is caused by the massive outliers of Brazil (massive production) and India (massive population). A log-log plot (with log axes on the x and y) reveals a different perspective on this data. You must be careful, however, because log axes are not incredibly easy for most people to read, and the essentially everyone (including those that can read log-log scales) expects the scales on a plot to be linear unless explicitly warned otherwise.\n\ncoffee_plot + \n  scale_x_log10() +\n  scale_y_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere are many more transformations the scale functions can perform (like a boxcox, or logit transform), and you can see a fuller list in the documentation of the scale_x_continuous function (or any of the continuous scale functions). Okay; bye, coffee data!\nWe can finally use scales functions to control the labels on the axes. I find this is particularly helpful if you have percentages on one axis. We have a proportion, which can be represented as a percentage.\n\np &lt;- p + scale_y_continuous(labels = scales::percent_format())\n\np\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n4.2.3 Themes\nThe plot we have still doesn‚Äôt look quite like the reference plot we aimed toward. This can be explained by a difference in theme. The reference plot uses the minimal theme, whereas ours uses the default theme. We can ‚Äúfix‚Äù that by using the theme_minimal function.\n\np + theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can see the other default themes by seeing the help page for theme_minimal (i.e., ?theme_minimal, which is also the help page for the other built-in ggplot themes). My other favorite is theme_classic.\n\np + theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI also use theme_void, but typically only when making pie charts. theme_void removes most of the elements in the plot, including the x and y axes and gridlines.\n\np + theme_void()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\np &lt;- p + theme_minimal()\n\n\n\n4.2.4 Guides, Legends, and Text\nAlthough it makes no sense not to have it in this case, there are some cases in which you would like to remove the legend, which appears to the right of the plot by default. There are two ways to do this, using guides or using theme.\nguides is a function that is occasionally helpful for specifying which type of scale variables should be mapped to. You could set the guide to the color and/or shape arguments to ‚Äúnone‚Äù to remove one or both aspects of the legend.\n\np + guides(\n  color = \"none\"\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou could also remove the legend, instead of removing variables from the legend. You would do this using the legend.position argument in the theme function.\n\np + theme(legend.position = \"none\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou could also move the legend around using the legend.position argument.\n\np &lt;- p + theme(legend.position = \"top\")\n\np\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFinally, you can use the theme function to make really specific changes to the plot, like changing the angle of the numbers of the y-axis\n\np + theme(axis.text.x = element_text(angle = -30),\n          axis.text.y = element_text(angle = 30))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023. ‚ÄúData Visualization.‚Äù In R for Data Science, 2nd ed. https://r4ds.hadley.nz/data-visualize.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. ‚ÄúData Visualization.‚Äù In R for Data Science, 1st ed. https://r4ds.had.co.nz/data-visualisation.html.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "05_probability.html",
    "href": "05_probability.html",
    "title": "5¬† Probability Primer",
    "section": "",
    "text": "What is the law of large numbers?\nWhy are normal distributions important? What is the central limit theorem?\nWhat is a probability distribution? How do you write a function for a pdf and what are the properties of a normal distribution?",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Probability Primer</span>"
    ]
  },
  {
    "objectID": "0R_references.html",
    "href": "0R_references.html",
    "title": "References",
    "section": "",
    "text": "Bakker, Arthur, and Koeno P. E. Gravemeijer. 2006. ‚ÄúAn\nHistorical Phenomenology of Mean and\nMedian.‚Äù Educational Studies in Mathematics\n62 (2): 149‚Äì68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. ‚ÄúA Brief History of\nS.‚Äù In Computational\nStatistics, edited by Peter Dirschedl and R√ºdiger\nOstermann, 81‚Äì110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. ‚ÄúCase\nStudy 1: AT&T\nDivestiture.‚Äù Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of\nSexuality. Vol. 1. 3 vols. Random House.\n\n\nFoundation, Free Software. n.d. ‚ÄúWhat Is Free\nSoftware? - GNU Project - Free Software\nFoundation.‚Äù Accessed May 9, 2024. https://www.gnu.org/philosophy/free-sw.html.\n\n\nIhaka, Ross, and Robert Gentleman. 1996. ‚ÄúR: A\nLanguage for Data Analysis and\nGraphics.‚Äù Journal of Computational and\nGraphical Statistics 5 (3): 299‚Äì314. https://doi.org/10.2307/1390807.\n\n\nShustek, Leonard J. 2016. ‚ÄúProgramming the ENIAC:\nAn Example of Why Computer History Is\nHard.‚Äù May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. ‚ÄúThe Medicalization of\n‚ÄòHomosexuality‚Äô.‚Äù Honors Capstone\nProjects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTukey, John W. 1972. ‚ÄúData Analysis, Computation and\nMathematics.‚Äù Quarterly of Applied Mathematics 30 (1):\n51‚Äì65. https://doi.org/10.1090/qam/99740.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to\nthe Entscheidungsproblem.‚Äù Journal of Math\n58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.\n\n\nWickham, Hadley. 2014. ‚ÄúTidy Data.‚Äù\nJournal of Statistical Software 59 (September): 1‚Äì23. https://doi.org/10.18637/jss.v059.i10.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023.\n‚ÄúData Visualization.‚Äù In R for Data\nScience, 2nd ed. https://r4ds.hadley.nz/data-visualize.\n\n\nWickham, Hadley, and Garet Grolemund. 2017a. ‚ÄúTidy\nData.‚Äù In R for Data Science,\n1st ed. https://r4ds.had.co.nz/tidy-data.html.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017b. ‚ÄúData\nVisualization.‚Äù In R for Data Science, 1st\ned. https://r4ds.had.co.nz/data-visualisation.html.\n\n\nWilkinson, Leland. 1999. ‚ÄúStatistical Methods in\nPsychology Journals.‚Äù American\nPsychologist.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "0A_resources.html",
    "href": "0A_resources.html",
    "title": "Appendix A ‚Äî Getting Help",
    "section": "",
    "text": "The contents of this appendix are not here!\n\ngetting help in R (? and ??)\nthe two types of help pages, and the structure of each\n\ndataset pages\nfunction pages\n\nCheatsheets\ndocumentation online\n\nrmarkdown\nquarto:\n\nquarto guides\nquarto reference\n\nrstudio user guide\ntidymodels (for machine learning, mostly)\ngoogle and stack exchange\n\nThere is a general guide to getting R help, and it includes a suggestion I will forward: use Google and include the term ‚ÄúR‚Äù in the search",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Getting Help</span>"
    ]
  }
]