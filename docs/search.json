[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hunterr_2024",
    "section": "",
    "text": "Preface\nListing¬†1: greeting\n\n\nprint(\"Good Morning! ü§ó\")\n\n\n\n\n[1] \"Good Morning! ü§ó\"",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#this-book",
    "href": "index.html#this-book",
    "title": "hunterr_2024",
    "section": "This book",
    "text": "This book\nThis document you are reading now is called a Quarto book. We will talk more about Quarto later, but for now, all you need to know about Quarto books is that they:\n\nare relatively simple to construct,\ncan contain R code snippets and their outputs,\nare pretty enough for me to be temporarily satisfied.\n\n\nFeatures of This Book\n\n\n\n\n\n\ncallouts\n\n\n\n\n\nThis book let‚Äôs me put many of my least relevant tangents in these collapsible little notes.\n\n\n\nThe features of this book, which are achieved easily through Quarto, explain its format. Code snippets appear in this book, but only if you want them to. Each code snippet is collapsible by clicking the triangle next to ‚Äúshow code for this result‚Äù. If you really hate code, you can click the ‚Äú&lt;/&gt; Code‚Äù button on the top of each page and hide (or show) all the code at once. Speaking of the ‚Äú&lt;/&gt; Code‚Äù button, if you click ‚ÄúView Source‚Äù you can see all of the code that was used to create the page.\nQuarto also lets me show you ‚Äúpaged tables.‚Äù In the following code, I generate 1000 random numbers between -10 and 10, which I label x. Then, I apply the mystery_function to each number, producing y. What does mystery_function do?\n\ntable_1 &lt;- tibble(\n  x = runif(n = 1000, min = -10, max = 10),\n  y = mystery_function(x)\n)\ntable_1\n\n\n  \n\n\n\nQuarto also let‚Äôs me show you plots, like the one below. What do you think mystery_function does now?\n\nqplot(x = table_1$x, y = table_1$y) +\n  labs(x = \"x\", y = \"mystery_function(x)\")\n\n\n\n\n\n\n\n\nIn reality, mystery_function just squares x, so:\n\\[\n\\mathtt{mystery\\_function}(x) = x^2\n\\tag{1}\\]\nAll of the chunks of code have line numbers. For any chunk, a little clipboard appears when you hover over the code listing, and you can copy it by clicking on the clipboard. (This includes the code in the ‚ÄúView Source‚Äù pane, meaning you can copy the entire page, text and all.)\nQuarto also processes citations, so I can, for example, easily direct you towards my two fathers: Turing (1936) and Foucault (1978). It will also process internal links so that I can, for example, send you back to the top of the page: Listing¬†1.\nFinally, Quarto allows me to annotate code, which is helpful to explain how it works when a high-level of technical detail is needed. For an example of those annotations and when they might be helpful, look to ?fig-r_and_s.\n\n\nTools\nR is an free and open-source statistical programming language.You use R by typing commands into an R console, which looks like this:\n\n\n\n\n\n\n\nRStudio\n\n\n\n\n\nRStudio is a graphical user interface (GUI) and interactive development environment (IDE) with which to use R. This means that RStudio contains an R console (lower left) and a variety of other tools (right), like a text editor (upper left). RStudio looks like this:\n\n\n\n\n\n\n\n\n\n\nTidyverse\n\n\n\n\n\nTidyverse is a collection of R packages that are designed to work well together and to acomplish data analysis tasks. The Tidyverse packages were developed by Hadley Wickham primarily, but also by teams of his collaborators.\nBecause Tidyverse is set of R pacakges, there is not picture to provide. Tidyverse exists as R functions that you can (and will) use in your code.\n\n\n\n\n\n\n\n\n\ngrammar of graphics\n\n\n\n\n\nThe grammar of graphics refers to another set of R packages, exemplified by ggplot2, which is included in the tidyverse. The grammar of graphics is the tool that most R programmers use to produce the ‚Äúproduction-quality graphics‚Äù for which R is known. Once you begin to produce plots with ggplot2, you are likely recognize that you have been seeing these plots for years.\n\n\n\nI believe that these tools will allow you to accomplish the vast majority of data analysis tasks, and that knowing how to use them will result in you understanding data analysis on a deeper level and being able to do it faster. That being said, these are not the only tools for data analysis. I encourage you to explore others, as well; particularly, if you intend to do a lot of work with text data, I would suggest python.\nBecause my audience does not intend to become ‚Äúprogrammers,‚Äù per se, I would also like to introduce you to the following tools, which allow R code to be integrated into readable documents instead of writing R code in separate text files with R code only (called scripts). R scripts have a place, to be clear, especially while you are learning. However, if you would like to share the analysis that you do with an R script, sharing that R script is not a good way to do that. When you send someone an R script, you are sending them a bunch of code, not an analysis\nThe current gold standard for the sort of work you are most likely to want to produce (reproducible research) is this - the document you are reading now. This is a document which includes code, the output of that code, and text explaining that code and providing context. This approach is called literate programming. The tools used to produce literate programming documents with R are:\n\nRMarkdown: a document preparation software based on R and a markup language called markdown that is mainly used to make static documents (like appendices to a journal article). RMarkdown uses a software called pandoc to turn the .rmd file into: a pdf (latex or a beamer presentation), a .html website, a word document, a power-point, and a lot of other formats you‚Äôre likely to never use.\nQuarto: a very similar, but more advanced and comprehensive software than RMarkdown. Most of the things you can do in Quarto are also possible in RMarkdown, like adding cross references (like this Equation¬†1), creating books (also like this), creating interactive data dashboards (which seems particularly trendy as of late), and creating blogs and websites. Quarto is made by the same company that makes RStudio.\nShiny: a software (written in R) that allows you to create interactive plots, which may be helpful if you are, say, trying to decide the optimal number of bins in a histogram. You could use shiny to create a histogram and a bins slider, so that you could easily see a variety of different bin sizes merely by moving a slider (instead of by writing, modifying, and rewriting code to achieve the same end).\n\nMy goal for you is to write code that is readable and to put that code inside documents that are actively fun and/or interesting to read. (That‚Äôs also, incidentally, my goal for me.)\nWe are also going to do statistics! There are two approaches to statistics that we‚Äôre going to adopt through these lessons, so I‚Äôd like to begin by elucidating the way in which these approaches are different. I am more familiar with the first approach (exploratory data analysis). Confirmatory data analysis uses many of the same tools (like hypothesis testing, which I will show you), but it uses them in a different and moer complicated way. The ultimate goal of both approaches is to predict the result of measurements.\n\nexploratory data analysis: If the purpose of a statistical model is to predict data, then a model that makes the most accurate prediction is the best model. The model creation process is iterative. Once you see the results of a model, you can use the results to modify the model itself. Generally, practitioners recognize that there are a variety of different types of models that could be used for any task. Thus, they usually construct a variety of different models and then compare them to select a final model. Practitioners will use numbers (in diagnostic and statistic tables) or visuals (like a residuals plot) when comparing models.\nconfirmatory data analysis: In the best case, at least according to the ‚ÄúOpen science‚Äù framework, the final statistical model will have been selected and preregistered before data is even collected. Statistics have to be rigorous to mean anything, a fact which the machine learning people (who do only exploratory work) ignore. They don‚Äôt check model assumptions using statistical tests like the Shapiro-Wilk normality test, and they are constantly ‚Äúp-hacking‚Äù and ‚ÄúHARKing‚Äù to forcibly extract findings from their data. Confirmatory data analysis rejects these practices, aiming instead for statistical models that are pre-specified (pre-registered), theoretically-based, and rigorous (whatever they take that to mean).\n\nI‚Äôm often somewhat flippant about the second approach, which suggest and attempts to discover Truth where I am skeptical it exists. In any case, the second approach is the only one that is taught in statistics courses. This is a mistake. Firstly, exploratory data analysis is much more commond. Secondly, it is easier to get started doing exploratory (rather than confirmatory) data analysis. Because both of these approaches adopt many of the same tools, it seems to me that starting with exploratory data analysis and trying to make that make sense is the most effective way to learn confirmatory data analysis (which will require additional effort and research that I can‚Äôt provide).\nThus, I intend to provide you with a strong understanding of data that you can directly apply to exploratory data analysis tasks. My hope is that this understanding will enable you to complete a diversity of tasks, including confirmatory data analysis, if that is of interest.\n\n\nLearning Objectives\nThere has to be some boring stuff because pedagogy. I have quite a few learning objectives for you, forming one big list, but I‚Äôll attempt to section them off so they are easier to read.\n\nR Learning Objectives\n\ndiscuss R as a language with a history: use knowledge about the history of R (and of scientific computing more generally) to describe what R is, what people ‚Äúsay‚Äù in this language, and why this language has the properties and characteristics that it does.\nR competence: read R expressions written by others (allowing the language to serve a communicative purpose), and write R expressions that are readable and align with best practices within the open source R community.\nR‚Äôs friends: Describe R‚Äôs relationship to RStudio, RMarkdown, Shiny, Quarto, and Tidyverse; and, describe what each of these tools is and why someone would use them.\n\nrun R code in several different ways: via the console, a script, and Quarto or RMarkdown documents.\n\ndescribe R‚Äôs data types and the use of each: strings, numerics (floating point ‚Äúdoubles‚Äù, integers, and complex numbers), logicals, datetimes, and factors\ndescribe R‚Äôs data structures and the use of each: including, vectors (1-dimensional arrays), matrices (2-dimensional arrays), arrays (more than 2-dimensional arrays), lists (key-value pairs), data frames, and tibbles\naccess R documentation, and read it effectively enough to solve a problem\n\n\n\nComputation Learning Objectives\n\ngenerate synthetic data: use simulation of simple events (like the rolling of dice or flipping of a coin) to gain visual intuition for the central limit theorem and the law of large numbers\nuse the Monte Carlo simulation framework to evaluate statistical tests (e.g., by determining what happens when assumptions are violated)\nprocess string data: convert strings to all upper or lower case, add prefixes or suffixes, splitting strings apart\nprocess numeric data: scale and center numeric data and write functions to accomplish non-standard transformations\nprocess language data: apply the principles of natural language processing to pre-process text data (by tokenizing and stemming text and describing both of those processes and why they are used)\n\n\n\nStatistics Learning Objectives\n\nnull hypothesis significance testing: use R to perform null-hypothesis significance tests, such as the one-sample, two-sample, and repeated measures t-test\nregression: use R to perform linear and logistic regressions, including regressions with polynomial terms\nmachine learning: use R to perform a more complicated machine learning task, likely by constructing a decision tree and a random forest classification model\ndimensionality reduction: perform principal component analysis and construct a latent semantic space, and explain why these two seemingly distinct methods are connected by singular value decomposition\n\n\n\nData Science Learning Objectives\n\nrectangular data: use the tidy data framework to read, write, and pre-process rectangular data in a consistent, efficient, and minimally complex manner.\n\nimport data from a variety of sources including: comma-separated values (.csv) files, excel spreadshees (.xlsx files), google sheets spreadsheets\n‚Äòtidy‚Äô data into the following format: one observation per row, one variable per column, one value per cell\nuse available tools that enable you to store data in a very consistent format with very little effort\n\nlanguage data: use R to pre-process, analyze and visualize text data\nvisualization: use R and the grammar of graphics (represented by ggplot2 and related packages) to visualize data and to share data visualizations with others\npublication: use RMarkdown or Quarto to conduct a linear or logistic regression, and to report and interpret the results of those tests\n\n\n\n\n\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem.‚Äù Journal of Math 58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_lore.html",
    "href": "01_lore.html",
    "title": "1¬† The Lore",
    "section": "",
    "text": "1.1 Early 19th Century: the birth of programming\nThe idea of a programmable computer is not difficult to understand. The primary goal is to make a machine that you can use for different tasks, depending on what program you feed to that machine. Programs are written in code, which today is stored as text files on a computer. The programmable machine also lives within the computer, so running the program is as simple as telling the machine part of the computer where the program is located; then, the machine part of the computer tries to read find the and read the program at the specified location, and the (hopefully) program runs.\nFolks in the 19th century did not have access to digital text files, and so they could not write their programs on them. How did they write programs, and which sort of programs did they write? These are the primary questions I hope to address in this very first section?",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#early-19th-century-the-birth-of-programming",
    "href": "01_lore.html#early-19th-century-the-birth-of-programming",
    "title": "1¬† The Lore",
    "section": "",
    "text": "1.1.1 Jacquard‚Äôs Loom: the punched card\nOne of the first programmable machines was Jacquard‚Äôs loom, which Jacquard patented in 1804. Jacquard was a weaver. Jacquard‚Äôs loom was a machine that could create a variety of different patterns, depending on which program was put into it?\nSo, Jacquard wrote programs to produce beautiful woven fabrics, but more important is how he wrote programs. Jacquard had no text files, so instead he developed a different form of machine input: the punched card. Each line on Jacquard‚Äôs punched cards contained information about a single row of the design. The cards could be fed sequentially into the loom to produce a large pattern.\n\n\n1.1.2 Babbage, Lovelace, and the Analytical Engine\nGeneral-purpose digital computers, the sort of computers that can run R, emerged as an idea in the early-to-mid 19th century. Up to that point, computers were either mechanical (mechanical computers are fascinating, by the way) or just humans.\nOne of the first to develop a design for a general-purpose computer was Charles Babbage, working in the early part of the 19th century. In the 1830‚Äôs he proposed a massively complicated, general-purpose, steam-powered computer, which he called the analytical engine. The computer was only capable of carrying out the four basic operations of arithmetic: addition, subtraction, multiplication, and division; it was designed to take input via punched card, just like Jacquard‚Äôs loom.\n\n\n\n\n\n\nLady Lovelace\n\n\n\n\n\nDuring the 1830‚Äôs and 1840‚Äôs, Lady Ada Lovelace communicated with Charles Babbage (and several others involved in similar work) with the intention to collaborate with him in studying the analytical engine. It was Lady Lovelace who wrote the first substantial computer program, whose purpose was to compute Fibonacci numbers (Tibees 2020). Her program, written in the iconic note G, used only the four simple arithmetic operations.\nLovelace was interested in discovering the capabilities of the analytical engine. Her program computing Fibonacci numbers was important because it used loops in computation. Lovelace, daughter of the poet Lord Byron, was also interested in non-mathematical applications for the machine. She suggested that a sufficiently mathematical theory of sound could enable to engine to compose complex and scientific symphonies (Tibees 2020). Isn‚Äôt that beautiful!",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#middle-and-late-19th-century",
    "href": "01_lore.html#middle-and-late-19th-century",
    "title": "1¬† The Lore",
    "section": "1.2 Middle and Late 19th century",
    "text": "1.2 Middle and Late 19th century\n\n1.2.1 1830-1870(ish)\nThe middle of the 19th century was a period of massive global shifts. Liberation from enslavement was spreading across the globe after the Haitian revolution left white men horrified at the peculiar institution, which was also losing economic utility (see Capitalism and Slavery by Eric Williams).\nAlso within that 40 year period was:\n\nabolition (as a matter of law, anyways): British Empire (1834), French Empire (1848), Russian Empire (1861), Dutch Empire (and Dutch East India Company; 1863-73), American Empire (1865), Portuguese Empire (1869)\nSamuel Colt‚Äôs invention of a revolver that can be mass-produced (1836?)\nthe development of the telegraph (1830s)\nthe trail of tears (starting 1836)\nthe revolutions of 1848 and the publication of the communist manifesto\nthe first woman‚Äôs rights convention in the U.S. (Seneca Falls Convention, 1848)\nthe discovery of the Bessemer Process which enables the mass-production of steel, paving the way for emerging steel tycoons (1855)\nDarwin published On the Origin of Species (1859)\nGatling‚Äôs invention of the machine gun (1861)\nMaxwell publishes his equations, proposing an incredibly successful theory of physics that understands electricity, magnetism, and light as essentially the same thing (1861)\nthe construction and openning of the Suez Canal (1860‚Äôs)\nMendel‚Äôs publication of his laws of genetic inheritance (1865)\nthe discovery of the cell and subsequent elaboration of cell theory (1865 and after)\nNobel‚Äôs invention of dynamite (1867)\nMarx‚Äô publication of the first volume of capital (1867)\nthe completion of the transcontinental railroad (U.S., 1869)\nMendeleev‚Äôs publication of the first periodic table (1869)\n\nIn this revisionist history of the computer (and ultimately of R), this period in history marked a transformation of power. The structure and organization of society was changing along with the flow of people, ideas, and commerce. Western, liberal democracies had to develop new technologies of population control in order to prevent all of these liberal changes from challenging their position of authority and power.\n\n\n1.2.2 Late 19th century\nWith the relative liberation of black bodies (and other bodies, as well) came a scientific imperative. Power continued to demand that these bodies be inferior, but evidence of inferiority was no longer to come from the conditions and dimensions of the body. Nay, the newly-available technologies of genetic inheritance and natural selection allowed a regime of a new flavor to take hold, one that cited hard science to support and justify the inequities in society. Inferiority was moving through the skin, into the body, and - importantly - into the mind.\nWilhelm Wundt opened the first psychology lab, and William James delivered the first psychology course and textbook. Galton, who was studying intelligence, popularized the idea of the median (Bakker and Gravemeijer 2006). Psychology and with it psychological statistics, was beginning to take shape to meet the new demands of the state: a theory and a technology that will find permanent, internal traits upon which to stratify society into haves and have-nots. The story of the emergence of psychological statistics is incomplete without mention of eugenics. The tools being developed were not neutral and scientific, but overtly political, aimed at achieving the goals of the state.\nAlso in the late 19th century was what Foucault called the implantation of perversions (Foucault 1978) - the creation of new symbolic threats to the body and to society as a whole. This operated through the invention of new characters that continue to exist within society today.\nFirstly, there was the medical specification of the homosexual (Townsend 2011). This began in 1864 with the work of Karl-Heinrich Ulrichs, who was gay himself. He specified men as either urnings or dionings. Urnings and Dionings are both male-bodied creatures, but the urning experiences the desires and character of a female (Townsend 2011). The dioning, by contrast, is normal. Discourse about the urning (renamed to the invert, and then to the homosexual) continued well into the 20th century, and the sissy (the archetype the invert represents) is, obviously, still with us.\nAlso within this time period, was the medical specification of the hysteric woman, which was initially the perogative of Jean-Martin Charcot.\nI‚Äôll mention just one more character that was invented in the later 19th century. For all of American history to this point, immigration law was about the process of naturalization - immigrants becoming citizens. From the beginning of the union, only white men of ‚Äúgood moral character‚Äù were allowed to become American citizens (Naturalization act of 1790?). There was little effort to actually prevent bodies from entering the country.\nUntil 1875. With the passage fo the Page Act of 1875, the United States declared its intention to keep undesirable bodies out of the country for the first time. Shortly thereafter, the ‚Äúillegal alien‚Äù was invented as a result of the Chinese Exclusion Act of 1882, which is the only American immigration law I am aware of that names a specific national group in its title.\nAll this to say that the nature and enforcement of undesirability were in massive flux in the late 19th century. The foreign element was moving within: the enslaved African could become a citizen and could vote, the invert or the hysteric could be hiding within anyone, and the state took up the power to deport bodies that did not belong. No longer was the anthropologist writing about the inferiority of foreign peoples (although to be clear, they absolutely were still doing that); the pschiatrist was now writing about our own inferiority.\nI consider the birth of statistics to be in this time period, which does not have pleasant implications for statistics as a field. There is a lot more to be said about the advent of statistics, and how statistics is designed to serve power (i.e., fulfill the demands of the state). However, I‚Äôm going to leave all of that unsaid and refocus on computation in general, and statistical computing in particular.\nThe late 19th century was also, notoriously, the era of massive trusts in the United States. These monstrous, monopolistic companies exploited both the consumer and the worker, but the United States did not yet have a legal mechanism for breaking them up. The most important monopoly for our purposes: the one that is most influential is the development of S and then R is the AT&T monopoly.\nAnother monopoly was also forming. Using Jacquard‚Äôs punched cards, an American man designed and patented a system to read punched cards. In 1890, this punched card system was used to complete the census, resulting in the 1890 census being completed two years quicker than the 1880 one. The company that developed this technology would go on to become IBM, which enjoyed monopoly status in the computing industry for several decades.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#sec-early-19th-century-the-advent-of-computing",
    "href": "01_lore.html#sec-early-19th-century-the-advent-of-computing",
    "title": "1¬† The Lore",
    "section": "1.3 Early 19th century: the advent of computing",
    "text": "1.3 Early 19th century: the advent of computing\nNear the end of the 19th century, a mathematician named David Hilbert decided that mathematics needed to be formalized. Up to that point, it had developed as myriad sub-disciplines that failed to cohere into a single, interconnected web of mathematics. Hilbert believed that it should, and his goal was to formalize this system. He believed that such a system (of mathematical axioms) needed to have three properties:\n\nto be consistent: it should not be possible to derive that a statement is both true and false\nto be complete: it should be possible to derive the truth of every true statement (or the falsity of its negation)\nto be decidable: there must be an algorithm that can identify all and only true statements in a finite number of steps.\n\n(The excitement about formalizing affected Hilbert, but by no means was he the first or the only to be caught up in this mess. Notoriously, Whitehead and Russel got spun up enough to publish a 126-page long proof that \\(1+1=2\\). I‚Äôm mostly attributing these three demands to Hilbert for sanity‚Äôs sake because I cannot stand to write out the sordid details. These three ‚Äúproperties‚Äù as I call them, are really inspired very loosely on any specific, cite-able Hilbert publication. He did publish a list of 23 questions, which refer to the properties I mention here, but understand this as a drastically over-simplified view of the mathematical debates unfolding at the time.)\nMathematics was not the only field to be heating up. There was growing speculation in physics that matter may not be as continuous as was previously assumed. In 1900, Max Planck published the first quantum theory in physics, which was aimed at modelling thermal radiation. Shortly thereafter, Albert Einstein published another quantum theory, this time aimed at modeling the the photoelectric effect. Both of these models used quantum stuff (i.e., minimal, discrete units of energy, creating measurements of energy that are always a multiple of the quantum unit), but the authors did not actually believe the world was quantum. Famously, Einstein‚Äôs theories of relativity both rely on space-time being continuous. They merely believed quantized math was the best way to explain non-quantum physical phenomena.\nNeils Bohr went the whole way, creating his model of the atom, with distinct, orbital electron shells. In the 1920‚Äôs quantum mechanics, as we know it today, came into existence. It did not make Einstein happy. Einstein wanted a deterministic world, where each cause has an specific, reliable effect. Quantum mechanics is not a deterministic theory of physics, but a probabilistic one. I take this diversion into the physical sciences not only to stress that this is a transition period within the physical sciences, but to temper my claim from the previous section. The ‚Äúdemands of power‚Äù did no less to supercharge the development of statistics and probability than did rapid changes in the way we understand and model the physical world.\nDuring my quantum mechanical tangent, G√∂del has proven that achieving the second property of Hilbert‚Äôs idealistic system is unlikely. In fact, G√∂del establishes that it is logically impossible that any formal mathematical system could be complete, as defined above.\nTo answer the question about whether mathematics is decidable, a new technology is needed. Before a mathematician can make formal claims about the capabilities or limitations of algorithms in general (as Hilbert demanded), she must first provide a rigorous definition of an algorithm. Two mathematicians took up this task, Alonzo Church who developed the lambda calculus, and Alan Turing who developed the Turing machine. Both men reached the same conclusion: mathematics cannot be decidable. It is logically impossible to make an algorithm (a Turing machine) that can identify all and only true statements (Turing 1936). There are, as it turns out, hard limits on the types of problems algorithms are able to solve (at least in a finite number of steps).\nThus, Turing half accidentally created the field of computer science while trying to answer a question about the foundations of mathematics. This is also an opportune time to introduce the term Turing-complete which refers to anything (model of computation, programming language, a book of instructions used by a human computer) that can simulate the a Turing machine. Any Turing-complete system is essentially equivalent to the original Turing machine described in (Turing 1936). The analytical engine is (theoretically, of course, it never got built) Turing-complete; Jacquard‚Äôs loom, by contrast, is not. Modern programming languages are, for the most part, Turing complete, meaning that any function you write in a modern programming language could be performed on the OG Turing machine from (Turing 1936).\nThe first electric, digital computer was not fully constructed until 1945. It was built by and for the U.S. military, who named the machine ENIAC. Thus, the first computations done on an electric, digital computer were intended to speed up the process of human and earthly destruction. ENIAC was a bunch of coordinated units that ran according to the placement of wires on the machine (Shustek 2016). The machine took IBM punched cards as input (remember the punched card monopolist from the end of the 19th century?).\nInitially, the wires on ENIAC had to be moved for each new problem (Shustek 2016). The process of re-configuring the machine for each new problem was tedious, but it was possible, and so ENIAC was Turing-complete. However, having to physically move wires prevented the machine from achieving the utility of a modern programmable computer.\nThis machine was very quickly modified in a way that dramatically changed its function. Instead of having to move wires, and then feed the machine (punched card) instructions based on the position of those wires, it would be much faster permanently code instructions (functions) into the machine. Then, the input of the machine could describe the sequence of functions. You could achieve looping by instructing ENIAC to perform a function repeatedly and conditional (if-statement) execution by instructing ENIAC to skip functions in the sequence.\nThis is the idea behind modern programming languages. Instructions for the computer, written in the computer‚Äôs language (ENIAC‚Äôs language was wires, the one we‚Äôll soon focus on is R) are stored within the machine. ‚ÄúProgramming‚Äù the machine involves telling it which instructions to perform and in which order. In 1948, the first ENIAC ‚Äúprogram‚Äù ran under this new computer architecture was a Monte Carlo simulation of neutron decay during nuclear fission (Shustek 2016).",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#att-bell-labs-and-s",
    "href": "01_lore.html#att-bell-labs-and-s",
    "title": "1¬† The Lore",
    "section": "1.4 AT&T, Bell Labs, and S",
    "text": "1.4 AT&T, Bell Labs, and S\nMonopolies suck, and AT&T did as well. Throughout the beginnning of the century, it gradually became clear that the benefits of a monopolist teleohpne provider were not going to materialize. In 1949, the U.S. Department of Justice sued AT&T for violating the anti-trust act, and the resulting 1956 consent decree prohibited AT&T from entering the computer business (Chang, Hen, and Kan n.d.).\nThis consent decree did not prevent the further degradation of AT&T‚Äôs service, nor did it prevent future anti-trust lawsuits. Throughout the 60‚Äôs and early 70‚Äôs, the U.S. government dogged AT&T with recurrent anti-trust lawsuits. In 1974, the Department of Justice began their final lawsuit against a monopoly AT&T. Over the course of the next decade, the government proved that AT&T was leveraging its monopoly power to predatory ends, annihilating potential competitors and pricing services far beyond the cost to provide them. This lawsuit ended in 1982 with the dissolution of AT&T into 7 Regional Bell Operating Companies (Chang, Hen, and Kan n.d.).\nBell Labs was probably the most important laboratory within AT&T. During the early 70‚Äôs, the researchers at the statistics research department within Bell Labs was using the programming language FORTRAN. FORTRAN is a general purpose, compiled programming language, developed by IBM (the punched card guys).\n(This isn‚Äôt super relevant, but I think it‚Äôs fun. Before 1968, computational statisticians had been using a algorithm called RANDU, which was a FORTRAN function that generated random numbers. In 1968, a mathematician proved that the allegedly random numbers actually all had to lie on a series of parallel hyper-planes, and we thus not actually random. wtf is a series of parallel hyper-planes? See below)\n\n\n\n\n\n\n\n\n\nFORTRAN, the name, stands for ‚Äúformula translation,‚Äù and it was primarily used for scientific computing, like computing weather models or doing computational physics - things that have to do with numbers, essentially. It is still used in these fields to some extent, although it is less popular for scientific computing than other, more recent programming languages, like R. FORTRAN is, computationally speaking, incredibly efficient, mostly by natively supporting parallel computation. For this reason, FORTRAN is still used to benchmark supercomputers. You can learn more about FORTRAN on its website.\nIn any case, in the 1970‚Äôs, the statistics research department at Bell Labs found FORTRAN to be somewhat insufficient, and they set out to develop a new language that would more fully suit their needs (Becker 1994).\n\n1.4.1 S\nS is a statistical computing language that was developed first at Bell Laboratories in the mid 1970s. At the time, statistics was undergoing a change. Previously, statistics had been developing as a set of methods - essentially algorithms that prescriptively described how to complete a statistical analysis from beginning to end. In the early 70‚Äôs, John Tukey was working at Bell Labs and at Princeton, and he was making a lot of noise about the problems with statistics. He popularized a different approach to statistics, establishing something of a binary between data analysis and statistics, just as I did between machine learning and statistics (in Section 1.2; Tukey (1972)).\nThe statistics research department was beginning to demand a tool that aligned with Tukey‚Äôs approach. FORTRAN, developed more than a decade before that demand was created at and by Bell Labs, did not measure up to the task. Instead, they decided to develop a new language, which they named S. Initially, there was a large focus on being able to import FORTRAN functions into S, so that there could be a smooth transition from FORTRAN to S within Bell Labs.\nS was built from the ground up to include graphics capabilities, and a structure that enabled and encouraged exploratory data analysis. The basic data structure in S is a vector of like-elements, which were used to make matrices and time-series; S also included lists (key-value maps) and the $ operator, which could be used to retrieve specific components of larger data structures (Becker 1994). It also included all of the arithmetic operators that you need in a desk calculator, making it useful for that purpose, as well.\nIn 1980, S was distributed outside of Bell Labs for the first time. Initially, it was distributed for a nominal fee and for educational use only, but by 1981 it was widely available (Becker 1994). After it began to be distributed, the developers added explicit looping (i.e., for loops), as well as the apply function, which could be used to loop over a vector while applying a function (Becker 1994). The developers also introduced the ‚Äúcategory‚Äù, which is now called the factor in R. Categories are vectors of data. They merge numerical and string data types - each entry in the vector is assigned a category label (so that you can read it), as well as a underlying integer (so that you can do math with categories).\nAlthough S was developed initially by statisticians, it clearly had utility as a data manipulation, graphics, and exploratory data analysis tool. In 1988, the developers released the ‚ÄúNew S,‚Äù renaming the software after some significant changes. The most significant feature of New S was the inclusion of first class functions, which are functions that you can assign to a name and and then refer to by that name. Functions are first class in that they are S objects, just like any vector or matrix. For the first time, S had depreciated functions, which R also has. Depreciated functions are functions for which there is a better alternative. They are generally still included in R and S distributions (so old code that uses depreciated functions can still run), but it‚Äôs best to avoid using them (and to use the better alternatives instead). By 1988, many of the FORTRAN functions from the initial development of S were rewritten in C, which is a general purpose programming languages on which New S is built (Becker 1994).\nIn 1991, the S development team expanded, and there was a focus on adding statistical software to the S language. Although S was developed by statisticians who intended to use it for statistics, the statistics are not inherent in S: ‚ÄúS is a computational language and environment for data analysis and graphics‚Äù (Becker 1994). As such, the developers added the formula class, which could be used to specify statistical models. The formula is marked by the ~ operator, with the dependent variable on the left and the independent variable(s) on the right (e.g., y ~ x + w + x*w).\nAlso in the 1991 release was the data.frame. Matrices are like vectors: they can only have one type of data. If you have a matrix that has even one number in it, then the entire matrix must be numeric, even if you want to use it to represent string data (like names and job titles) or categorical data (Becker 1994). So, a matrix is a combination of multiple vectors, all of the same type. A data.frame, by contrast, is a combination of vectors of any type. You can have a string vector (column) in the data frame representing job title, as well as a numeric vector representing income. As with matrices, you can use the $ operator to pick a vector out of the data frame (e.g., data$income picks out the income vector in the data frame called data).\nIt‚Äôs not really possible for a programming language to die. As we have seen with FORTRAN and S, new programming languages often use code from their older counterparts, especially at the beginning. Even though I can no longer find S on the internet and run it on my computer, a very large number of S functions continue to exist in R.\nI am able to find relatively scant documentation about this final period in the history of S, so the rest of this section is at least somewhat speculative (except claims that are cited, of course).\nWhat is the need for R if S exists? Well, well, well. Let‚Äôs talk about corporate fuckery, which both killed S and prevented it from dying. I have been making a much bigger deal over anti-trust law than the vast majority of those who introduce R to their students. To this point, as far as AT&T and Bell Labs are concerned, I have presented a world in which they are legally prohibited from selling computers (and presumably, computer software) as a result of the Consent Decree from 1956.\nUp to this point, no one was making money off of S. Although Bell was initially charging folks a nominal fee to use the software (Becker 1994), this practice ended quickly, meaning that the software was being distributed for free. As a result of the anti-trust, Bell Labs was not going to monetize this technology. Instead, one of their former employees had to do it.\nIn the 70s and 80s, the graphical user interface (GUI) was being born. This emerging technology came with a new generation of capitalists who had not been subject to extensive anti-trust, in which former trade union president Ronald Reagan did not believe - the capitalists who bring us Microsoft and Apple, who own outright the operating systems of about 85% of the worlds‚Äô computers (and many phones and other devices, as well).\nS wasn‚Äôt fated to become Windows; it was fated to become S-PLUS. S-PLUS is/was a statistical computing software with a graphical user interface. It was developed by a company owned by a former Bell Labs employee and University of Washington professor, R Douglas Martin. His work is primarily in econometrics, and he has extensively published about investment risks. Because of this, and because S-PLUS was and is mostly used by economists, a cynic might call it an application to be used for those who are unwilling or unable to learn how to code (similar in character to Microsoft‚Äôs SPSS). S-PLUS started circulating (for a fee) in about the year 1987, and it did include features that S did not (like generalized linear models).\nLet me just quickly recap, so I can make sure everyone is oriented in time - I‚Äôm discussing a lot of events that overlap and are not all well documented. In 1980, S was released to the public; in 1988, S had a significant update, becoming ‚ÄúNew S‚Äù; around 1987, a former employee of Bell Labs developed S-PLUS; in 1991, S had an update that focused on statistics.\nIn 1991, two statisticians quietly began work on the project (R) that would more-or-less kill S and S-PLUS.\nIn 1993, S and S-PLUS were reunited when Bell Labs sold S to the company that had developed S-PLUS. That company, in turn, immediately merged with a company called MathSoft. S-PLUS was only available on windows, and its relationship with Microsoft strengthened when features were added to connect S-PLUS to Excel and to SPSS.\nPart of the company (MathSoft) was sold, it got renamed (to Insightful), the exclusive license to distribute S turned into AT&T (i.e., Lucent, one of the companies that remained after AT&T) selling S so that it became the property of Insightful. Then Insightful got bought by a company called TIBCO, and then‚Ä¶\nI think you get the general idea. S and S-PLUS got bought, and sold, and licensed, and merged, and acquired to the point that it no longer really exists in any meaningful, public way. But by the 2000s, that didn‚Äôt matter.\n\n\n\n\n\n\nBakker, Arthur, and Koeno P. E. Gravemeijer. 2006. ‚ÄúAn Historical Phenomenology of Mean and Median.‚Äù Educational Studies in Mathematics 62 (2): 149‚Äì68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. ‚ÄúA Brief History of S.‚Äù In Computational Statistics, edited by Peter Dirschedl and R√ºdiger Ostermann, 81‚Äì110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. ‚ÄúCase Study 1: AT&T Divestiture.‚Äù Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nShustek, Leonard J. 2016. ‚ÄúProgramming the ENIAC: An Example of Why Computer History Is Hard.‚Äù May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. ‚ÄúThe Medicalization of ‚ÄòHomosexuality‚Äô.‚Äù Honors Capstone Projects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTukey, John W. 1972. ‚ÄúData Analysis, Computation and Mathematics.‚Äù Quarterly of Applied Mathematics 30 (1): 51‚Äì65. https://doi.org/10.1090/qam/99740.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem.‚Äù Journal of Math 58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "0R_references.html",
    "href": "0R_references.html",
    "title": "References",
    "section": "",
    "text": "Bakker, Arthur, and Koeno P. E. Gravemeijer. 2006. ‚ÄúAn\nHistorical Phenomenology of Mean and\nMedian.‚Äù Educational Studies in Mathematics\n62 (2): 149‚Äì68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. ‚ÄúA Brief History of\nS.‚Äù In Computational\nStatistics, edited by Peter Dirschedl and R√ºdiger\nOstermann, 81‚Äì110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. ‚ÄúCase\nStudy 1: AT&T\nDivestiture.‚Äù Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of\nSexuality. Vol. 1. 3 vols. Random House.\n\n\nShustek, Leonard J. 2016. ‚ÄúProgramming the ENIAC:\nAn Example of Why Computer History Is\nHard.‚Äù May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. ‚ÄúThe Medicalization of\n‚ÄòHomosexuality‚Äô.‚Äù Honors Capstone\nProjects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTukey, John W. 1972. ‚ÄúData Analysis, Computation and\nMathematics.‚Äù Quarterly of Applied Mathematics 30 (1):\n51‚Äì65. https://doi.org/10.1090/qam/99740.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to\nthe Entscheidungsproblem.‚Äù Journal of Math\n58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "0A_resources.html",
    "href": "0A_resources.html",
    "title": "Appendix A ‚Äî Getting Help",
    "section": "",
    "text": "The contents of this appendix are not here!\n\ngetting help in R (? and ??)\nthe two types of help pages, and the structure of each\n\ndataset pages\nfunction pages\n\nCheatsheets\ndocumentation online\n\nrmarkdown\nquarto:\n\nquarto guides\nquarto reference\n\nrstudio user guide\ntidymodels (for machine learning, mostly)\ngoogle and stack exchange\n\nThere is a general guide to getting R help, and it includes a suggestion I will forward: use Google and include the term ‚ÄúR‚Äù in the search",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Getting Help</span>"
    ]
  }
]