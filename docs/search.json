[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hunterr_2024",
    "section": "",
    "text": "Preface\nListing¬†1: greeting\n\n\nprint(\"Good Morning! ü§ó\")\n\n\n\n\n[1] \"Good Morning! ü§ó\"",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#this-book",
    "href": "index.html#this-book",
    "title": "hunterr_2024",
    "section": "This book",
    "text": "This book\nThis document you are reading now is called a Quarto book. We will talk more about Quarto later, but for now, all you need to know about Quarto books is that they:\n\nare relatively simple to construct,\ncan contain R code snippets and their outputs,\nare pretty enough for me to be temporarily satisfied.\n\n\nFeatures of This Book\n\n\n\n\n\n\ncallouts\n\n\n\n\n\nThis book let‚Äôs me put many of my least relevant tangents in these collapsible little notes.\n\n\n\nThe features of this book, which are achieved easily through Quarto, explain its format. Code snippets appear in this book, but only if you want them to. Each code snippet is collapsible by clicking the triangle next to ‚Äúshow code for this result‚Äù. If you really hate code, you can click the ‚Äú&lt;/&gt; Code‚Äù button on the top of each page and hide (or show) all the code at once. Speaking of the ‚Äú&lt;/&gt; Code‚Äù button, if you click ‚ÄúView Source‚Äù you can see all of the code that was used to create the page.\nQuarto also lets me show you ‚Äúpaged tables.‚Äù In the following code, I generate 1000 random numbers between -10 and 10, which I label x. Then, I apply the mystery_function to each number, producing y. What does mystery_function do?\n\ntable_1 &lt;- tibble(\n  x = runif(n = 1000, min = -10, max = 10),\n  y = mystery_function(x)\n)\ntable_1\n\n\n  \n\n\n\nQuarto also let‚Äôs me show you plots, like the one below. What do you think mystery_function does now?\n\nqplot(x = table_1$x, y = table_1$y) +\n  labs(x = \"x\", y = \"mystery_function(x)\")\n\n\n\n\n\n\n\n\nIn reality, mystery_function just squares x, so:\n\\[\n\\mathtt{mystery\\_function}(x) = x^2\n\\tag{1}\\]\nAll of the chunks of code have line numbers. For any chunk, a little clipboard appears when you hover over the code listing, and you can copy it by clicking on the clipboard. (This includes the code in the ‚ÄúView Source‚Äù pane, meaning you can copy the entire page, text and all.)\nQuarto also processes citations, so I can, for example, easily direct you towards my two fathers: Turing (1936) and Foucault (1978). It will also process internal links so that I can, for example, send you back to the top of the page: Listing¬†1.\nFinally, Quarto allows me to annotate code, which is helpful to explain how it works when a high-level of technical detail is needed. For an example of those annotations and when they might be helpful, look to Figure¬†2.1.\n\n\nTools\nR is an free and open-source statistical programming language.You use R by typing commands into an R console, which looks like this:\n\n\n\n\n\n\n\nRStudio\n\n\n\n\n\nRStudio is a graphical user interface (GUI) and interactive development environment (IDE) with which to use R. This means that RStudio contains an R console (lower left) and a variety of other tools (right), like a text editor (upper left). RStudio looks like this:\n\n\n\n\n\n\n\n\n\n\nTidyverse\n\n\n\n\n\nTidyverse is a collection of R packages that are designed to work well together and to acomplish data analysis tasks. The Tidyverse packages were developed by Hadley Wickham primarily, but also by teams of his collaborators.\nBecause Tidyverse is set of R pacakges, there is not picture to provide. Tidyverse exists as R functions that you can (and will) use in your code.\n\n\n\n\n\n\n\n\n\ngrammar of graphics\n\n\n\n\n\nThe grammar of graphics refers to another set of R packages, exemplified by ggplot2, which is included in the tidyverse. The grammar of graphics is the tool that most R programmers use to produce the ‚Äúproduction-quality graphics‚Äù for which R is known. Once you begin to produce plots with ggplot2, you are likely recognize that you have been seeing these plots for years.\n\n\n\nI believe that these tools will allow you to accomplish the vast majority of data analysis tasks, and that knowing how to use them will result in you understanding data analysis on a deeper level and being able to do it faster. That being said, these are not the only tools for data analysis. I encourage you to explore others, as well; particularly, if you intend to do a lot of work with text data, I would suggest python.\nBecause my audience does not intend to become ‚Äúprogrammers,‚Äù per se, I would also like to introduce you to the following tools, which allow R code to be integrated into readable documents instead of writing R code in separate text files with R code only (called scripts). R scripts have a place, to be clear, especially while you are learning. However, if you would like to share the analysis that you do with an R script, sharing that R script is not a good way to do that. When you send someone an R script, you are sending them a bunch of code, not an analysis\nThe current gold standard for the sort of work you are most likely to want to produce (reproducible research) is this - the document you are reading now. This is a document which includes code, the output of that code, and text explaining that code and providing context. This approach is called literate programming. The tools used to produce literate programming documents with R are:\n\nRMarkdown: a document preparation software based on R and a markup language called markdown that is mainly used to make static documents (like appendices to a journal article). RMarkdown uses a software called pandoc to turn the .rmd file into: a pdf (latex or a beamer presentation), a .html website, a word document, a power-point, and a lot of other formats you‚Äôre likely to never use.\nQuarto: a very similar, but more advanced and comprehensive software than RMarkdown. Most of the things you can do in Quarto are also possible in RMarkdown, like adding cross references (like this Equation¬†1), creating books (also like this), creating interactive data dashboards (which seems particularly trendy as of late), and creating blogs and websites. Quarto is made by the same company that makes RStudio.\nShiny: a software (written in R) that allows you to create interactive plots, which may be helpful if you are, say, trying to decide the optimal number of bins in a histogram. You could use shiny to create a histogram and a bins slider, so that you could easily see a variety of different bin sizes merely by moving a slider (instead of by writing, modifying, and rewriting code to achieve the same end).\n\nMy goal for you is to write code that is readable and to put that code inside documents that are actively fun and/or interesting to read. (That‚Äôs also, incidentally, my goal for me.)\nWe are also going to do statistics! There are two approaches to statistics that we‚Äôre going to adopt through these lessons, so I‚Äôd like to begin by elucidating the way in which these approaches are different. I am more familiar with the first approach (exploratory data analysis). Confirmatory data analysis uses many of the same tools (like hypothesis testing, which I will show you), but it uses them in a different and moer complicated way. The ultimate goal of both approaches is to predict the result of measurements.\n\nexploratory data analysis: If the purpose of a statistical model is to predict data, then a model that makes the most accurate prediction is the best model. The model creation process is iterative. Once you see the results of a model, you can use the results to modify the model itself. Generally, practitioners recognize that there are a variety of different types of models that could be used for any task. Thus, they usually construct a variety of different models and then compare them to select a final model. Practitioners will use numbers (in diagnostic and statistic tables) or visuals (like a residuals plot) when comparing models.\nconfirmatory data analysis: In the best case, at least according to the ‚ÄúOpen science‚Äù framework, the final statistical model will have been selected and preregistered before data is even collected. Statistics have to be rigorous to mean anything, a fact which the machine learning people (who do only exploratory work) ignore. They don‚Äôt check model assumptions using statistical tests like the Shapiro-Wilk normality test, and they are constantly ‚Äúp-hacking‚Äù and ‚ÄúHARKing‚Äù to forcibly extract findings from their data. Confirmatory data analysis rejects these practices, aiming instead for statistical models that are pre-specified (pre-registered), theoretically-based, and rigorous (whatever they take that to mean).\n\nI‚Äôm often somewhat flippant about the second approach, which suggest and attempts to discover Truth where I am skeptical it exists. In any case, the second approach is the only one that is taught in statistics courses. This is a mistake. Firstly, exploratory data analysis is much more commond. Secondly, it is easier to get started doing exploratory (rather than confirmatory) data analysis. Because both of these approaches adopt many of the same tools, it seems to me that starting with exploratory data analysis and trying to make that make sense is the most effective way to learn confirmatory data analysis (which will require additional effort and research that I can‚Äôt provide).\nThus, I intend to provide you with a strong understanding of data that you can directly apply to exploratory data analysis tasks. My hope is that this understanding will enable you to complete a diversity of tasks, including confirmatory data analysis, if that is of interest.\n\n\nLearning Objectives\nThere has to be some boring stuff because pedagogy. I have quite a few learning objectives for you, forming one big list, but I‚Äôll attempt to section them off so they are easier to read.\n\nR Learning Objectives\n\ndiscuss R as a language with a history: use knowledge about the history of R (and of scientific computing more generally) to describe what R is, what people ‚Äúsay‚Äù in this language, and why this language has the properties and characteristics that it does.\nR competence: read R expressions written by others (allowing the language to serve a communicative purpose), and write R expressions that are readable and align with best practices within the open source R community.\nR‚Äôs friends: Describe R‚Äôs relationship to RStudio, RMarkdown, Shiny, Quarto, and Tidyverse; and, describe what each of these tools is and why someone would use them.\n\nrun R code in several different ways: via the console, a script, and Quarto or RMarkdown documents.\n\ndescribe R‚Äôs data types and the use of each: strings, numerics (floating point ‚Äúdoubles‚Äù, integers, and complex numbers), logicals, datetimes, and factors\ndescribe R‚Äôs data structures and the use of each: including, vectors (1-dimensional arrays), matrices (2-dimensional arrays), arrays (more than 2-dimensional arrays), lists (key-value pairs), data frames, and tibbles\naccess R documentation, and read it effectively enough to solve a problem\n\n\n\nComputation Learning Objectives\n\ngenerate synthetic data: use simulation of simple events (like the rolling of dice or flipping of a coin) to gain visual intuition for the central limit theorem and the law of large numbers\nuse the Monte Carlo simulation framework to evaluate statistical tests (e.g., by determining what happens when assumptions are violated)\nprocess string data: convert strings to all upper or lower case, add prefixes or suffixes, splitting strings apart\nprocess numeric data: scale and center numeric data and write functions to accomplish non-standard transformations\nprocess language data: apply the principles of natural language processing to pre-process text data (by tokenizing and stemming text and describing both of those processes and why they are used)\n\n\n\nStatistics Learning Objectives\n\nnull hypothesis significance testing: use R to perform null-hypothesis significance tests, such as the one-sample, two-sample, and repeated measures t-test\nregression: use R to perform linear and logistic regressions, including regressions with polynomial terms\nmachine learning: use R to perform a more complicated machine learning task, likely by constructing a decision tree and a random forest classification model\ndimensionality reduction: perform principal component analysis and construct a latent semantic space, and explain why these two seemingly distinct methods are connected by singular value decomposition\n\n\n\nData Science Learning Objectives\n\nrectangular data: use the tidy data framework to read, write, and pre-process rectangular data in a consistent, efficient, and minimally complex manner.\n\nimport data from a variety of sources including: comma-separated values (.csv) files, excel spreadshees (.xlsx files), google sheets spreadsheets\n‚Äòtidy‚Äô data into the following format: one observation per row, one variable per column, one value per cell\nuse available tools that enable you to store data in a very consistent format with very little effort\n\nlanguage data: use R to pre-process, analyze and visualize text data\nvisualization: use R and the grammar of graphics (represented by ggplot2 and related packages) to visualize data and to share data visualizations with others\npublication: use RMarkdown or Quarto to conduct a linear or logistic regression, and to report and interpret the results of those tests\n\n\n\n\n\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem.‚Äù Journal of Math 58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_lore.html",
    "href": "01_lore.html",
    "title": "1¬† The Lore",
    "section": "",
    "text": "1.1 Early 19th Century: the birth of programming\nThe idea of a programmable computer is not difficult to understand. The primary goal is to make a machine that you can use for different tasks, depending on what program you feed to that machine. Programs are written in code, which today is stored as text files on a computer. The programmable machine also lives within the computer, so running the program is as simple as telling the machine part of the computer where the program is located; then, the machine part of the computer tries to read find the and read the program at the specified location, and the (hopefully) program runs.\nFolks in the 19th century did not have access to digital text files, and so they could not write their programs on them. How did they write programs, and which sort of programs did they write? These are the primary questions I hope to address in this very first section?",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#early-19th-century-the-birth-of-programming",
    "href": "01_lore.html#early-19th-century-the-birth-of-programming",
    "title": "1¬† The Lore",
    "section": "",
    "text": "1.1.1 Jacquard‚Äôs Loom: the punched card\nOne of the first programmable machines was Jacquard‚Äôs loom, which Jacquard patented in 1804. Jacquard was a weaver. Jacquard‚Äôs loom was a machine that could create a variety of different patterns, depending on which program was put into it?\nSo, Jacquard wrote programs to produce beautiful woven fabrics, but more important is how he wrote programs. Jacquard had no text files, so instead he developed a different form of machine input: the punched card. Each line on Jacquard‚Äôs punched cards contained information about a single row of the design. The cards could be fed sequentially into the loom to produce a large pattern.\n\n\n1.1.2 Babbage, Lovelace, and the Analytical Engine\nGeneral-purpose digital computers, the sort of computers that can run R, emerged as an idea in the early-to-mid 19th century. Up to that point, computers were either mechanical (mechanical computers are fascinating, by the way) or just humans.\nOne of the first to develop a design for a general-purpose computer was Charles Babbage, working in the early part of the 19th century. In the 1830‚Äôs he proposed a massively complicated, general-purpose, steam-powered computer, which he called the analytical engine. The computer was only capable of carrying out the four basic operations of arithmetic: addition, subtraction, multiplication, and division; it was designed to take input via punched card, just like Jacquard‚Äôs loom.\n\n\n\n\n\n\nLady Lovelace\n\n\n\n\n\nDuring the 1830‚Äôs and 1840‚Äôs, Lady Ada Lovelace communicated with Charles Babbage (and several others involved in similar work) with the intention to collaborate with him in studying the analytical engine. It was Lady Lovelace who wrote the first substantial computer program, whose purpose was to compute Fibonacci numbers (Tibees 2020). Her program, written in the iconic note G, used only the four simple arithmetic operations.\nLovelace was interested in discovering the capabilities of the analytical engine. Her program computing Fibonacci numbers was important because it used loops in computation. Lovelace, daughter of the poet Lord Byron, was also interested in non-mathematical applications for the machine. She suggested that a sufficiently mathematical theory of sound could enable to engine to compose complex and scientific symphonies (Tibees 2020). Isn‚Äôt that beautiful!",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#middle-and-late-19th-century",
    "href": "01_lore.html#middle-and-late-19th-century",
    "title": "1¬† The Lore",
    "section": "1.2 Middle and Late 19th century",
    "text": "1.2 Middle and Late 19th century\n\n1.2.1 1830-1870(ish)\nThe middle of the 19th century was a period of massive global shifts. Liberation from enslavement was spreading across the globe after the Haitian revolution left white men horrified at the peculiar institution, which was also losing economic utility (see Capitalism and Slavery by Eric Williams).\nAlso within that 40 year period was:\n\nabolition (as a matter of law, anyways): British Empire (1834), French Empire (1848), Russian Empire (1861), Dutch Empire (and Dutch East India Company; 1863-73), American Empire (1865), Portuguese Empire (1869)\nSamuel Colt‚Äôs invention of a revolver that can be mass-produced (1836?)\nthe development of the telegraph (1830s)\nthe trail of tears (starting 1836)\nthe revolutions of 1848 and the publication of the communist manifesto\nthe first woman‚Äôs rights convention in the U.S. (Seneca Falls Convention, 1848)\nthe discovery of the Bessemer Process which enables the mass-production of steel, paving the way for emerging steel tycoons (1855)\nDarwin published On the Origin of Species (1859)\nGatling‚Äôs invention of the machine gun (1861)\nMaxwell publishes his equations, proposing an incredibly successful theory of physics that understands electricity, magnetism, and light as essentially the same thing (1861)\nthe construction and openning of the Suez Canal (1860‚Äôs)\nMendel‚Äôs publication of his laws of genetic inheritance (1865)\nthe discovery of the cell and subsequent elaboration of cell theory (1865 and after)\nNobel‚Äôs invention of dynamite (1867)\nMarx‚Äô publication of the first volume of capital (1867)\nthe completion of the transcontinental railroad (U.S., 1869)\nMendeleev‚Äôs publication of the first periodic table (1869)\n\nIn this revisionist history of the computer (and ultimately of R), this period in history marked a transformation of power. The structure and organization of society was changing along with the flow of people, ideas, and commerce. Western, liberal democracies had to develop new technologies of population control in order to prevent all of these liberal changes from challenging their position of authority and power.\n\n\n1.2.2 Late 19th century\nWith the relative liberation of black bodies (and other bodies, as well) came a scientific imperative. Power continued to demand that these bodies be inferior, but evidence of inferiority was no longer to come from the conditions and dimensions of the body. Nay, the newly-available technologies of genetic inheritance and natural selection allowed a regime of a new flavor to take hold, one that cited hard science to support and justify the inequities in society. Inferiority was moving through the skin, into the body, and - importantly - into the mind.\nWilhelm Wundt opened the first psychology lab, and William James delivered the first psychology course and textbook. Galton, who was studying intelligence, popularized the idea of the median (Bakker and Gravemeijer 2006). Psychology and with it psychological statistics, was beginning to take shape to meet the new demands of the state: a theory and a technology that will find permanent, internal traits upon which to stratify society into haves and have-nots. The story of the emergence of psychological statistics is incomplete without mention of eugenics. The tools being developed were not neutral and scientific, but overtly political, aimed at achieving the goals of the state.\nAlso in the late 19th century was what Foucault called the implantation of perversions (Foucault 1978) - the creation of new symbolic threats to the body and to society as a whole. This operated through the invention of new characters that continue to exist within society today.\nFirstly, there was the medical specification of the homosexual (Townsend 2011). This began in 1864 with the work of Karl-Heinrich Ulrichs, who was gay himself. He specified men as either urnings or dionings. Urnings and Dionings are both male-bodied creatures, but the urning experiences the desires and character of a female (Townsend 2011). The dioning, by contrast, is normal. Discourse about the urning (renamed to the invert, and then to the homosexual) continued well into the 20th century, and the sissy (the archetype the invert represents) is, obviously, still with us.\nAlso within this time period, was the medical specification of the hysteric woman, which was initially the perogative of Jean-Martin Charcot.\nI‚Äôll mention just one more character that was invented in the later 19th century. For all of American history to this point, immigration law was about the process of naturalization - immigrants becoming citizens. From the beginning of the union, only white men of ‚Äúgood moral character‚Äù were allowed to become American citizens (Naturalization act of 1790?). There was little effort to actually prevent bodies from entering the country.\nUntil 1875. With the passage fo the Page Act of 1875, the United States declared its intention to keep undesirable bodies out of the country for the first time. Shortly thereafter, the ‚Äúillegal alien‚Äù was invented as a result of the Chinese Exclusion Act of 1882, which is the only American immigration law I am aware of that names a specific national group in its title.\nAll this to say that the nature and enforcement of undesirability were in massive flux in the late 19th century. The foreign element was moving within: the enslaved African could become a citizen and could vote, the invert or the hysteric could be hiding within anyone, and the state took up the power to deport bodies that did not belong. No longer was the anthropologist writing about the inferiority of foreign peoples (although to be clear, they absolutely were still doing that); the pschiatrist was now writing about our own inferiority.\nI consider the birth of statistics to be in this time period, which does not have pleasant implications for statistics as a field. There is a lot more to be said about the advent of statistics, and how statistics is designed to serve power (i.e., fulfill the demands of the state). However, I‚Äôm going to leave all of that unsaid and refocus on computation in general, and statistical computing in particular.\nThe late 19th century was also, notoriously, the era of massive trusts in the United States. These monstrous, monopolistic companies exploited both the consumer and the worker, but the United States did not yet have a legal mechanism for breaking them up. The most important monopoly for our purposes: the one that is most influential is the development of S and then R is the AT&T monopoly.\nAnother monopoly was also forming. Using Jacquard‚Äôs punched cards, an American man designed and patented a system to read punched cards. In 1890, this punched card system was used to complete the census, resulting in the 1890 census being completed two years quicker than the 1880 one. The company that developed this technology would go on to become IBM, which enjoyed monopoly status in the computing industry for several decades.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#sec-early-19th-century-the-advent-of-computing",
    "href": "01_lore.html#sec-early-19th-century-the-advent-of-computing",
    "title": "1¬† The Lore",
    "section": "1.3 Early 19th century: the advent of computing",
    "text": "1.3 Early 19th century: the advent of computing\nNear the end of the 19th century, a mathematician named David Hilbert decided that mathematics needed to be formalized. Up to that point, it had developed as myriad sub-disciplines that failed to cohere into a single, interconnected web of mathematics. Hilbert believed that it should, and his goal was to formalize this system. He believed that such a system (of mathematical axioms) needed to have three properties:\n\nto be consistent: it should not be possible to derive that a statement is both true and false\nto be complete: it should be possible to derive the truth of every true statement (or the falsity of its negation)\nto be decidable: there must be an algorithm that can identify all and only true statements in a finite number of steps.\n\n(The excitement about formalizing affected Hilbert, but by no means was he the first or the only to be caught up in this mess. Notoriously, Whitehead and Russel got spun up enough to publish a 126-page long proof that \\(1+1=2\\). I‚Äôm mostly attributing these three demands to Hilbert for sanity‚Äôs sake because I cannot stand to write out the sordid details. These three ‚Äúproperties‚Äù as I call them, are really inspired very loosely on any specific, cite-able Hilbert publication. He did publish a list of 23 questions, which refer to the properties I mention here, but understand this as a drastically over-simplified view of the mathematical debates unfolding at the time.)\nMathematics was not the only field to be heating up. There was growing speculation in physics that matter may not be as continuous as was previously assumed. In 1900, Max Planck published the first quantum theory in physics, which was aimed at modelling thermal radiation. Shortly thereafter, Albert Einstein published another quantum theory, this time aimed at modeling the the photoelectric effect. Both of these models used quantum stuff (i.e., minimal, discrete units of energy, creating measurements of energy that are always a multiple of the quantum unit), but the authors did not actually believe the world was quantum. Famously, Einstein‚Äôs theories of relativity both rely on space-time being continuous. They merely believed quantized math was the best way to explain non-quantum physical phenomena.\nNeils Bohr went the whole way, creating his model of the atom, with distinct, orbital electron shells. In the 1920‚Äôs quantum mechanics, as we know it today, came into existence. It did not make Einstein happy. Einstein wanted a deterministic world, where each cause has an specific, reliable effect. Quantum mechanics is not a deterministic theory of physics, but a probabilistic one. I take this diversion into the physical sciences not only to stress that this is a transition period within the physical sciences, but to temper my claim from the previous section. The ‚Äúdemands of power‚Äù did no less to supercharge the development of statistics and probability than did rapid changes in the way we understand and model the physical world.\nDuring my quantum mechanical tangent, G√∂del has proven that achieving the second property of Hilbert‚Äôs idealistic system is unlikely. In fact, G√∂del establishes that it is logically impossible that any formal mathematical system could be complete, as defined above.\nTo answer the question about whether mathematics is decidable, a new technology is needed. Before a mathematician can make formal claims about the capabilities or limitations of algorithms in general (as Hilbert demanded), she must first provide a rigorous definition of an algorithm. Two mathematicians took up this task, Alonzo Church who developed the lambda calculus, and Alan Turing who developed the Turing machine. Both men reached the same conclusion: mathematics cannot be decidable. It is logically impossible to make an algorithm (a Turing machine) that can identify all and only true statements (Turing 1936). There are, as it turns out, hard limits on the types of problems algorithms are able to solve (at least in a finite number of steps).\nThus, Turing half accidentally created the field of computer science while trying to answer a question about the foundations of mathematics. This is also an opportune time to introduce the term Turing-complete which refers to anything (model of computation, programming language, a book of instructions used by a human computer) that can simulate the a Turing machine. Any Turing-complete system is essentially equivalent to the original Turing machine described in (Turing 1936). The analytical engine is (theoretically, of course, it never got built) Turing-complete; Jacquard‚Äôs loom, by contrast, is not. Modern programming languages are, for the most part, Turing complete, meaning that any function you write in a modern programming language could be performed on the OG Turing machine from (Turing 1936).\nThe first electric, digital computer was not fully constructed until 1945. It was built by and for the U.S. military, who named the machine ENIAC. Thus, the first computations done on an electric, digital computer were intended to speed up the process of human and earthly destruction. ENIAC was a bunch of coordinated units that ran according to the placement of wires on the machine (Shustek 2016). The machine took IBM punched cards as input (remember the punched card monopolist from the end of the 19th century?).\nInitially, the wires on ENIAC had to be moved for each new problem (Shustek 2016). The process of re-configuring the machine for each new problem was tedious, but it was possible, and so ENIAC was Turing-complete. However, having to physically move wires prevented the machine from achieving the utility of a modern programmable computer.\nThis machine was very quickly modified in a way that dramatically changed its function. Instead of having to move wires, and then feed the machine (punched card) instructions based on the position of those wires, it would be much faster permanently code instructions (functions) into the machine. Then, the input of the machine could describe the sequence of functions. You could achieve looping by instructing ENIAC to perform a function repeatedly and conditional (if-statement) execution by instructing ENIAC to skip functions in the sequence.\nThis is the idea behind modern programming languages. Instructions for the computer, written in the computer‚Äôs language (ENIAC‚Äôs language was wires, the one we‚Äôll soon focus on is R) are stored within the machine. ‚ÄúProgramming‚Äù the machine involves telling it which instructions to perform and in which order. In 1948, the first ENIAC ‚Äúprogram‚Äù ran under this new computer architecture was a Monte Carlo simulation of neutron decay during nuclear fission (Shustek 2016).",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#att-bell-labs-and-s",
    "href": "01_lore.html#att-bell-labs-and-s",
    "title": "1¬† The Lore",
    "section": "1.4 AT&T, Bell Labs, and S",
    "text": "1.4 AT&T, Bell Labs, and S\nMonopolies suck, and AT&T did as well. Throughout the beginnning of the century, it gradually became clear that the benefits of a monopolist teleohpne provider were not going to materialize. In 1949, the U.S. Department of Justice sued AT&T for violating the anti-trust act, and the resulting 1956 consent decree prohibited AT&T from entering the computer business (Chang, Hen, and Kan n.d.).\nThis consent decree did not prevent the further degradation of AT&T‚Äôs service, nor did it prevent future anti-trust lawsuits. Throughout the 60‚Äôs and early 70‚Äôs, the U.S. government dogged AT&T with recurrent anti-trust lawsuits. In 1974, the Department of Justice began their final lawsuit against a monopoly AT&T. Over the course of the next decade, the government proved that AT&T was leveraging its monopoly power to predatory ends, annihilating potential competitors and pricing services far beyond the cost to provide them. This lawsuit ended in 1982 with the dissolution of AT&T into 7 Regional Bell Operating Companies (Chang, Hen, and Kan n.d.).\nBell Labs was probably the most important laboratory within AT&T. During the early 70‚Äôs, the researchers at the statistics research department within Bell Labs was using the programming language FORTRAN. FORTRAN is a general purpose, compiled programming language, developed by IBM (the punched card guys).\n(This isn‚Äôt super relevant, but I think it‚Äôs fun. Before 1968, computational statisticians had been using a algorithm called RANDU, which was a FORTRAN function that generated random numbers. In 1968, a mathematician proved that the allegedly random numbers actually all had to lie on a series of parallel hyper-planes, and we thus not actually random. wtf is a series of parallel hyper-planes? See below)\n\n\n\n\n\n\n\n\n\nFORTRAN, the name, stands for ‚Äúformula translation,‚Äù and it was primarily used for scientific computing, like computing weather models or doing computational physics - things that have to do with numbers, essentially. It is still used in these fields to some extent, although it is less popular for scientific computing than other, more recent programming languages, like R. FORTRAN is, computationally speaking, incredibly efficient, mostly by natively supporting parallel computation. For this reason, FORTRAN is still used to benchmark supercomputers. You can learn more about FORTRAN on its website.\nIn any case, in the 1970‚Äôs, the statistics research department at Bell Labs found FORTRAN to be somewhat insufficient, and they set out to develop a new language that would more fully suit their needs (Becker 1994).\n\n1.4.1 S\nS is a statistical computing language that was developed first at Bell Laboratories in the mid 1970s. At the time, statistics was undergoing a change. Previously, statistics had been developing as a set of methods - essentially algorithms that prescriptively described how to complete a statistical analysis from beginning to end. In the early 70‚Äôs, John Tukey was working at Bell Labs and at Princeton, and he was making a lot of noise about the problems with statistics. He popularized a different approach to statistics, establishing something of a binary between data analysis and statistics, just as I did between machine learning and statistics (in Section 1.2; Tukey (1972)).\nThe statistics research department was beginning to demand a tool that aligned with Tukey‚Äôs approach. FORTRAN, developed more than a decade before that demand was created at and by Bell Labs, did not measure up to the task. Instead, they decided to develop a new language, which they named S. Initially, there was a large focus on being able to import FORTRAN functions into S, so that there could be a smooth transition from FORTRAN to S within Bell Labs.\nS was built from the ground up to include graphics capabilities, and a structure that enabled and encouraged exploratory data analysis. The basic data structure in S is a vector of like-elements, which were used to make matrices and time-series; S also included lists (key-value maps) and the $ operator, which could be used to retrieve specific components of larger data structures (Becker 1994). It also included all of the arithmetic operators that you need in a desk calculator, making it useful for that purpose, as well.\nIn 1980, S was distributed outside of Bell Labs for the first time. Initially, it was distributed for a nominal fee and for educational use only, but by 1981 it was widely available (Becker 1994). After it began to be distributed, the developers added explicit looping (i.e., for loops), as well as the apply function, which could be used to loop over a vector while applying a function (Becker 1994). The developers also introduced the ‚Äúcategory‚Äù, which is now called the factor in R. Categories are vectors of data. They merge numerical and string data types - each entry in the vector is assigned a category label (so that you can read it), as well as a underlying integer (so that you can do math with categories).\nAlthough S was developed initially by statisticians, it clearly had utility as a data manipulation, graphics, and exploratory data analysis tool. In 1988, the developers released the ‚ÄúNew S,‚Äù renaming the software after some significant changes. The most significant feature of New S was the inclusion of first class functions, which are functions that you can assign to a name and and then refer to by that name. Functions are first class in that they are S objects, just like any vector or matrix. For the first time, S had depreciated functions, which R also has. Depreciated functions are functions for which there is a better alternative. They are generally still included in R and S distributions (so old code that uses depreciated functions can still run), but it‚Äôs best to avoid using them (and to use the better alternatives instead). By 1988, many of the FORTRAN functions from the initial development of S were rewritten in C, which is a general purpose programming languages on which New S is built (Becker 1994).\nIn 1991, the S development team expanded, and there was a focus on adding statistical software to the S language. Although S was developed by statisticians who intended to use it for statistics, the statistics are not inherent in S: ‚ÄúS is a computational language and environment for data analysis and graphics‚Äù (Becker 1994). As such, the developers added the formula class, which could be used to specify statistical models. The formula is marked by the ~ operator, with the dependent variable on the left and the independent variable(s) on the right (e.g., y ~ x + w + x*w).\nAlso in the 1991 release was the data.frame. Matrices are like vectors: they can only have one type of data. If you have a matrix that has even one number in it, then the entire matrix must be numeric, even if you want to use it to represent string data (like names and job titles) or categorical data (Becker 1994). So, a matrix is a combination of multiple vectors, all of the same type. A data.frame, by contrast, is a combination of vectors of any type. You can have a string vector (column) in the data frame representing job title, as well as a numeric vector representing income. As with matrices, you can use the $ operator to pick a vector out of the data frame (e.g., data$income picks out the income vector in the data frame called data).\nIt‚Äôs not really possible for a programming language to die. As we have seen with FORTRAN and S, new programming languages often use code from their older counterparts, especially at the beginning. Even though I can no longer find S on the internet and run it on my computer, a very large number of S functions continue to exist in R.\nI am able to find relatively scant documentation about this final period in the history of S, so the rest of this section is at least somewhat speculative (except claims that are cited, of course).\nWhat is the need for R if S exists? Well, well, well. Let‚Äôs talk about corporate fuckery, which both killed S and prevented it from dying. I have been making a much bigger deal over anti-trust law than the vast majority of those who introduce R to their students. To this point, as far as AT&T and Bell Labs are concerned, I have presented a world in which they are legally prohibited from selling computers (and presumably, computer software) as a result of the Consent Decree from 1956.\nUp to this point, no one was making money off of S. Although Bell was initially charging folks a nominal fee to use the software (Becker 1994), this practice ended quickly, meaning that the software was being distributed for free. As a result of the anti-trust, Bell Labs was not going to monetize this technology. Instead, one of their former employees had to do it.\nIn the 70s and 80s, the graphical user interface (GUI) was being born. This emerging technology came with a new generation of capitalists who had not been subject to extensive anti-trust, in which former trade union president Ronald Reagan did not believe - the capitalists who bring us Microsoft and Apple, who own outright the operating systems of about 85% of the worlds‚Äô computers (and many phones and other devices, as well).\nS wasn‚Äôt fated to become Windows; it was fated to become S-PLUS. S-PLUS is/was a statistical computing software with a graphical user interface. It was developed by a company owned by a former Bell Labs employee and University of Washington professor, R Douglas Martin. His work is primarily in econometrics, and he has extensively published about investment risks. Because of this, and because S-PLUS was and is mostly used by economists, a cynic might call it an application to be used for those who are unwilling or unable to learn how to code (similar in character to Microsoft‚Äôs SPSS). S-PLUS started circulating (for a fee) in about the year 1987, and it did include features that S did not (like generalized linear models).\nLet me just quickly recap, so I can make sure everyone is oriented in time - I‚Äôm discussing a lot of events that overlap and are not all well documented. In 1980, S was released to the public; in 1988, S had a significant update, becoming ‚ÄúNew S‚Äù; around 1987, a former employee of Bell Labs developed S-PLUS; in 1991, S had an update that focused on statistics.\nIn 1991, two statisticians quietly began work on the project (R) that would more-or-less kill S and S-PLUS.\nIn 1993, S and S-PLUS were reunited when Bell Labs sold S to the company that had developed S-PLUS. That company, in turn, immediately merged with a company called MathSoft. S-PLUS was only available on windows, and its relationship with Microsoft strengthened when features were added to connect S-PLUS to Excel and to SPSS.\nPart of the company (MathSoft) was sold, it got renamed (to Insightful), the exclusive license to distribute S turned into AT&T (i.e., Lucent, one of the companies that remained after AT&T) selling S so that it became the property of Insightful. Then Insightful got bought by a company called TIBCO, and then‚Ä¶\nI think you get the general idea. S and S-PLUS got bought, and sold, and licensed, and merged, and acquired to the point that it no longer really exists in any meaningful, public way. But by the 2000s, that didn‚Äôt matter.\n\n\n\n\n\n\nBakker, Arthur, and Koeno P. E. Gravemeijer. 2006. ‚ÄúAn Historical Phenomenology of Mean and Median.‚Äù Educational Studies in Mathematics 62 (2): 149‚Äì68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. ‚ÄúA Brief History of S.‚Äù In Computational Statistics, edited by Peter Dirschedl and R√ºdiger Ostermann, 81‚Äì110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. ‚ÄúCase Study 1: AT&T Divestiture.‚Äù Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nShustek, Leonard J. 2016. ‚ÄúProgramming the ENIAC: An Example of Why Computer History Is Hard.‚Äù May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. ‚ÄúThe Medicalization of ‚ÄòHomosexuality‚Äô.‚Äù Honors Capstone Projects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTukey, John W. 1972. ‚ÄúData Analysis, Computation and Mathematics.‚Äù Quarterly of Applied Mathematics 30 (1): 51‚Äì65. https://doi.org/10.1090/qam/99740.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem.‚Äù Journal of Math 58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "02_r_and_friends.html",
    "href": "02_r_and_friends.html",
    "title": "2¬† R and Friends",
    "section": "",
    "text": "2.1 R and Friends\nIn 1991, two professors in New Zealand began to develop R, a process which they documented in (Ihaka and Gentleman 1996). R is very similar to S; so similar, in fact, that it is frequently called a dialect of S. What is the difference between S and R? The creators of R describe it as having the syntax of S (meaning that most examples, including the following example, can be run in S or R) but the semantics of Scope (which is a programming language from the Lisp family).\nProbably the key difference between the two languages is the lexical scoping. Whenever you use R (or most other programming languages), you have to have something called a frame. Frames include things like functions and named variables. Each function creates its own frame. The frame for the function f in the example below (from Ihaka and Gentleman (1996)) contains the named variable y and the named function g. The named function g, as a function, creates its own frame (in which to store variables and functions). There is also something called a global frame which, in the following example, includes an assignment of the value 123 to the name y and the assignment of some function to the name f.\nAs you can see, in R, running the function f with 10 as an argument results in the function returning 100 (10 times 10). In S, this very same code would have resulted in function f returning the value 123. In S, when we define the function g, S uses the global frame as the basis for the function, including the assignment of the value 123 to y. R, by contrast, creates g with a locally-scoped frame, meaning that the frame for g includes the assignment of the value x * x to the variable y (assignments which are inherited from the parent frame). Thus, in S, the function g is evaluated as print(123), but the R function is evaluated as print(x * x) (the function f is responsible for substituting x to make print(10 * 10).\nUnlike Scheme, but like S, R uses lazy evaluation. In essence, this means that R does not run your code unless it absolutely has to. I‚Äôll use the example of Figure¬†2.1 to explain what this means. Lines 2 through 6 contain the declaration of function f (even though line 4 also contains the declaration of function g. When you run line 2, all of the lines down to line 6 (where the closing bracket, } is located) get stored in R‚Äôs memory next to the name f. However, R will not run the function f until you actually go to use it (i.e., until you make the function call in line 7). This is why we call R lazy, but what‚Äôs the big deal?\nIf you make a syntax error in your declaration of function f, R is going to have to tell you that you made a syntax error at some point. In a language that is not lazy, the language evaluates function f when you store it. Thus, a non-lazy language will send you a syntax error after you run the function declaration (i.e., after you run line 2, which also causes lines 3-6 to run). If Figure¬†2.1 were written in a non-lazy language, the syntax error would occur where the 1 annotation is. However, in R, the function is merely stored when you run lines 2-6. Function f does not actually run until you call it in line 7 (marked with a 3 in Figure¬†2.1). Laziness is a feature that R inherited from S, which is also lazy.\nThis is getting a bit technical. The two men who developed R are Ross Ihaka and Robert Gentleman. On a family tree posted on Ihaka‚Äôs personal website, he lists himself as the academic grandchild of John Tukey, then statistician at Bell Labs that popularized exploratory data analysis (the framework that created the need for S, which was also, if you‚Äôll recall developed at Bell Labs).\nIhaka is a now retired statistician from the University of Auckland. Gentleman is a bioinformatician who currently works at Harvard and 23andMe. Allegedly, Ihaka and Gentleman developped R for use in teaching statistics. That was part of both of their jobs as professors, after all. However, this doesn‚Äôt seem very plausible (why would the professors write their own statistical programming languages rather than using a well-documented one which would seem to be better for pedagogy), nor have I seen any specific evidence for it. That being said, in the years 1993-94, R was stuck at the University of Auckland, being used by them, probably their peers, and less probably their students, but the software was not yet being distributed, as S or S-PLUS was.\nIn 1995, one of their colleagues convinced them to licence use of the software as free software under a GNU general public license.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>R and Friends</span>"
    ]
  },
  {
    "objectID": "02_r_and_friends.html#r-and-friends",
    "href": "02_r_and_friends.html#r-and-friends",
    "title": "2¬† R and Friends",
    "section": "",
    "text": "y &lt;- 123 # assign 123 to the name y\n1f &lt;- function(x) {\n  y &lt;- x * x # assign x times x to the name y\n2  g &lt;- function() print(y) # create new function and new scope\n  g() # return the output of function g\n}\n3f(x=10)\n\n\n1\n\nthe the beginning of the declaration of function f (between lines 2 and 6)\n\n2\n\nthe declaration of function g\n\n3\n\na function call for function f (with the argument x set to equal 10)\n\n\n\n\n\n\n\n\n[1] 100\n\n\n\n\nFigure¬†2.1: an example from Ihaka and Gentleman (1996)\n\n\n\n\n\n\n\n\n\n\n2.1.1 Free Software\nI just bolded the term free software; why? As it turns out, the term free software has a specific definition that extends far beyond the idea of ‚Äúsoftware that you don‚Äôt have to pay for.‚Äù So what is free software? Free software is characterized by the four freedoms (Foundation n.d.):\n\n\n\nFree Software Foundation‚Äôs Four Essential Freedoms\n\n\nThe idea of free software, and it‚Äôs formation in the four freedoms seen above, came to be popular in the mid-80‚Äôs after the Reagan government had made clear it‚Äôs stance (and the republican, and soon the democratic, party‚Äôs stance) on anti-trust enforcement. In the wake of the disruption to the computing (IBM) and telephone (AT&T) industries, the Reaganites declared that we were entering into an era of free, competitive trade while setting up a regulatory framework that would allow tech companies to consolidate power and market share ad infinitum, resulting in the current big 4(-ish): Apple, Alphabet (Google), Amazon, and Meta (and Microsoft, Nvidia, and potentially Tesla and like Netflix, depending on who you ask).\nIt is a good thing for us, then, that none of these companies own R, which the developers have promised will remain free software indefinitely.\n\n\n2.1.2 Open Sourcing and Crowd Sourcing\nThese days, it feels like only a real purist will call R free software. The more en vogue term is ‚Äúopen source.‚Äù The Free Software Foundation would like you to treat the terms as separate however (see this article).\nIn reality, the labels ‚Äúopen source‚Äù and ‚Äúfree software‚Äù are mostly synonymous in that they refer to many of the same software. As Freedoms 1 and 3 make clear, software must be open source before it can be free. The free software folks‚Äô biggest problem with ‚Äúopen source‚Äù is one of semantics, really. They claim that the ‚Äúopen source‚Äù movement argues too much about how free software is good for business and software development (i.e., because curious users can look for and find bugs). The free software people are not interested in these practical matters, focusing instead of the moral question of what sort of software is right and wrong. They correspondingly argue their case in the form of moral imperatives (the four freedoms).\nI am less interested in these theoretical questions, and more interested in explaining to you what the implication of free or open software is bound to be (at least in the case of R): crowd-sourced development.\nR itself provides you with basic statistical functionality. However, the vast majority of what is commonly called ‚ÄúR‚Äù is not actually part of the base distribution of R. Instead, most of the functionality is packaged within ‚Äúpackages,‚Äù which are you load into R with the library() function, as shown below:\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\nHadley Wickham\n\n\n\n\n\n\n\n\nFoundation, Free Software. n.d. ‚ÄúWhat Is Free Software? - GNU Project - Free Software Foundation.‚Äù Accessed May 9, 2024. https://www.gnu.org/philosophy/free-sw.html.\n\n\nIhaka, Ross, and Robert Gentleman. 1996. ‚ÄúR: A Language for Data Analysis and Graphics.‚Äù Journal of Computational and Graphical Statistics 5 (3): 299‚Äì314. https://doi.org/10.2307/1390807.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>R and Friends</span>"
    ]
  },
  {
    "objectID": "03_r_101.html",
    "href": "03_r_101.html",
    "title": "3¬† R 101",
    "section": "",
    "text": "3.1 Introduction\nThis tutorial is adapted from a fabulous youtube video by Very Normal. I recommend watching this video before reading.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#data-types",
    "href": "03_r_101.html#data-types",
    "title": "3¬† R 101",
    "section": "3.2 data types",
    "text": "3.2 data types\nclass: the function that you use when you want to know what type of data is stored in an R object.\n\n3.2.1 Numbers\nIntegers are specified with an L:\n\nint &lt;- 5L\nclass(int)\n\n[1] \"integer\"\n\nprint(int)\n\n[1] 5\n\n\nwithout the L, the integer will be interpreted as numeric. Numeric data are numbers that have a decimal point (in this case .0, which R does not print):\n\nnum &lt;- 5\nclass(num)\n\n[1] \"numeric\"\n\nprint(num)\n\n[1] 5\n\n\nNumeric is a super-ordinate category that contains doubles and integers. Numeric data is stored as doubles (i.e., numbers that have decimal points). You can see how a value is stored on the machine (i.e., to see that numeric values are stored as doubles, always) using the typeof function.\n\ntypeof(int)\n\n[1] \"integer\"\n\ntypeof(num)\n\n[1] \"double\"\n\n\n\n\n\n\n\n\nDouble Precision?\n\n\n\n\n\nComputers store numbers in sequences of 0s and 1s. A single position in this sequence, which could be occupied by either a 0 or a 1 is called a bit. A number stored in this way is called a binary number. Here is an example of an 8-bit binary number:\n00101101\nDecoding this number is a straightforward matter. Each position in the sequence represents a power of 2. The rightmost position represents \\(2^0\\), and the leftmost position (in an 8 bit number) represents \\(2^7\\). The number encoded is a weighted sum of the powers of 2. 00101101 is decoded as:\n\\[\n\\text{number} = \\sum_{i=0}^{7} \\text{bit}_i \\times 2^i\n\\]\nOmitting the 0s, 00101101 works out to be:\n\\[\n(1 \\times 2^2) + (1 \\times 2^4) + (1 \\times 2^5) + (1 \\times 2^7) = 4 + 16 + 32 + 128 = 180\n\\]\nYou might notice a couple of things about this scheme of an 8-bit number that I have put forth. This scheme can only represent integers (a result of using integer multiplication with powers of 2, which are always integers). You may also have noticed that one could represent 0 in this format, but never -1. All of the integers decoded in this manner will be positive.\nYou might use a sign bit at the beginning of the sequence to fix this second problem. When the sign bit is 1, the integer is negative; when it is 0, the integer is positive. The new decoding scheme using a sign bit would be represented like this:\n\\[\n\\text{number} = (-1)^{bit_0} + \\sum_{i=0}^6 bit_{i+1} \\times 2^i\n\\]\nAgain ignoring the 0s (except for the sign bit), 00101101 works out to be:\n\\[\n(-1)^0 + (1 \\times 2^1) + (1 \\times 2^3) + (1 \\times 2^4) + (1 \\times 2^6) = 1 + 2 + 8 + 16 + 64 = 91\n\\]\nWe could now easily represent -91 by changing the sign bit: 10101101.\nThis scheme still can‚Äôt represent non-integers, and our 8-bits are quite limiting. The largest and smallest numbers we can represent are positive and negative 127 (01111111 and 11111111, respectively). The limiting factor is, of course, the limited number of bits we have (only 8), and the limited amount of information those 8 bits can encode (see information theory for a formalized, quantitative way to think about information in bits - Developed by Claude Shannon at Bell Labs).\nFor historical reasons, a 32 bit number is called a single precision number. A 32 bit number encoded as described above, it is a 32 bit integer. In R, integers are stored as 32 bit numbers. This means that the range of integers R can store goes from -2,147,483,647 (-2.1 billion; 11111111111111111111111111111111) and 2,147,483,647 (2.1 billion; 01111111111111111111111111111111)1.\nDouble precision floating point numbers (doubles) are 64 bit numbers with decimal points. The two schemes I outline can only encode integers. Encoding decimal values requires a more complicated scheme that I have no intention of fully explaining (as I have for 8 and 32 bit integers). Here is a Wikipedia page if you care all that much.\nBriefly, the leftmost bit is still a sign bit, interpreted in the same we we interpreted it above. The next 11 bits represent an exponent (encoded in binary). 11 bits can encode integers from 0 to 2047. To achieve negative exponents (needed for numbers between 0 and 1 and therefore numbers between -1 and 0), the exponent is subtracted from 1023. Therefore, the range of values the exponent can take covers all the integers from -1023 to 1024. The rightmost 52 bits represent a sum of fractions that is computed as a weighted sum, very similarly to how the integers were calculated from 7 bits above. The only difference is that each bit represents multiplication by a negative power of 2. This results in a weighted sum of fractions, where only the negative powers of 2 corresponding to positions with 1s in them are included in this sum.\nIn any case, the real referent of double precision floating point number is the number of bits the computer uses to store the number. Single point floating point numbers (i.e., numbers with decimals) are stored according to a similar sign bit, exponent, fraction scheme, only with 32 bits. The benefit to having more bits is having more precision. There are, however, computational costs, and in many cases (especially if you are only using a 64 bit computer, as you are), using more bits exponentially increases the amount of time the computer will take to complete a single computation.\n\n\n\nAs you can see, the class numeric are stored as doubles (i.e, using 64 bits), whether the value being stored is an integer or not. Values with the class integer are stored as integers (i.e., using only 32 bits).\n\n\n\n\n\n\nComplex Numbers\n\n\n\n\n\nThe other type of numeric data in R is complex numbers, which are so very cool and beautiful, but which you are never realistically going to use and which I will not, therefore, bore you with the details of. However, complex numbers are numbers with a real part (which is multiplied by \\(1\\)) and an imaginary part (which is multiplied by \\(i = \\sqrt{-1}\\)). The value of the complex number is the sum of the real and imaginary parts. You can store a complex number like this:\n\nz &lt;- 0.8 + 1i\nclass(z)\n\n[1] \"complex\"\n\n\nI‚Äôll just quickly show you the seq function so you can see the effect of complex exponentiation. We can use the seq function to get a sequence of numbers from from to to, by incremebts of by.\n\nexps &lt;- seq(from = 1, to = 12, by = 0.2)\nexps\n\n [1]  1.0  1.2  1.4  1.6  1.8  2.0  2.2  2.4  2.6  2.8  3.0  3.2  3.4  3.6  3.8\n[16]  4.0  4.2  4.4  4.6  4.8  5.0  5.2  5.4  5.6  5.8  6.0  6.2  6.4  6.6  6.8\n[31]  7.0  7.2  7.4  7.6  7.8  8.0  8.2  8.4  8.6  8.8  9.0  9.2  9.4  9.6  9.8\n[46] 10.0 10.2 10.4 10.6 10.8 11.0 11.2 11.4 11.6 11.8 12.0\n\n\nWe can then raise the complex number z to each of these exponents to see the effect of complex exponentiation.\n\nzs &lt;- tibble(exp = seq(from = 1, \n                       to = 17.5, \n                       by = 0.1),\n             z = z ** exp)\nzs\n\n\n  \n\n\n\nNow we can look at the pretty spiral that complex exponentiation results in:\n\n\n\n\n\n\n\n\n\nIsn‚Äôt that just gorgeous? Complex numbers can also spiral inwards:\n\nz &lt;- 0.15 + 0.85i\n\n\n\n\n\n\n\n\n\n\nI drew a unit circle in that plot for a reason. As you can see in the plot below, the first complex number we used (\\(0.8 + 1i\\)) corresponds to a point outside of the unit circle on the complex plane. The point corresponding to \\(z = 0.15 + 0.85i\\), by contrast, lies within the unit circle. This is why the spiral of the first complex number spirals outwards, while the spiral of the second complex number spirals inwards. The unit circle is the boundary between the two types of spirals.\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.1.1 Arithmetic\nR has a number of arithmetic operators, many of which you will be familiar with.\nThe unary operators are - and +, which negate and do nothing, respectively. You would use - when saving a negative number.\nThese are five binary operators that are used frequently in the arithmetic common in public schools:\n\n+: addition\n-: subtraction\n*: multiplication\n/: division\n^: exponentiation2\n\nExponentiation and division offer two useful examples to demonstrate Inf and -Inf values. Anything divided by infinity is zero, which is more of a convention in mathematics than a thing that you calculate out (which would take a log time). This division by infinity resulting in zero rule is coded into R:\n\n10 / Inf\n\n[1] 0\n\n\nR also recognizes that anything to a negative infinite power is zero, and that anything (including an infinite value) to the zeroth power is equal to 1.\n\n10 ^ -Inf\n\n[1] 0\n\nInf ^ 0\n\n[1] 1\n\n\nIn addition to addition, subtraction, multiplication, division, and exponentiation, R has a few other arithmetic operators that you may not be as familiar with:\n\n%%: modulus (remainder of division)\n%/%: integer division (division that rounds down to the nearest whole number)\n\nThese functions revolve around division. Take the number 73. When you divide 73 by 10, you get 7 with a remainder of 3. In this case, 7 is the integer divisor of 73 divided by 10, and 3 is the modulus of 73 divided by 10.\n\n73 %/% 10\n\n[1] 7\n\n73 %% 10\n\n[1] 3\n\n\nThese functions are part of modular arithmetic, which winds up being incredibly important to computer science and cryptography, as well as to fields of mathematics like number theory.\n\n\n\n\n\n\nRounding, Truncation, etc.\n\n\n\n\n\nR has 5 function used to round numbers:\n\nceiling(): rounds to greatest and nearest integer (will increase absolute value of a positive number and decrease the absolute value of a negative number)\nfloor(): rounds to least and nearest integer (will decrease the absolute value of a positive number and increase the absolute value of a negative number)\ntrunc(): truncates the decimal part of a number\nround(): rounds to the nearest integer (or to a specified number of decimal places)\nsignif(): rounds to a specified number of significant digits3\n\n\nx &lt;- 5.7\ny &lt;- -5.7\n\nceiling(x)\n\n[1] 6\n\nceiling(y)\n\n[1] -5\n\nfloor(x)\n\n[1] 5\n\nfloor(y)\n\n[1] -6\n\ntrunc(y)\n\n[1] -5\n\nx &lt;- 13950.28738\nround(x, digits = 3)\n\n[1] 13950.29\n\nsignif(x, digits = 3)\n\n[1] 14000\n\n\n\n\n\n\n\n\n\n\n\nRelational Logic\n\n\n\n\n\nRelational logic is used to compare two values. The result of a relational logic operation is a logical value, either TRUE or FALSE. The relational operators in R are:\n\n‚Äú==‚Äù: is the thing on the left identical to the thing on the right?\n‚Äú!=‚Äù: not identical\n‚Äú&lt;‚Äù: less than\n‚Äú&lt;=‚Äù: less than or equal to\n‚Äú&gt;‚Äù: greater than\n‚Äú&gt;=‚Äù: greater than or equal to\n\nThat might seem relatively straightforward. I raise you the following complication:\n\na &lt;- 0.1\nb &lt;- 0.3 - 0.2\nprint(c(a, b))\n\n[1] 0.1 0.1\n\na == b\n\n[1] FALSE\n\n\nThat‚Äôs really odd, isn‚Äôt it? The way R stores the two numbers (see the first callout about double precision) means that these two numbers are different by a very small amount. How much, you ask?\n\na - b\n\n[1] 2.775558e-17\n\n\nAs I said, a very small amount. This is why you should never use == to compare floating point numbers. Instead, you should use the all.equal() function, which will compare two numbers to a specified tolerance.\n\nall.equal(a, b)\n\n[1] TRUE\n\n\n\n\n\n\n\n\n3.2.2 Characters\nCharacters are fairly self explanatory. You can tell R that something is a character by putting it in quotes.\n\na &lt;- \"Good morning!\"\nclass(a)\n\n[1] \"character\"\n\nprint(a)\n\n[1] \"Good morning!\"\n\n\nYou can also convert something from another class to a string. When you do so, R will begin to print it in quotes:\n\nx &lt;- 124L\nclass(x)\n\n[1] \"integer\"\n\nprint(x)\n\n[1] 124\n\nx &lt;- as.character(x)\nclass(x)\n\n[1] \"character\"\n\nprint(x)\n\n[1] \"124\"\n\n\nSee another example of the quotes with logical data:\n\nx &lt;- TRUE\nclass(x)\n\n[1] \"logical\"\n\nprint(as.character(x))\n\n[1] \"TRUE\"\n\n\nThere is one type of data that does not encapsulate in quotes when it is converted to a character. This is the NA or missing value, which R will essentially always represent as NA, regardless of it‚Äôs class.\n\ny &lt;- NA\nclass(y)\n\n[1] \"logical\"\n\ny &lt;- as.double(y)\nclass(y)\n\n[1] \"numeric\"\n\nprint(as.character(y))\n\n[1] NA\n\n\n\n\n3.2.3 Logical Data\nLogical data is also simple to understand. Neither TRUE nor FALSE are unfamiliar to the reader.\n\nTRUE & FALSE\n\n[1] FALSE\n\nTRUE | FALSE\n\n[1] TRUE\n\n\nIt‚Äôs useful to note what happens when you turn logical data into numeric data. TRUE becomes 1 and FALSE becomes 0.\n\nvals &lt;- c(TRUE, FALSE)\nprint(as.numeric(vals))\n\n[1] 1 0\n\n\nYou can also write TRUE as simply T and FALSE as F.\n\n\n\n\n\n\nTip¬†3.1: Logical Arithmetic\n\n\n\n\n\nThe : operator is used to create a sequence of numbers, starting at the number on the right, and continuing by adding 1 until the number on the left is reached.\n\n0.1:5.75\n\n[1] 0.1 1.1 2.1 3.1 4.1 5.1\n\nx &lt;- 0:6\n\nIf we wanted to find only the numbers that are less than or equal to 4, we could use the &lt; operator (remember this list includes 0 at the beginning).\n\nx &lt;= 4\n\n[1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE\n\n\nIf we wanted to see how many of the numbers are less than or equal to 4, we could use the sum() function.\n\nsum(x &lt;= 4)\n\n[1] 5\n\n\nIf you just wanted to see if any or all of the numbers are less than four, you could use the any() or all() functions.\n\nany(x &lt;= 4)\n\n[1] TRUE\n\nall(x &lt;= 4)\n\n[1] FALSE\n\n\n\n\n\n\n\n3.2.4 Factors\nThere is an excelent chapter about factors in R for Data Science.\nFactors combine integers and strings, and they are useful for categorical data (which is exceptionally common in psychology research). One categorical variable is the day of the week. I am going to store all of the days of the week in a vector called days.\n\ndays &lt;- c(\"monday\", \n          \"tuesday\", \n          \"wednesday\", \n          \"thursday\", \n          \"friday\", \n          \"saturday\", \n          \"sunday\")\n\nHere is a list of days of the week. We‚Äôll use it as our data.\n\n# draw a random sample of 100 days of the week\ndata &lt;- sample(days, 100, replace = TRUE)\n# print the first 15 days\nhead(data, n=15)\n\n [1] \"tuesday\"   \"wednesday\" \"monday\"    \"friday\"    \"thursday\"  \"monday\"   \n [7] \"wednesday\" \"saturday\"  \"sunday\"    \"tuesday\"   \"tuesday\"   \"sunday\"   \n[13] \"sunday\"    \"friday\"    \"saturday\" \n\n\nUsing a factor, rather than a character, is useful for sorting purposes. Sorting our character days results in a list that is in the wrong order.\n\nsort(days)\n\n[1] \"friday\"    \"monday\"    \"saturday\"  \"sunday\"    \"thursday\"  \"tuesday\"  \n[7] \"wednesday\"\n\n\nDays of the week is a good candidate for a factor because there are a fixed number of known days of the week. We can convert the vector days to a factor by using the factor() function.\n\ny &lt;- factor(data)\nhead(y, n=15)\n\n [1] tuesday   wednesday monday    friday    thursday  monday    wednesday\n [8] saturday  sunday    tuesday   tuesday   sunday    sunday    friday   \n[15] saturday \nLevels: friday monday saturday sunday thursday tuesday wednesday\n\n\nAs you can see, failing to supply factor levels results in the levels being sorted in the same meaningless way as before. We can fix this by supplying the levels in the order we want them to appear.\n\ndata &lt;- factor(data, levels = days)\nhead(data, n=15)\n\n [1] tuesday   wednesday monday    friday    thursday  monday    wednesday\n [8] saturday  sunday    tuesday   tuesday   sunday    sunday    friday   \n[15] saturday \nLevels: monday tuesday wednesday thursday friday saturday sunday\n\n\n\nnlevels(data)\n\n[1] 7\n\nlevels(data)\n\n[1] \"monday\"    \"tuesday\"   \"wednesday\" \"thursday\"  \"friday\"    \"saturday\" \n[7] \"sunday\"   \n\n# if you want to see the numeric backbone of the factor\nas.vector(unclass(data)) |&gt; head(n=15)\n\n [1] 2 3 1 5 4 1 3 6 7 2 2 7 7 5 6\n\n\nYou may want sunday to be the first day of the week. You can do this by relevelling the factor\n\ndata &lt;- fct_relevel(data, \"sunday\")\nhead(data, n=15)\n\n [1] tuesday   wednesday monday    friday    thursday  monday    wednesday\n [8] saturday  sunday    tuesday   tuesday   sunday    sunday    friday   \n[15] saturday \nLevels: sunday monday tuesday wednesday thursday friday saturday\n\n\nYou can also reverse factors.\n\ndata &lt;- fct_rev(data)\nhead(data, n=15)\n\n [1] tuesday   wednesday monday    friday    thursday  monday    wednesday\n [8] saturday  sunday    tuesday   tuesday   sunday    sunday    friday   \n[15] saturday \nLevels: saturday friday thursday wednesday tuesday monday sunday\n\n\nSometimes you will want to change the labels for a factor. You can acomplish this with fct_recode. The new labels are on the left, and the old labels are on the right. Notice that the order of the labels in fct_recode does not change the ordering of the factors levels.\n\ndata &lt;- fct_recode(data,\n                  \"mon\" = \"monday\",\n                  \"tue\" = \"tuesday\",\n                  \"wed\" = \"wednesday\",\n                  \"thu\" = \"thursday\",\n                  \"fri\" = \"friday\",\n                  \"sat\" = \"saturday\",\n                  \"sun\" = \"sunday\",)\nhead(data, n=15)\n\n [1] tue wed mon fri thu mon wed sat sun tue tue sun sun fri sat\nLevels: sat fri thu wed tue mon sun\n\n\nYou might also want to collapse the levels of a factor into a set of super-ordinate labels, like ‚Äúworkday‚Äù and ‚Äúweekend‚Äù. You can do this with fct_collapse.\n\ndata1 &lt;- fct_collapse(data,\n                    workday = c(\"mon\", \"tue\", \"wed\", \"thu\", \"fri\"),\n                    weekend = c(\"sat\", \"sun\"))\nhead(data1, n=15)\n\n [1] workday workday workday workday workday workday workday weekend weekend\n[10] workday workday weekend weekend workday weekend\nLevels: weekend workday\n\n\nThe fct_lump function will keep the most common n levels and lump the others into a new level called ‚ÄúOther‚Äù. This is particularly helpful if you have categorical data with a lot of very small categories.\n\ndata1 &lt;- fct_lump(data, n = 3)\nhead(data1, n=15)\n\n [1] tue   Other mon   Other thu   mon   Other Other Other tue   tue   Other\n[13] Other Other Other\nLevels: thu tue mon Other\n\n\n‚ÄúOther‚Äù looks strange here because its the only level with an upper case letter. As with any of the functions I show you, you can look at the help page for the function to see what arguments it takes. The fct_lump function takes an argument (other_level) that allows you to specify the name of the new level.\n\ndata1 &lt;- fct_lump(data, n = 3, other_level = \"???\")\nhead(data1, n=15)\n\n [1] tue ??? mon ??? thu mon ??? ??? ??? tue tue ??? ??? ??? ???\nLevels: thu tue mon ???\n\n\n\n\n\n\n\n\nGenerating Levels\n\n\n\n\n\nSometimes you need to generate repetative factor levels (e.g., for a repeated measures design, or for a truth table). Our truth table will have 4 propositions: R, P, D, and R. Therefore our truth table will have 81 rows (because each proposition can either be true, false, or gay and we have four propositions; \\(3 ^4 = 81\\)).\nThe gl function generates a factor with up to n levels, each repeated k times.\n\ngl(n = 3, k = 2)\n\n[1] 1 1 2 2 3 3\nLevels: 1 2 3\n\n\nThe real clincher comes in the form of the labels argument. This allows you to specify the labels for each level.\n\noutcomes &lt;- c(\"true\", \"false\", \"gay\")\ngl(n = 3, k = 3, labels = outcomes)\n\n[1] true  true  true  false false false gay   gay   gay  \nLevels: true false gay\n\n\nWe can also (for our truth table) use the length arugment to make sure that all of our vectors are the 81 elements long that we need them to be.\nThis is what I need for my truth table\n\nR &lt;- gl(n = 3, k = 27, labels = outcomes)\np &lt;- gl(n = 3, k = 9, length = 81, labels = outcomes)\nd &lt;- gl(n = 3, k = 3, length = 81, labels = outcomes)\nr &lt;- gl(n = 3, k = 1, length = 81, labels = outcomes)\n\ntibble(R, p, d, r)\n\n\n  \n\n\n\n\n\n\n\n\n3.2.5 Coersion and Checking\nHere is a list of the data types you‚Äôve seen thus far:\n\ninteger\nnumeric\ncomplex\ncharacter\nlogical\nfactor\n\nSometimes you want to check that the class of a variable is what you expect if to be (especially before you try to do something that only one type can do, like division). You can use the is.* functions to check the class of a variable (replacing * with the appropriate type).\n\nx &lt;- \"zzz\"\nis.integer(x)\n\n[1] FALSE\n\nis.character(x)\n\n[1] TRUE\n\n\nYou can also coerce one type of data into another, although this is sometimes risky. Coercing something into a character is usually safe, but coercing something into a type like numeric or logical can cause errors. Often R will create NA values and produce a warning, as shown below.\n\nx &lt;- \"zzz\"\nas.numeric(x)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\nx &lt;- \"123\"\nas.numeric(x)\n\n[1] 123",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#data-structures",
    "href": "03_r_101.html#data-structures",
    "title": "3¬† R 101",
    "section": "3.3 Data Structures",
    "text": "3.3 Data Structures\n\n3.3.1 Vectors\nA vector is a one dimensional array of elements of the same type (e.g., all characters, all numerics). In R, essentially everything is a vector if you analyze it at a fine enough level of detail. You can create a vector with the c() function4.\n\nc(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\n\n\n\nGenerating Sequences\n\n\n\nA far easier way to generate the vector above would be to use the : operator, which is described in Tip¬†3.1.\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nLook at the help page of the seq() function by typing ?seq in the console. seq takes arguments from, to, by, and along, which I will highlight here. We could recereate our vector with the following code:\n\nseq(from = 1, to = 10, by = 1)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nYou could provide only a from and a by and a length to get the same sequence\n\nseq(from = 1, by = 1, length = 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nYou could also go backwards:\n\nseq(from = 10, to = 1, by = -1)\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nIf you want a sequence that is the same length as another vector, you could use the along argument.\n\nx &lt;- c(198, 3, -13, 0, 178, 20)\nseq(from = 0, to = 1, along = x)\n\n[1] 0.0 0.2 0.4 0.6 0.8 1.0\n\n\nThe rep() function is used to repeat a value a certain number of times.\n\nrep(1, times = 10)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n\nYou don‚Äôt just have to repeat single values. You can repeat vectors as well.\n\nrep(1:3, times = 3)\n\n[1] 1 2 3 1 2 3 1 2 3\n\n\nThe each argument determines how many time each element in the vector is repeated each time there is a repetition.\n\nrep(1:3, each = 3)\n\n[1] 1 1 1 2 2 2 3 3 3\n\n\nThere is no reason not to use the each and times arguments at the same time.\n\nrep(1:4, each = 2, times = 3)\n\n [1] 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4 1 1 2 2 3 3 4 4\n\n\nYou can also specify how many times each element is repeated in each repetition. I specify cat to repeat five times, dog once, and fish twice.\n\npets &lt;- c(\"cat\", \"dog\", \"fish\")\nrep(pets, c(5, 1, 2))\n\n[1] \"cat\"  \"cat\"  \"cat\"  \"cat\"  \"cat\"  \"dog\"  \"fish\" \"fish\"\n\n\n\n\nYou can name elements in a vector by using the names() function. This is particularly useful for counts.\n\ncounts &lt;- c(101, 28, 57, 4, 2)\nnames(counts) &lt;- c(\"A\", \"B\", \"C\", \"D\", \"F\")\n\ncounts\n\n  A   B   C   D   F \n101  28  57   4   2 \n\n\n\n\n\n\n\n\nSubscripting\n\n\n\n\n\nYou can subset a vector by using square brackets. You can subset by index, with the index starting at 1 (not 0).\n\nx &lt;- 1:10\nx[4]\n\n[1] 4\n\n\nWe can also subset based on another vector, selecting whichever indexes you like.\n\nx[c(2, 4, 8)]\n\n[1] 2 4 8\n\n\nYou can use negative indexes to exclude certain elements.\n\nx[-2]\n\n[1]  1  3  4  5  6  7  8  9 10\n\nx[-c(2, 4, 8, 3, 1)]\n\n[1]  5  6  7  9 10\n\n\nYou can also subset based on logicals. This is particularly useful for filtering data.\n\nx[x &gt; 5]\n\n[1]  6  7  8  9 10\n\n\n\n\n\nvectors are very typical in data science and statistics. Take, for example, the heights of all the characters in Star Wars (in cm).\n\nheights &lt;- dplyr::starwars$height\nnames(heights) &lt;- dplyr::starwars$name\nhead(heights, n = 30)\n\n       Luke Skywalker                 C-3PO                 R2-D2 \n                  172                   167                    96 \n          Darth Vader           Leia Organa             Owen Lars \n                  202                   150                   178 \n   Beru Whitesun Lars                 R5-D4     Biggs Darklighter \n                  165                    97                   183 \n       Obi-Wan Kenobi      Anakin Skywalker        Wilhuff Tarkin \n                  182                   188                   180 \n            Chewbacca              Han Solo                Greedo \n                  228                   180                   173 \nJabba Desilijic Tiure        Wedge Antilles      Jek Tono Porkins \n                  175                   170                   180 \n                 Yoda             Palpatine             Boba Fett \n                   66                   170                   183 \n                IG-88                 Bossk      Lando Calrissian \n                  200                   190                   177 \n                Lobot                Ackbar            Mon Mothma \n                  175                   180                   150 \n         Arvel Crynyd Wicket Systri Warrick             Nien Nunb \n                   NA                    88                   160 \n\n\nWhere in the vector is the greatest height and where is the lowest? Let‚Äôs sort the vector and then view the first 3 and last 3 elements in the sorted version.\n\n# sort the vector\nheights2 &lt;- sort(heights,\n                decreasing = TRUE)\n\nThere is a problem here, even if it‚Äôs not immediately apparent. Sorting, by default, will remove all the NA values from the vector. We can check that elements were removed by comparing the length of the sorted and the unsorted vectors; the sorted vector is smaller.\n\n# prints number of elements in vector (number of characters)\nlength(heights)\n\n[1] 87\n\nlength(heights2) &lt; length(heights)\n\n[1] TRUE\n\n\nLet‚Äôs try sorting again, but this time we will tell R what to do with the NA values using the na.last argument. By setting it to TRUE, we put all the NAs at the end of the vector.\n\nheights2 &lt;- sort(heights,\n                  decreasing = TRUE,\n                  na.last = TRUE)\n\n# verify that heights2 is the same length as the original data\nlength(heights2) == length(heights)\n\n[1] TRUE\n\n# print first 5 heights\nhead(heights2,\n     n = 3)\n\nYarael Poof     Tarfful     Lama Su \n        264         234         229 \n\n\nThe tallest character is Yarael Poof (with a height of 264 cm), and\n\n# should be all NA values (which we put at the end)\ntail(heights2,\n     n = 3)\n\n   Poe Dameron            BB8 Captain Phasma \n            NA             NA             NA \n\n# we can use subscriptting to filter out NAs\ntail(heights2[!is.na(heights2)],\n     n = 3)\n\nWicket Systri Warrick          Ratts Tyerel                  Yoda \n                   88                    79                    66 \n\n\nYoda has the smallest height (66 cm). We could also have achieved these conclusions using the min or max functions, but there‚Äôs a problem. The result we get is not Yoda and Yarael Poof; it‚Äôs NA.\n\nmax(heights)\n\n[1] NA\n\nmax(heights)\n\n[1] NA\n\n\nMany vector functions have a na.rm argument which tells R to do the computation while ignoring all missing values. Setting this argument to TRUE is typically required to produce a numerical result (e.g., R won‚Äôt compute a mean for a vector with an NA value). In the case of min and max\n\nmax(heights, na.rm = TRUE)\n\n[1] 264\n\nmin(heights, na.rm = TRUE)\n\n[1] 66\n\n\nA large range of descriptive statistics are available with built-in functions.\n\n### centrality statistics\nsum(heights, na.rm = TRUE)\n\n[1] 14143\n\nmean(heights, na.rm = TRUE)\n\n[1] 174.6049\n\nmedian(heights, na.rm = TRUE)\n\n[1] 180\n\n# get the first, second (median), and third quartiles\nquantile(heights, \n         probs = seq(from = 0.25,\n                     to = 0.75,\n                     by = 0.25),\n         na.rm = TRUE)\n\n25% 50% 75% \n167 180 191 \n\n### spread statistics\n# range\nrange(heights, na.rm = TRUE)\n\n[1]  66 264\n\n# variance\nvar(heights, na.rm = TRUE)\n\n[1] 1209.242\n\n# standard deviation\nsd(heights, na.rm = TRUE)\n\n[1] 34.77416\n\n\n\n3.3.1.1 Correlation, Variance, and Covariance\nThese functions are incredibly important (at least in principle) to a lot of frequentist statistics. They take 2 numeric vectors, an x and y; or just a single matrix or data frame.\n\n# x and y are perfectly negatively correlated (r=-1)\nx &lt;- 1:5\ny &lt;- 5:1\n\n# computing the covariance, removing na values\nvar(x,\n    y,\n    na.rm = TRUE)\n\n[1] -2.5\n\n# this is an equivalent computation of covariance\ncov(x,\n    y,\n    use = \"na.or.complete\",\n    method = \"pearson\")\n\n[1] -2.5\n\n# you could also use a different method\ncov(x,\n    y,\n    use = \"na.or.complete\",\n    method = \"kendall\")\n\n[1] -20\n\n\nCo-variances are typically standardized into correlations before they are interpreted. You can use the cor function to get a correlation coefficient calculated according to the methods outlined by \"pearson\", \"kendall\", and \"spearman\". We will calculate Pearson‚Äôs \\(r\\).\n\n\n3.3.1.2 Random Sampling\nR has a function called sample that is useful for generating a random sample of data, provided with some probabilities. For example, if you wanted to simulate the roll of a die, you could take a random sample (of size 1) from the sequence of numbers from 1 to 6:\n\nsample(1:6, size = 1)\n\n[1] 6\n\n\nOften you will want a sample with a size larger than 1. You can use the replace argument to allow for repeated sampling, as would be the case in 3 repeated rolls of a die.\n\nsample(1:6, size = 3, replace = TRUE)\n\n[1] 2 5 5\n\n\nYou can also use the prob argument to specify the probability of each outcome. For example, if you wanted to simulate a weighted die, you could use the following probabilities:\n\nsample(1:6, size = 3, replace = TRUE, prob = c(0.1, 0.1, 0.1, 0.1, 0.1, 0.5))\n\n[1] 6 6 3\n\n\nR also allows you to generate random numbers from a variety of distributions - this is the r* series of functions (where * is the name of a distribution - see ?distributions for more information). For example, to generate 5 random numbers from a normal distribution with a mean of 0 and a standard deviation of 1, you could use the following code:\n\nrnorm(5, mean = 0, sd = 1)\n\n[1] -2.2704375  0.1465201 -1.3294686 -1.3534238  0.7100011\n\n\nEach distribution also has an associated d, p, and q function. The d function gives the density of the distribution at a given point, the p function gives the cumulative distribution function of the distribution at a given point, and the q function gives the quantile of the distribution at a given probability.\nThe difference between d and p is easiest to show graphically.\n\ntibble(x = seq(from = -5, \n               to = 5, \n               by = 0.1),\n       y = dnorm(x, mean = 0, sd = 1)) |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Density of the Normal Distribution\",\n       subtitle = \"using dnorm(mean = 0, sd = 1)\",\n       x = \"x\",\n       y = \"Probability Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\ntibble(x = seq(from = -5, \n               to = 5, \n               by = 0.1),\n       y = pnorm(x, mean = 0, sd = 1)) |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Cumulative density of the Normal Distribution\",\n       subtitle = \"using pnorm(mean = 0, sd = 1)\",\n       x = \"x\",\n       y = \"Probability Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nI showed the quantile function above. the q series distribution functions are used to calculate quantiles. For example, to calculate the 0.95 quantile of the standard normal distribution, you could use the following code:\n\nqnorm(0.95, mean = 0, sd = 1)\n\n[1] 1.644854\n\n\nIf we wanted specific quantiles, we should specify them as the first argument. Here I select a variety of probabilities that show the empirical rule - about 68% of the data should be within 1 standard deviation of the mean, 95% within 2, and 99.7% within 3.\n\ntibble(quantile = c(0.0015, 0.025, 0.05, 0.16, 0.84, 0.95, 0.975, 0.9985),\n       z = qnorm(quantile, mean = 0, sd = 1))\n\n\n  \n\n\n\n\n\n\nFrom the R book by Michael Stanley (pg. 41)\n\n\n\nvectors: stores ordered data of the same type\n\nhave indexes that start at 1\n\nmatrix: stores data in 2 dimensions\narrays: higher dimensional matrices\nlists: key value pairs\n\nif you don‚Äôt put names in the list, R will index them with numbers\n\ndataframes: each row is an observation, each column is a characteristic\ntibbles: data frames with extra functionality",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#iteration",
    "href": "03_r_101.html#iteration",
    "title": "3¬† R 101",
    "section": "3.4 Iteration",
    "text": "3.4 Iteration\n\nfor loop: will run code a certain number of times\nwhile loop: will run code until a certain condition is no longer met\n\nuseful in optimization tasks (like sample size calculations)",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#control-flow",
    "href": "03_r_101.html#control-flow",
    "title": "3¬† R 101",
    "section": "3.5 Control Flow",
    "text": "3.5 Control Flow\nif (condition) {\n  code to run if condition is met\n}\nyou can add else statements, and even chain else statements, but it is easy to get confused",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#functions",
    "href": "03_r_101.html#functions",
    "title": "3¬† R 101",
    "section": "3.6 functions",
    "text": "3.6 functions\nfunction_name = function(inputs) {\n\n  a bunch of code you would like to reuse\n  the last line of code is the output of the function\n\n}",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#libraries",
    "href": "03_r_101.html#libraries",
    "title": "3¬† R 101",
    "section": "3.7 Libraries",
    "text": "3.7 Libraries\n\nhow to load and detach libraries\ndevtools - downloading from github\nCRAN",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#rstudio",
    "href": "03_r_101.html#rstudio",
    "title": "3¬† R 101",
    "section": "3.8 RStudio",
    "text": "3.8 RStudio",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#tidyverse",
    "href": "03_r_101.html#tidyverse",
    "title": "3¬† R 101",
    "section": "3.9 Tidyverse",
    "text": "3.9 Tidyverse\n\n3.9.1 readr\n\nread_csv and related funcions for reading data\nreadxl::read_xlsx for excel files\n\n\n\n3.9.2 tibble\n\nhow to make a tibble\nalmost all of the functions in the tidyverse input and output tibbles; we can pipe data\n\n\n\n3.9.3 dplyr\n\nmanipulation of data, especially useful for cleaning\nselect: select or remove\nfilter\nmutate: create new columns\n\n\n\n3.9.4 stringr\n\n\n3.9.5 lubridate\n\n\n3.9.6 forcats\n\n\n3.9.7 purr\n\nlist columns are super useful - lists can have different data types\nmap functions - output is a list column\n\n\n\n3.9.8 tidyr\n\npivot wider\npivot longer\n\n\n\n3.9.9 ggplot2\n\nmappings\ngeom - like geom_line or geom_point\nyou can add as many layers as you want\nthemes",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#footnotes",
    "href": "03_r_101.html#footnotes",
    "title": "3¬† R 101",
    "section": "",
    "text": "Editors note: This is a simplified telling of how R stores integers in memory. For simplicity, the binary digit is interpreted as if the first (leftmost) bit corresponded to \\(2^0\\). IRL, R most likely reads the bits backwards such that the last (rightmost) bit corresponds to \\(2^0\\). This is just one example, but there are numerous complications that I simply do not mention.‚Ü©Ô∏é\nYou can use the operator ** in place of ^, and I frequently do. R translates ** into ^ before evaluating any expression.‚Ü©Ô∏é\nmostly a science/engineering thing; see Wikipedia page for review.‚Ü©Ô∏é\nc() stands for concatenate.‚Ü©Ô∏é",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "04_visualization.html",
    "href": "04_visualization.html",
    "title": "4¬† Visualization",
    "section": "",
    "text": "4.1 Plotting Basics\nThe first step to making any plot is calling the ggplot function. If you look at the help page for the ggplot function (by typing ?ggplot into an R console) , you will find that it takes 2 arguments: data and mapping. Ggplot expects that the data argument is going to be a data frame with tidy data in.\nggplot(data = time_use)\nAs you can see, ggplot creates an empty plot. Next, we will add a mapping argument. The mapping will tell ggplot which aesthetics (like the x-axis, y-axis, color, shape, etc.) will be represented by which variables in the data. In this case, we want the time_spent variable on the x-axis and the women_to_men variable on the y-axis, as they appear in the reference plot. When we call the ggplot while providing a data and a mapping argument, R will create a blank plot with axes.\nggplot(data = time_use, mapping = aes(x = time_spent, y = women_to_men))\nAs with any function in R, we do not need to write all of the arguments on a single line. We can separate the arguments onto different lines, as long as we make sure each line ends with a comma or the end parenthesis (i.e., the end of the function call). You may find that the following two code snippets are more readable than the one above, even though all three would produce the same output.\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men))\nggplot(\n  data = time_use,\n  mapping = aes(\n    x = time_spent,\n    y = women_to_men\n  )\n)\nTo R, it does not matter whether you type this function call out in 1 line or in 7; it is the same arguments being passed to the same function and thus produces the same result.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "04_visualization.html#plotting-basics",
    "href": "04_visualization.html#plotting-basics",
    "title": "4¬† Visualization",
    "section": "",
    "text": "4.1.1 Geoms\nOur plot is missing something: can you spot it? There is no data on our plot! We represent data in a ggplot by using a geom function. Almost all of these functions start with geom_ (like geom_bar or geom_smooth) or stat_ (like stat_count). You can see a fuller list of the geom functions available on the ggplot2 cheat sheet.\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe add a geom by literally adding it (with the + operator) to the ggplot function call. The + always has to go at the end of the line. You can separate the functions by as many empty or commented lines as you‚Äôd like. You can also do this within function calls. So, this code will run:\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  \n  \n  # this is a comment\n  \n  geom_point()\n\n\n\n\n\n\n\n\nas will this code:\n\nggplot(data = time_use,\n       \n       # this is a comment\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point()\n\n\n\n\n\n\n\n\nand if you wanted to be really verbose, you could even do something like this:\n\n# create ggplot\nggplot(\n  \n  # add data to plot\n  data = time_use,\n  \n  # create mapping\n  mapping = aes(\n    # map x-axis\n    x = time_spent,\n    # map y-axis\n    y = women_to_men\n  )\n) +\n  \n  # add scatterplot\n  geom_point()\n\n\n\n\n\n\n\n\nWhat is the difference between a mapping and a style? A mapping connects one aesthetic to a variable, but a style just sets the aesthetic. For example, styling our scatter plot might mean turning all the points blue, whereas a mapping would match each activity to a color based on a scale. This is a graph that uses color as a style:\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\n\nand this is a plot that uses color as a map:\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = activity))\n\n\n\n\n\n\n\n\nYou can put mappings in the ggplot function, or in any geom function. In the above code, the mapping for x and y is in the ggplot function, and the mapping for color is in the geom_point function.\nEvery geom will inherit the mapping from the ggplot function. If we added another geom to the plot, we could see this. In the plot below, I added the geom_smooth, and - as you can see - it inherits the x and the y aesthetic from the ggplot function, but it does not inherit the color argument from the geom_point function.\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = activity)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIf you wanted R to make differently colored smooth lines in this plot, you could add a color aesthetic to the geom_smooth function, or you could move the color aesthetic from the geom_point function to the ggplot function, thereby allowing the geom_smooth function to inherit a color aesthetic.\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, \n                     y = women_to_men,\n                     color = activity)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet‚Äôs do a little investigating with just the geom_smooth function.\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe used the color aesthetic to make separate lines above, but we didn‚Äôt need to. We can also use the group or linetype arguments to create separate lines\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(group = activity))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(linetype = activity))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can change the data supplied to geom_smooth to draw only one line:\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = continent)) +\n  geom_smooth(data = filter(time_use, activity == \"Unpaid work\"),\n    mapping = aes(group = activity))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou should use the help page (?geom_smooth) to read more about the aesthetics and arguments the function can take. I will only highlight two more: method, and se. I frequently find myself favoring a linear regression line (rather than the default which uses the LOESS smoothing function (a type of local, polynomial regression). You can get a straight line by changing the method argument to ‚Äúlm‚Äù. The se argument can be set to T or TRUE or F or FALSE, and it controls whether the plot includes an gray error area.\n\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(group = activity), \n              method = \"lm\",\n              se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOn a completely separate note, you use a very similar process of setting the data and mapping arguments to make a bar chart. Ggplot makes a distinction between a bar chart and a column chart (even though they can appear to be identical). A bar chart has a single, discrete aesthetic mapping (like one that maps x to continent).\n\nggplot(data = distinct(time_use, country, continent),\n       mapping = aes(x = continent)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nA column chart takes 2 mappings: a discrete x aesthetic (like activity) and a continuous y aesthetic (like time_spent).\n\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                color = activity)\n) +\n  geom_col()\n\n\n\n\n\n\n\n\nBar charts are a great example of the difference between the color and the fill aesthetics. Points and lines have only colors, but bars and columns (and other geoms, like density plots) have fills, as well. Let‚Äôs fix the last plot.\n\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                fill = activity)\n) +\n  geom_col()\n\n\n\n\n\n\n\n\nThis plot does not provide a helpful comparison of the time use in OECD countries across various continents because all of the columns have a different height. In reality, we would like all of the columns to be the same height so we can compare proportions. We can do this using the position argument\n\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                fill = activity)\n) +\n  geom_col(position = \"fill\")\n\n\n\n\n\n\n\n\nThis is a better plot to view the differences in OECD time use across continents. However, there is (again) very limited data for several continents which are either missed (i.e., most of Africa, Oceania, South America, and Asia) or have only very few countries (i.e., North America)\n\n\n4.1.2 Faceting\nUsing just the ggplot function and a few geom functions, we can get damn near the reference plots we started with.\n\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFaceting is tricky to explain in words, but it‚Äôs very easy to understand if you can see it.\n\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere are two faceting functions: facet_wrap and facet_grid. Both have a scales argument, which is set as ‚Äúfixed‚Äù by default. You can change it to:\n\n‚Äúfree_x‚Äù to allow the x axes to have different limits in the different plots\n‚Äúfree_y‚Äù to do the same for the y axes\n‚Äúfree‚Äù to have both axes take different limits for each separate plot\n\nIn this case, we‚Äôll just set the scales argument to \"free\" so that all of the data aren‚Äôt packed so tightly.\n\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2,\n             scales = \"free\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nfacet_grid is helpful if you would like to facet on two (discrete!) variables at once (like activity and continent in the plot below).\n\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 0.5,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_grid(continent ~ activity, \n             scales = \"free\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWith so few data, such extensive faceting is neither informative nor helpful, so we‚Äôll stick with just one facet: activity.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "04_visualization.html#plot-styling",
    "href": "04_visualization.html#plot-styling",
    "title": "4¬† Visualization",
    "section": "4.2 Plot Styling",
    "text": "4.2 Plot Styling\nOur current plot is beginning to look very similar to the reference plot we were attempting to recreate. We have written 23 lines so far. To save space, I am going to save the plot that we currently have under the name p. When we call p, R will make the plot. We can also add things to p, just as we added them to the ggplot function.\n\n# save ggplot object under the name \"p\"\np &lt;- ggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2,\n             scales = \"free\")\n\np\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n4.2.1 Labels\nBy default, ggplot does not create a title or subtitle for the plot, and it uses the names of the variables for the axes and scales (i.e., color scale in our plot). The labs function allows you to update various text elements in the plot.\n\np &lt;- p + labs(title = \"Gender Parity in Time Spent in OECD Nations\",\n         subtitle = \"ATTN: all plots on different axes\",\n         x = \"time spent (minutes per day)\",\n         y = \"ratio of women's time spent to men's\")\n\np\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAdding labels to the outside of your plot is as simple as that. You could also use labels as a geom using the geom_text function. It takes an additional aesthetic, label that we have not yet seen.\n\nggplot(data = filter(time_use, activity == \"Unpaid work\"),\n       mapping = aes(x = time_spent,\n                     y = women_to_men,\n                     color = continent)) +\n  geom_text(mapping = aes(label = country))\n\n\n\n\n\n\n\n\n\n\n4.2.2 Scales and Coordinates\nOur plot is different from the reference plot in which colors it uses. The reference plot uses black, orange, blue, green, and yellow; but, ours uses red, beige, green, blue, and purple. What gives? The reference plot uses a different color scale than does our plot. The color scale is the part of the plot that matches each continent to a unique color. The reference plot uses a color scale from the ggthemes package that is colorblind friendly.\n\np &lt;- p + ggthemes::scale_color_colorblind()\n\np\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nscales also control the x and y axes, specifically continuous scales. We can use a scale function to set the limits or breaks for the axes. In our case, this allows us to demonstrate how much larger the gender disparity appears to be for unpaid work rather than for any of the other (measured) uses of time.\n\np + \n  scale_y_continuous(limits = c(0, 7), breaks = c(1, 3.5, 7))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nwe can use scale functions to add transformations, as well. There are functions like scale_x_sqrt and scale_x_log10 that put the data on square root or log axes. Our time use data is not a great example of the utility of these types of axes. Instead, look at this data about coffee production:\n\ncoffee_plot &lt;- ggplot(data = coffee,\n       mapping = aes(x = pounds,\n                     y = pop_2019)) +\n  geom_text(mapping = aes(label = country)) +\n  labs(x = \"pounds of coffee produced in 2019\",\n       y = \"population in 2019\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\ncoffee_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe coffee data is dominated by small producers - with many populations and productions below 10 million people and pounds. The relationship between the variables is obscured by the clumping of data, which is caused by the massive outliers of Brazil (massive production) and India (massive population). A log-log plot (with log axes on the x and y) reveals a different perspective on this data. You must be careful, however, because log axes are not incredibly easy for most people to read, and the essentially everyone (including those that can read log-log scales) expects the scales on a plot to be linear unless explicitly warned otherwise.\n\ncoffee_plot + \n  scale_x_log10() +\n  scale_y_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere are many more transformations the scale functions can perform (like a boxcox, or logit transform), and you can see a fuller list in the documentation of the scale_x_continuous function (or any of the continuous scale functions). Okay; bye, coffee data!\nWe can finally use scales functions to control the labels on the axes. I find this is particularly helpful if you have percentages on one axis. We have a proportion, which can be represented as a percentage.\n\np &lt;- p + scale_y_continuous(labels = scales::percent_format())\n\np\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n4.2.3 Themes\nThe plot we have still doesn‚Äôt look quite like the reference plot we aimed toward. This can be explained by a difference in theme. The reference plot uses the minimal theme, whereas ours uses the default theme. We can ‚Äúfix‚Äù that by using the theme_minimal function.\n\np + theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can see the other default themes by seeing the help page for theme_minimal (i.e., ?theme_minimal, which is also the help page for the other built-in ggplot themes). My other favorite is theme_classic.\n\np + theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI also use theme_void, but typically only when making pie charts. theme_void removes most of the elements in the plot, including the x and y axes and gridlines.\n\np + theme_void()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\np &lt;- p + theme_minimal()\n\n\n\n4.2.4 Guides, Legends, and Text\nAlthough it makes no sense not to have it in this case, there are some cases in which you would like to remove the legend, which appears to the right of the plot by default. There are two ways to do this, using guides or using theme.\nguides is a function that is occasionally helpful for specifying which type of scale variables should be mapped to. You could set the guide to the color and/or shape arguments to ‚Äúnone‚Äù to remove one or both aspects of the legend.\n\np + guides(\n  color = \"none\"\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou could also remove the legend, instead of removing variables from the legend. You would do this using the legend.position argument in the theme function.\n\np + theme(legend.position = \"none\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou could also move the legend around using the legend.position argument.\n\np &lt;- p + theme(legend.position = \"top\")\n\np\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFinally, you can use the theme function to make really specific changes to the plot, like changing the angle of the numbers of the y-axis\n\np + theme(axis.text.x = element_text(angle = -30),\n          axis.text.y = element_text(angle = 30))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023. ‚ÄúData Visualization.‚Äù In R for Data Science, 2nd ed. https://r4ds.hadley.nz/data-visualize.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. ‚ÄúData Visualization.‚Äù In R for Data Science, 1st ed. https://r4ds.had.co.nz/data-visualisation.html.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "05_probability.html",
    "href": "05_probability.html",
    "title": "5¬† Mathematical Primer",
    "section": "",
    "text": "What is the law of large numbers?\nWhy are normal distributions important? What is the central limit theorem?\nWhat is a probability distribution? How do you write a function for a pdf and what are the properties of a normal distribution?",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Mathematical Primer</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html",
    "href": "06_eugenics.html",
    "title": "6¬† Regression and Power",
    "section": "",
    "text": "6.1 Cancelling Karl Pearson\nAs previously mentioned, correlation is excessively important to frequentist statistics. It is used to determine the strength and direction of a linear relationship between two variables. I learned many of the statistical techniques I will teach you later on (linear and logistic regression, ANOVA, principal component analysis, it goes on and on) in a class called ‚ÄúCorrelational Techniques‚Äù.\nThe University College London has a Pearson archive from which some of the following information comes. (Other info is cited or editorial.)\nPearson was born in London in 1957, and graduate from Cambridge with a degree in mathematics at the age of 22 (in 1879). Pearson was encouraged by, collaborated with, and wrote a biography of Francis Galton. If you‚Äôll recall, Galton popularized the median and also eugenics. Perason and Galton also collaborated frequently with Walter Weldon, who was a zoologist. He (and the three of them more generally) believed that evolution was an essentially statistical phenomena. Collectively, they founded the journal Biometrika in 1901.\nIn the spirit of Hilbert and Friends (see Section 1.3), these three were interested in formalizing the theory of evolution in mathematical notation. We must admit, as we did with the mathematicians, that their aim is incredibly ambitious. They believe that there is a mathematical truth that defines and controls life in the broadest and most encompassing understanding of that term. Discovering that truth (which I do not believe exists) was their aim. This formulation of life in general, requiring de-individualized consideration at population levels, was a profitable ideology (episteme) under which to enact control over individual bodies and lives through public policy and social convention (Foucault 1978). This framework enables statements about ‚Äútraits‚Äù (a euphemism for the people with those traits) and their disastrous effect on the ‚Äúpopulation‚Äù (or even ‚Äúlife‚Äù) as a whole, naturally leading to the conclusion that removing the ‚Äútraits‚Äù (which are, again, people) from the population would make that population better and more fit, as assessed with statistics (like Galton‚Äôs median). These men express an excessive trust in mathematics as an explanation for even very complex (and random!) biological processes and traits (like sexual orientation or ‚Äúduration of life‚Äù, traits so complex and mutable in their formation that I take the them to be unpredictable, certainly not in any mathematically rigorous and informative way).\nI‚Äôm going to pull a Foucault and analyze this journal, and the philosophies expressed in its early issues, as a technology: a system that is shaped by its socio-historical context and its function. The philosophy of Biometrika reeks of the ‚Äúgo fast and break things‚Äù approach that is all to familiar as mottos of the plutocracy (Elon Musk, fossil fuel magnates, bankers that caused the collapse of the global housing market, the AI people who continue to develop their technology despite their own protests/marketing that it is powerful enough to pose a threat to the human race, etc.). The tool these technocrats (scientists) were going to abuse was the correlation, and boy did they ever abuse it! Not only did correlation entail causation, but in the pages of Biometrika, every correlation was subject to interpretation as having a bio-evolutionary cause.\nThe editors surely saw themselves as progressive, and they acted in surprisingly progressive ways. In the first volume, for example, there is a reiteration that researchers should make their data available:\nThe guards against any critical verification actually taking place were three fold. Firstly, the editors intend the journal to merge biology and mathematics, producing article that will be unfamiliar to practitioners of each. Those practitioners would have to turn to the experts (in many cases, the editors of the journal) to learn how to misinterpret the data in the same way. Secondly, because ‚Äúthe new methods occupy an altogether higher plan than that in which ordinary statistics and simple averages move and have their being‚Ä¶ the arithmetic they require is laborious, and the mathematical investigations on which the arithmetic rests are difficult reading even for experts.‚Äù The inability to understand and interpret the incredibly complicated methods on which their conclusions rested led many authors in Biometrika (and certainly many more readers) to make false or overly ambitious (i.e., ill-supported) conclusions, many of which continue to circulate to this day under the guise of scientific fact. Thirdly, the level of mathematical understanding required to challenge the conclusions (and the failing willingness of those at the helm of the institution to listen to those who didn‚Äôt look like them) resulted in only a very homogeneous and powerful group of scientists being left to perform ‚Äúverification.‚Äù That‚Äôs not to say that there was no one left to criticize biometry; there certainly were, not that the editors of Biometrika (or at least Galton) cared:\nThis early quote hints at a period before biometrics had the serious legitimacy it would later acquire. Biometrika would not only be a tool to increase the legitimacy of this growing field, but also to potentially keep it afloat if its favor began to tank. This must have seemed like a venue of considerable importance, then. Biometrics (and therefore biometrika) often made opposite assumptions to the dominant theory of Mendelian heredity (yet another way in which this theory was progressive for the time).\nPearson was interested in using science to improve mankind. This is a fundamentally progressive objective, and he rejected the utility of phrenology (which he refers to as anthropometry).\nTo return briefly to the notion of Foucaultian bio-power (for which I have given only a meager explanation), we can see that Karl Pearson‚Äôs explicit aim, at least circa 1920 was that his work would be integrated into the work of the state; that his statistics (and his eugenics) become the knowledge against which to judge the value of individual human lives, establishing the cut-off under which one was liable to be sterilized, killed, or subject to such a thorough judicial and scientific examination that one wished they had been killed. Pearson is really quite explicit about this:\nPearson here asks not only for anthropology to be reformed in his image (by ditching all the soft human shit and turning to quantification and numbers instead), and then for states to ruthlessly and expansively apply this science to (control their citizenry in hopes that they might) make the population more evolutionarily fit (and productive to the capitalist system). He even goes so far as to repeatedly call anthropology (as he envisioned it) the ‚Äúqueen of the sciences‚Äù (Pearson 1920)2. He sees state use of anthropology as necessary to progress the species (and guard against defeat in armed conflict).",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#cancelling-karl-pearson",
    "href": "06_eugenics.html#cancelling-karl-pearson",
    "title": "6¬† Regression and Power",
    "section": "",
    "text": "A single individual may have a variation which fits it to survive, but unless that variation appears in many individuals, or unless that individual increases and multiples without loss of the useful variation up to comparatively great numbers‚Äìshortly, until the fit type of life because a mass-phenomenon, it cannot be an effective factor in evolution. The moment this point is grasped, then whether we hold variation to be continuous or discontinuous in magnitude, to be slow or sudden in time, we recognize that the problem of evolution is a problem in statistics, in the vital statistics of populations. Whatever views we hold on selection, inheritance, or fertility, we must ultimately turn to the mathematics of large numbers, to the theory of mass-phenomena, to interpret safely our observations. As we cannot follow the growth of nations without the statistics of birth, death, duration of life, marriage and fertility, so it is impossible to follow the changes in any type of life without its vital statistics. The evolutionist has to become in the widest sense of the words a registrar-general for all forms of life.\n‚Äú(II.) The Spirit of Biometrika‚Äù (1901), emphasis added\n\n\n\nThese words‚Ä¶ may well serve as a motto for Biometrika and for all biometricians: I have no faith in anything short of actual measurement and the Rule of Three1.\n‚Äú(II.) The Spirit of Biometrika‚Äù (1901), emphasis original\n\n\n\n\nI have begun to think that no one ought to publish biometric results, without lodging a well arranged and well bound manuscript copy of all his data, in some place where it should be accessible, under reasonable restrictions, to those who desire to verify his work.\nGALTON (1901)\n\n\n\nIt is not in the least my intention to insinuate that Biometry might be served by any modern authority in so rough a fashion [as the Royal Society treated the alleged founders of geology], but I offer the anecdote as forcible evidence that a new science cannot depend on a welcome from the followers of older ones, and to confirm former conclusions that it is advisable to establish a special Journal of Biometry.\nGALTON (1901)\n\n\n\n\nI am afraid I am a scientific heretic‚Äìan outcast from the true orthodox faith‚ÄìI do not believe in science for its own sake. I believe only in science for man‚Äôs sake‚Ä¶ the progress of mankind in its present stage depends on characters wholly different from those which have so largely occupied the anthropologist‚Äôs attention. Seizing the superficial and easy to observe, he has let slip the more subtle and elusive qualities on which progress, on which national fitness for this or that task essentially depends. The pulse-tracing, the reaction-time, the mental age of men under his control are for more important to the commanding officer‚Äìnay, I will add, to the employer of labor‚Äìthan any record of span, of head-measurement or pigmentation categories. The psycho-physical and psycho-physiological characters are of far greater weight in the struggle of nations to-day than the superficial measurements of man‚Äôs body. Physique, in the fullest sense, still counts something still, bit it is physique as measured by health, not by stature or eye-color. But character, strength of will, mental quickness count more, and if anthropometry is to be useful to the state it must turn from these rusty old weapons, these measurements of stature and records of eye-color to more certain appreciations of bodily health and mental aptitude‚Äìto what we may term ‚Äòvigorimetry‚Äô and to psychometry.\nPearson (1920)\n\n\n\nWe have to make anthropology a wise counsellor of the state, and this means a counsellor in political matters, in commercial matters, and in social matters.\nPearson (1920)\n\n\n\nThen, I think, you will agree with me that rightly or wrongly there is a conviction spreading in Germany that the war arose and that the war was lost because a nation of professed thinkers had studied all sciences, but had omitted to study aptly the science of man. And in a certain sense this is an absolutely correct conviction, for if the science of man stood where we may hope it will stand in the dim and distant future, man would from the past and the surrounding present have some grasp of future evolution, and so have a greater chance of guiding its controllable factors.\nPearson (1920)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#eugenics",
    "href": "06_eugenics.html#eugenics",
    "title": "6¬† Regression and Power",
    "section": "6.2 Eugenics",
    "text": "6.2 Eugenics\nEugenics relies on biological determinism.\n\nHe has never grasped that the man of to-day is precisely what heredity and his genealogy, his past history and his prehistory, have made him. He does not recognise that it is impossible to build your man for the future until you have studies the origins of his physical and mental constitution‚Ä¶ Man has not a plastic mind and body which the enthusiastic reformer can at will mould to the model of his golden age ideals. He has taken thousands of years to grow into what he is, and only by like processes of evolution‚Äìintensified and speeded up, if we work consciously and with full knowledge of the past‚Äìcan we build his future.\nPearson (1920)\n\nPearson took advantage of socio-political circumstances (like racialized grief after the killing of millions of white boys in WW1) to create moral and ontological panics.\n\nWe have seen a large part of the youth who were best fitted mentally and physically to be parents of feauture generations perish throughout Europe: the dysgenic effect of this slaughter will show itself each twenty to twenty-five years for centuries to come in the census returns of half the countries of the world.\nPearson (1920)\n\nViewing it in socio-historical context, especially considering the incredibly innovation in destructive weapons over the second half of the 19th century, Viewing it in socio-historical context, especially considering the incredibly innovation in destructive weapons over the second half of the 19th century, the case for the inherent wickedness (or the inherent weakness of humanity in resistance to wickedness) was getting easier and easier to evidence, as violence became more heinous and more immediate (through photography, for example).\nEugenics was thoroughly connected to capitalism, and Pearson‚Äôs vision of it was that it would touch every aspect of society, optimizing the human condition (and surplus profits) as it progressed.\n\nWide, however, as is the anthropometric material in our universities and public schools, it only touches a section of the population. The modern anthropologist has to go further; he has to enter the doors of the primary schools; he has to study the general population in all its castes, its craftsmen, and its sedentary workers. Anthropology has to be useful to commerce and to the State, not only in association with foreign races, but still more in the selection of the right men and women for the staff of factory, mine, office, and transport. The selection of workmen to-day by what is too often a rough trial and discharge method is one of the wasteful factors of production.\n(Pearson 1920)\nBut the anthropologist, if he is to advance his science and emphasise its service to the State, must pass beyond the university, the school, and the factory. He must study what makes for wastage in our present loosely organized socitey; he must ingestigate the material provided by the reformatory, prison, asylums for the insane and mentally defective; he must carry his researches into the inebriate home, the sanatorium, and the hospital, side by side with his medical collaborator.\nThe future lies with the nation that most truly plans for the future, that studies most accurately the factors which will improve the racial qualities of future generations either physically or mentally. Is anthropology to lie outside of this essential function of the science of man? If I understand the recent manifesto of the German anthropologists, they are determined that it shall not be so. The war is at an end, but the critical time is with us again, I sadly fear, in twenty to thirty years. How will the States of Europe stand then? It depends to not little extent on how each of them may be cultivated the science of man and applied its teaching to the improvement of national physique and mentality. Let us take care that our nation is not last in this legitimate rivalry. The organisation of existing human society with a view to its future welfare is the crowning task of the science of man; it need the keenest-minded investigators, the most stringent technique, and the utmost sympathy from all classes of society itself.\nPearson (1920)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#eugenic-statistics-today",
    "href": "06_eugenics.html#eugenic-statistics-today",
    "title": "6¬† Regression and Power",
    "section": "6.3 Eugenic Statistics Today",
    "text": "6.3 Eugenic Statistics Today\nHow are we, as people who want to do statistics, to make sense of this incredibly alarming information about the original purpose of the tools we are going to use. How do we prevent falling into the same trap that the eugenicists fell, believing contentedly that we are contributing to the progress of humanity while, in fact, our work is just a collection of our preconceived notions that could wind up causing destruction and ruin at a scale we couldn‚Äôt imagine in our wildest dreams?\nIn my psychology lectures, Pearson is a eugenicist, and that makes him a bad person (fair enough, honestly). This causes a paradox because no one wants to stop using the methods that Pearson and his contemporaries popularized and - in many cases - invented. Those things are good ideas, but they come from a bad person. How could that be?\nDoes this mean that bad persons can have good ideas? That Pearson wasn‚Äôt actually a bad person, or that he was more morally complex than brief presentation by my psychology professors suggests?\nI don‚Äôt know how people justify the use of these methods to themselves. I don‚Äôt know how they expect that they will use the same methods in similar ways without reproducing and enacting the functions these tools were designed for. My speculation is as follows.\nThis material is often difficult to read (and it is common to find sensational articles and quotes that are truly abhorrent, and which I mostly did not include here). It causes you to question humanity by demonstrating the sorts of things we‚Äôre capable of thinking and doing, and the steps we are willing to take to ignore the harm our thoughts and actions have on others, including to reanalyze that harm as benefit (or just fit, if you‚Äôre a eugenicist). Because that reading is so uncomfortable, and there‚Äôs really no productive advantage to do it, (and also sometimes good sources can be difficult to find,) a lot of people just don‚Äôt.\nThe problem then becomes ignorance. If no one knows and talks about these things, we should only expect that scientists are going to go around licencing their work as ethically acceptable because it is, or at least sometimes looks, progressive. They, like Pearson, are trying to improve the condition of some population of people through acquisition of knowledge.\nScientists don‚Äôt use Pearson‚Äôs tools because they do the best job at reflecting the true nature of the world. Pearson‚Äôs tools serve a useful purpose, but this is not it. To the extent correlational statistics represent anything faithfully, it is the socio-historical context of their development and use. I would argue that these tools are such useful tools in the human sciences because the assumptions and philosophies behind the tools are the same philosophies and assumptions that limit human action in the real world, either through social convention, government policy, or some other effect of social power relations.\nThese tools remain useful for control of populations and individual lives because the philosophies underlying these tools are thoroughly dispersed throughout the social order. Their use has expanded beyond the notion bio-power, expanding now to a politics of attention, as well as various other innovations. By politics of attention, I mean to point out that correlational statistics (how similar is person x‚Äôs watch history to person y) form the foundation of many of the algorithmic tools that are used to control human attention (and thus thought and emotion) on digital platforms of all sorts today.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#notes-from-pearsons-book-about-states",
    "href": "06_eugenics.html#notes-from-pearsons-book-about-states",
    "title": "6¬† Regression and Power",
    "section": "6.4 Notes from Pearson‚Äôs Book about States",
    "text": "6.4 Notes from Pearson‚Äôs Book about States\nThis book (Pearson 1919)!\n\n‚ÄúIt is little wonder, perhaps, that the first investigators in this new field [biology] went widely astray. They extended without due thought fascinating biological hypotheses to the case of man. They found the struggle of individual against individual in many vital fields, and they extended the survival of the fitter as a governing principle to all individual life within human communities; they did not stay to inquire why or how communities had themselves come into existence; they neglected the suggestions of the hive and the herd, and reached (as in the cases of both Spencer and Huxley) fallacious conclusions as to the functions of the state and the sources of social conduct. In short, they over-emphasized the intra-racial struggle, and under-emphasized the inter-racial contest, as factors producing and developing the political and moral characteristics in man.‚Äù (2)\ndiscussion of why we Darwinian genetics applies (assumptions about the human race and then support of those assumptions with argument) - pages 3-4\n\n(c) - death is 50-80% heritable essentially - repetition from study in volume 1 issue 1 or 2 or Biometrika\n‚ÄúWe are forced, then, to the conclusion that the Darwinian theory in the case of mankind is a law and not a‚Äùplausible hypothesis.‚Äù (4)\n\nlook how great Romans, Hebrews, Greeks, and Europeans are (5-6)\nPearson saw the population as a biological entity of sorts, and spoke about it with florid language:\n\nThe struggle of nations is the commonplace of history; but the realization that this struggle is a factor in human development,‚Äìthat big battalions or an armada are not sufficient insurance for success in it, but that organization and intelligence in every function of national life are requisite for victory,‚Äìthis is the special truth that dawned upon us at the end of the nineteenth century. Formerly territory was blindly seized, trade routes and commercial markets blindly opened or controlled, manufacturing processes and means of transit developed or not, according as they might seem profitable or not to individuals. The bearing of these things and a multitude of others‚Äìsuch as the physique of the nation, the skill of its craftsmen, the intelligence of its trade-leaders, the activity of its educators, the organization and preservation of its material resources‚Äìwas unrecognized in their relation to national fitness for the international struggle. The politician could tell the nation that it must have more ships or more rifles for the national safety, or he could emphasize the importance of the ‚Äúopen door‚Äù for national welfare, but he did not provide for the intelligent building of the ships, the intelligent sighting of the rifles, the intelligent training of the merchants who were to enter the open food amid the great international crush to get inside. He did not see that ultimately the training of even the apparently most insignificant workers in the community, the fitness for its purpose of the simplest manufacturing or agricultural process, may be vital to a nation in the evenly balanced contest of modern civilization. To stand still‚Äìfor a moment to depend only on the possession of material resources, of the existing trade routes, or of means of transit‚Äìis to lose points in the game. Where all are pressing forward, not to advance is to fall behind.‚Äù (6-7)\n\nfactors of national success (7)\n\nthe physical powers:\n\nvital: fertility rates, ‚Äúhealth and sanitation, the energy, vigour, and and absolute strength of individuals‚Äù\nmaterial: wealth in resources (minerals, energy sources, water resources including docks)\n‚Äúequipment: the power to seize and the power to hold‚Äù\n\nthe mental powers:\n\n‚Äúcarry[ing] out mechanical work quickly and effectively‚Äù\ndiscovery and imagination\nincitement and stimulation\n‚Äúmorale and patriotism‚Äù\nendurance\ntendency away from ‚Äúepochs of national hysteria‚Äù\n‚Äúfollow[ing] for years definite policies with only future profit in view‚Äù\ngoverning effectively\n\nin lieu of the power to govern effectively, the power to ‚Äúultimately assimilate divergent racial groups. (the tell is here)\n\n\n\n‚ÄúBrute force, strength and bravery, material wealth, have in turn been dominant in the state; to-morrow will be marked by the dominance of intelligence. The most intelligent nations will be victorious in the struggle; and it befits each state that would be great to-morrow as well as to-day to educate and organize itself, from the statesman at the top to the plough-boys and factory hands at the basis. In the future it will not be possible either to organize and lead a nation or to make cheese effectively without training‚Äìwithout a knowledge of what science has to say about men or milk.‚Äù (8-9)\n‚ÄúCaste and class may be exaggerated so much that they do far more harm than good, but to a certain extent they may serve for differentiating workers within the community. the nation stands equally in need of its ploughmen, its craftsmen, its traders, its brain-workers, and its leaders; and it is desirable to have some preliminary classification of what work an individual is best suited for.‚Äù (9)\nsimping for caste systems (9-10)\n\n‚ÄúIt is cruel to the individual, it serves no social purpose, to drag a man of only moderate intellectual power from the hand-working to the brain-working group; yet this seems too often the result of the present system. If there be a moderately capable worker, the state should strive, in the first place, that he should be trained to better craftsmanship. Do not let it assume that he will turn out a Faraday [(very successful scientist)] because he shows some relative capacity. In at least nine cases out of ten disappointment will be in store for the state if it does. Let there be a ladder from class to class, and occupation to occupation, but let it not be a very easy ladder to climb; great ability will get up it, and that is all that is socially advantageous. We have to remember, for example, that the middle class in England, which stands there for intellectual culture and brain-work, is the product of generations of selection from other classes and of in-marriage. A hundred men of this class, quite apart from training and tradition, will provide a greater percentage of men capable of doing brain-work, than a hundred men from the farming class, or a hundred craftsmen.‚Äù (10)\n(11) - current class system is not ‚Äúa mere historical anomaly; it is largely the result of long-continued selection, economically differentiating the community into classes roughly fitted to certain types of work.‚Äù\n\neducation must be specified for each class of workers\n\n‚ÄúMake it easy for the Michael Faradays to climb, but only for such as he was; the increase of the intellectual proletariat is a sign not of effeiciency but chaos in national education.‚Äù (12)\n\n‚ÄúThe statesmen of the old school, blamelessly ignorant of the laws of national development, were inclined to look upon race-progress as due to mighty forces beyond human control, and thus to believe that executive and legislature could do little to make or mar national welfare. But as we learn to understand better the laws ruling living organisms, our appreciation of the factors in human history changes: man cannot modify the law of gravitation, but he can make its effects subserve his own ends; and this is equally true of the laws which rule organic and inorganic material.‚Äù\n\n‚Äúwithout proper selection or fit training the statesmen of the oligarchy may forget inter-racial competition under the bias of class interest, and the intra-racial dominance of a caste will become the chief objective of a false statecraft.‚Äù (14)\n\n‚ÄúHistorical evolution has left most civilized nations, after a rough and tumble experience, with a democratic government more or less tempered by oligarchic and autocratic institutions. This may be the best practical solution of the problem in the present stage of national development, but such a system is terribly cumbersome in its processes for ensuring that the keenest brains and the best organizing power of individuals shall be secured as the brains and the organizing power of the nation at large. If the best trained, the most intelligent community is destined to be the surviving type of the present century, then the cry must not only be: Educate your democracy! but also: Select and train your aristocracy for statecraft!‚Äù (15)\n‚Äúthere is no small doubt that we safely may assume that all qualities in man are inherited, and inherited at such a rate that very few‚Äìtwo to four‚Äìgenerations suffice for selection to produce a class breeding true to itself, then the selection of an aristocracy even by the rough process of ennobling great ability or great wealth (acquired by the owner) is intelligible.‚Äù\nneed to train statesmen (16, again!). What kinds of statesmen, you ask?\n\n‚ÄúThere should be one school at least where colonial institutions, ambitions, and developments are studied and appreciated; where national customs, racial prejudices, the foreign press, its powers and limitations, are calmly, and apart from political intrigue, investigated and weight in the balance; where the students‚Äô own nation, its comparative power and influence, its morale, and its policy are dealt with in an atmosphere comparatively free from party strife, and at an age when the mental judgement has not had its roadway worn into ruts by the continual traffic of men and affairs.‚Äù (17)\n‚ÄúUnless we have the statesman of insight, who recognizes that every function of the state, every phase of national life, has a theory of its own; that there is a right way and a wrong way of conducting all state business, whether it be concerned with the wealth, the physique, the intellectual efficiency, or the morale of a nation;‚Äìwe cannot place knowledge‚Äìscience in its broadest and truest sense‚Äìin its rightful position of consultant alongside executive. We must have stored knowledge, science theoretical or empirical, at the service of the state for the ordinary routine of every department of national activity.‚Äù (18)\n\n‚ÄúReadiness for pioneer-work is one of the best tests for efficiency in the modern state. The mineral wealth, the climate, the agricultural resources of a new territory are to be reported upon with a view to its incorporation or development: the men to do this effectively must be ready trained and at hand. A troublesome native tribe is to be tutored by the touch of the masterhand: the man who can guide them with experience, with knowlege of their language, of their religion and customs, cannot be reared‚Äìhe must be forthcoming on the spot.‚Äù (19)\n‚ÄúNo nation can nowadays risk being a single step behindhand in its offensive or defensive services, in its methods of production, of its trade or of transit, or in the general education of its citizens,‚Äìtheir craftsmanship and their ingenuity,‚Äìor, again, in their average physique and reproductive power‚Äù (20)\nThe shift to the importance of intelligence is explained as a change caused by the machine: ‚Äúto-day it is the machine that does the work, and not the man; the important things are the brain which organizes and the intelligence which creates and guides the machine.‚Äù (20)\n\neducation:\n\n‚ÄúSooner or later the primary school must fall absolutely into the hands of the state, and, free from direct local control, be managed by a single council of education and a minster responsible to the national assembly. Every other system is merely tinkering at best; there are not sufficient real educational experts in the country to provide the capacity which is needful on innumerable school boards, to say nothing of parish committees and district councils. Local vigilance committees may well be organizes to see that the national system is effectively carried out locally, but local bodies are not in the intellectual position to draft an efficient educational system; nor, if they could do so, are they able either to put it into practice economically, or to avoid the friction of local sectarian feeling.‚Äù (21-22)\nschools as surveying institutions: ‚ÄúBut state control of primary schools is not only essential from this aspect, but also from the importance which must be attached to the nation having a complete and uniform record of the physical condition of its children. Is the stamina of the nation being not only maintained but strengthened?‚Äù (22)\n\n‚Äúa systematic anthropometric record of the schools would tell us whether our children progress or not from generation to generation, and what is the nature of the special precautions, if any, to be taken with regard not only to individuals but to whole localities.‚Äù (23)\n‚Äúthe sorting and sifting of population, the creation of a local sub-race, suitable to a developing local industry, is by no means so rapid as it ought to be. An effective record, made on a common system, of the physique and intelligence of the children of the nation would immensely assist the quest ofr suitable types of manual labor or of speical intelligence.‚Äù (23)\n\nschools do too many sports, which takes time away from intelligence acquisition, but sports are also good in other ways (24)\n\nif you‚Äôre going to let the kids do sports, at least measure it so that you can use it in your system of bio-political surveillance (24)\nbut sports are good actually because they‚Äôre correlated with good health\n‚Äúclearly games and aptitude for games ought to be encouraged in primary school.‚Äù (26)\n\nprimary school should focus on motor skills needed for craftsmanship and ‚Äúinquiry into things observed expanding as time goes on into a conception of the methods of science‚Äù (27) facts are secondary\n\n‚ÄúLet the child very gradually become conscious of the fact that man is fittest not as individual, but as society.‚Äù (28)\n‚ÄúThe state, as unsectarian, has first to inculcate the social duties: to emphasize the need of developing the physique, the intelligence, and the spirit of co-operative action as essentials of true patriotism.‚Äù (29)\n\nsecondary school: we need specialization (segregation ‚Äúaccording to the nature of the work they are to undertake in life‚Äù)\n\nthere should be ‚Äúcraft-schools‚Äù all across the country, ‚Äúsubject to a much greater local influence than the primary schools‚Äù - craft schools should not be taught by intellectuals or academics, but by practitioners. All that is needed is the communication of the method of labor. rural craft schools:\n\nmust ‚Äúlay the basis (1) of good craftsmanship and (2) of good citizenship. Under the first heading no form of labor is to be considered beneath educational treatment [(empirical and theoretical elaboration)]‚Äù (30)\n‚ÄúEducation is in no case to leave the feeling that it is finer to follow one trade than another, but is to develop the consciousness that it is a disgrace to follow any craft without intelligent appreciation of the why of its processes.‚Äù\ncraft schools should do apprenticeships (the students should be working) - ‚Äúthe secondary craft-school must inspire its pupils with a desire to know the reason for the rote which apprenticeship is sure to thrust upon them.‚Äù\n\nurban craft schools\n\n‚Äúthese places and their courses are largely chaotic at present. They have not settled whether it is their function to give secondary craft-education to boys and girls, to give higher craft-education to the non-commissioned officers of industry, to train the commissioned officers themselves‚Äìthe proper work of higher technical colleges.‚Äìor to provide cheaply a one-sided and, in nine cases out of ten, inferior academic education for young men and women who believe their success and standing in life will be assured if they are hall-marked with a university degree.‚Äù (34 - honestly go off)\n\nThere should be athleticsc in secondary school (37) ‚Äúbut a certain portion of the time devoted to athletic exercises should now be applied to developing qualities which may hereafter be of service for national offence or defence.‚Äù\nschools that do academic training should be separate from those that do technical training, even at the level of staff (teachers at the technical schools should have gone to them).\n\nanother reiteration of segregation: ‚ÄúThe secondary craft-school, the higher craft-school, the technical college, and the university serve quite different functions, educate for different carers and occupations in life; if economy or convenience bring any two under one roof, then there should be a differentiation of teachers; if even this is not possible, there should at the very least be a differentiation of material and of plan of instruction.‚Äù (35)\n‚ÄúHence it should be a sine qu√¢ non of every craft-school, whether secondary or higher, that each pupil should study one brach of pure science, or one literature, or one historical period, apart from his technical studies, as a field for rational enjoyment in adult life.‚Äù (36)\n‚ÄúIn the case of girls the horizon must appear somewhat narrower, and it is, perhaps, only their teachers and elders who can realize the national importance of those forms of physical training which may aid them to be the healthy mothers of a strong race. Still it is highly important that they should realize they belong to a larger whole: that they have a function in the state as well as a relation to individuals. Bandaging, first-aid, the elements of nursing, the car of infants‚Äìand the aged‚Äìmay all be taught as extensions of household economy, and the social value of such work inculcated‚Äù (38)\nvery strong desire to model the system off of the german education system (42-3)\nthe goal of secondary education ‚Äúwill not be to give the lad information useful to him in his future calling, but to develop his intelligence by the application of scientific method and processes with which he will later be concerned.‚Äù\n‚ÄúAn important addition, however, should be made to teaching of such modern secondary schools, not as part of the recreative but as part of the bread studies,‚Äì a reading knowledge of one, and a speaking knowledge of a second language should be insisted upon.‚Äù (44)\n\n‚ÄúIn any specialize branch of science there are rarely at any given epoch more than two or three master-minds, and these are diverse in country and in tongue. To follow these personally or in written word is an impossibility without linguistic knowledge, and science-abstracts and text-books are a deadening and nigh worthless substitute for direct contact with a master-mind.‚Äù (44)\n\nrunning this modern system in tandem with the old system might be a good way to further differentiate individuals in society (47)\nmuch more elaboration about the need for specialization (and the recreative and also religious studies) (48-49)\n‚ÄúWe must base national education on the need for national reaction against a changing environment; we must consciously prepare for the struggle, and by an intelligent study of human evolution arouse the patriotism and race pride of the young to assist directly in developing their intelligence for national ends.‚Äù (49 - would be good to use for a summary)\ntechincal schools and polytechnics should be organized in the most efficient and non-overlapping way possible (51)\neveryone should get a degree if they want one, even the lower class savages: ‚ÄúWe send peripatetic teachers out to fulfil the all-important function of raising the general culture of the people: we fancy it academic extension, and demand that it shall lead to a university degree. Nay, a degree having come to be looked upon as a mark of caste or gentility, the branding-iron is, in the true democratic state, to be brought to every man‚Äôs chamber‚Äù (51-52)\n\nbut only for show: ‚ÄúNor can we bring science and learning in their highest expression to each student‚Äôs door. He must go out on his go out on his Wanderjahre in pursuit of the master-teacher, or of the school which has specialized in his chosen study. What is true of the university is equally true of the higher craft-school: the student must seek the specialized teacher and the specialized school, and not trust a local polytechnic to be an effective educational omnium gatherum.‚Äù (52)\n\npost secondary education: ‚ÄúWe now turn to the highest forms of education, which, whatever we may hope for in a distant future, can at present only be organized for the brain-workers of the community‚Äìfor its thinkers and leaders.‚Äù (53) - Once again having everything mingling together too much is the downfall of the system because it cannot produce sufficiently specialized teachers to instruct the students (who are there to become specialists)\n\nWe have started again on the wrong system‚Äìmultiplication of little centres, doing their individual best no doubt, but not what is best for the nation. Three or four technical universities would suffice for the whole nation, but we have established fifteen or twenty technical colleges, on the theory that knowledge, like milk, must be delivered at each man‚Äôs door. The result is that all the schools are, broadly speaking, doing the same elementary work, and there is no specialization. (56)\n\n\nthe university: ‚Äúpure science or pure scholarship, without regard to the needs of special industries in profession‚Äù (57)\n\n‚Äúas soon as it becomes a recognized principle that intelligence can be trained and developed by observation, and reasoning on observation applied to technical or professional subjects, much of the monopoly value of pure academic studies will disappear‚Äù (57)\nmostly ‚Äútraining for specialized careers, namely, for statesmen, scientists, historians, literary men, educators, and makers of all forms of knowledge,‚Äìin sort, for the intellectual leaders of the nation‚Äù (58)\n‚Äúacademic studies will become more intense and more definite in character. Above all, the research training will more and more supplant the examination training.‚Äù (58)\n‚Äúwe need a training in method, and not, in the first place, a mere knowledge of facts, nor even the laws under which these facts may be classified. It is so easy to provide facts and formulae, so difficult to give insight into method, that text-books, degree schedules, and examinations invariably turn to the former; and the latter, to be learn only from direct touch with the investigator or from the classical memoir of the master, is thrust ruthlessly aside.‚Äù (59)\n‚ÄúThe university of the future will bring its undergraduates, not into touch with an army of tutors and‚Äùcoaches,‚Äù nor with their impedimenta of examination schedules and text-books, but directly into the field, the library, the laboratory, where the material of knowledge is accumulated and classified, and into personal touch with the men who make it.‚Äù (60)\n\nthe technical college: technical colleges should become specialized technical universities (56), of which there will be only a small need. The others should be closed or converted to higher craft-schools.\n\n‚ÄúUse their staff or buildings, where possible, for special departments of the university, but recognize once and for all that under the stress of modern competition these are matters of national importance; and that to bring our technical intelligence up to the level of that of our neighbors, we do not want local engineering professors, or local colleges, but national technical universities, each with ten or more complete laboratories, a score of special technical professors, and with equipment and funds comparable only with those of the whole of pure-science faculty of a first-class modern university. Such universities would train not only the nation‚Äôs industrial leaders but the teachers for the secondary and higher craft-schools; and by bringing both classes into touch with actual knowledge-making, indicate on the one hand how the problems of practical life, on the other the problems of craft education, may be met and solved.‚Äù (57)\n\nthe professional schools:\n\nlaw school: comparative and historical knowledge is lacking (62), leading to low imagination in writing legislation and ‚Äútoo little sympathy in dealing with legal institutions of subject or assimilated races.‚Äù\nmedical school: ‚Äúit is impossible that the current system of support by fluctuating charity can permanently continue. Alongside the public charities have arisen infirmaries, fever hospitals, and asylums supported by public funds, and in many cases but little used for clinical instruction.‚Äù (62)\n\nneed for scientific method in medicine (65): ‚ÄúMuch of the strength of proof in medical science depends entirely on statistics; copious raw material can be obtained from hospital practice, but this is rather too largely drawn from special classes of the community. the bulk of data from all classes either escapes written record, or remained‚Äùunstandardized‚Äù in case-books; here it is monopolized by the individual as ‚Äúexperience,‚Äù when by co-operative action it might be statistically generalized into proof. The quantitative value of the correlation between environment, age, or physical characters and the special features or virulence of any disease is probably unknown at the present day in a single instance; and yet it is hard to conceive that clinical prognosis would not be greatly advanced, especially among the younger members of the profession, by a quantitative knowledge of this kind. An authoritative body standardizing records, collecting individual experience and reducing it by adequate statistical theory, seems almost a necessity for medical progress at the present day.‚Äù (66)\n\n\nthe commercial university: for commerce and business studies; these people will be doing fiscal policy and negotiating terrifs (67).\n\nthese guys seem to be really involved in the colonialism part of this project; the instructors are to take one out of every 3 years travelling (67), as in the Russian system\n‚ÄúTo the commercial university a relatively considerable number of studentships should be attached, the holders of which should be compelled to travel and report on foreign and colonial commercial methods and possibilities. These reports should in the first place be looked upon as exercises, but selected reports might well deserve publication as monographs and commercial research. Past holders of such scholarships, the pick of academic training; with their minds freed from insular method and local custom by the insight of travel, would undoubtedly be in constant demand for pioneer work.‚Äù (69)\n‚Äúthey could yearly send out men to study the flora and fauna of almost untouched districts; to learn the native languages, religions, and customary laws of British and other possessions; to study under the masters of pure science, history, or philosophy who exist outside their own walls; and to return, as the American travelling fellows have done from their European universities, to develop their home institutions and widen their educational systems by leaps and bounds.‚Äù (70)\n\n\ngovernment schools: ‚Äúit is very desirable that the government schools should be limited to those branches of instruction which are needed only for the national service. For example, schools of offence and defence‚Äìnaval colleges, staff colleges, artillery and military engineering schools; to these ought probably to be added, schools for home and imperial civil service‚Äìfor consuls, native state residents, and the lower branches of the diplomatic service‚Äù (71)\n\ngospel of specialization again\nefficiency - only put government schools where they will not overlap with adequate offerings from another educational body (73)\n\n‚ÄúOne of the greatest dangers of science, and especially science in the consultative science of the state, is the possible creation of a scientific hierarchy, resting on past achievement and believing itself at the summit of scientific knowledge. As soon as man ceases to research, he has fallen behindhand; his tools grow rusty, and he ceases to grasp new methods and possibilities. Hence one of the greatest problems of the state is how to draw into its service not only those who have achieve as consultants, but those who are achieving as discoverers.\n\nprizes! (with monetary rewards) ‚Äúsuch prizes and research should be independent of national laboratories and government consultants, to whom more specialized problems and routine difficulties should be submitted for solution or advice.‚Äù (76)\n‚ÄúHere, as in other fields, a differentiation of pure and applied sciences is necessary.‚Äù (87)\n\nmany many more examples of technical schools, which I suppose the government is going to sponsor or run (? not super clear about those details)\n\nabout astronomy, geography, geology, and meteorology departments: ‚ÄúThe link between the central home observatories and those in the colonies and dependencies has hardly been strong enough, nor the whole chain of institutes systematized; and this is particularly the case in the meteorological service‚Äù (81)\n\n\nMedical and Sanitary Institutes\n\n‚ÄúHere again therea re innumerable questions to which municipalities, or home and colonial governments, need answers, and definite and prompt answers‚Äù (83)\nthe institutes of medicine and sanitation should preempt the public call for health and sanitation services because by that point it will be too late (see covid)\n\nnational institute for anthropology\n\n‚ÄúWith possibly more races under the British flag than under any other imperial symbol since the days of the Roman eagle, we have yet entirely failed to systematize and nationalize our study of those races. There is national museum or institute where one may learn the cranial, anthropometric, and physical characters of the various races under our sway, still less something of their languages, folk-customs, industries, and religions. An institute carrying out a complete anthropological survey of the empire is as necessary from the imperial standpoint as those dealing with geographical or geological surveys. The Americans have recognized this, and their anthropological reports and museums under state supervision will soon be a model for such work everywhere.‚Äù (84)\n\nbotany and zoology - zoology apparently behind that of the Americans (87)\nthe biological farm (87) - a special farm that serves doubly as a biology laboratory\n\n‚Äúsuch statistical and secular experiments cannot be stisfactorily undertaken in existing biological laboratories: they need a considerable range and variety of lang, water, wood, and open field, free from intrusion and under the management of skilled keepers; there must be shed for the breeding of, at any rate, the smaller mammals, houses for insects and birds, and ample space for all kinds of other extensive experiments on heredity and variation. There must be, in addition, workroom, microscope rooms, dissecting laboratories, and instrument departments for the use of measures, computers, and researchers. Such a biological farm could in the present state of affairs do epoch-making work within the space of a very few years.‚Äù (88)\n‚ÄúIt ought to be perfectly possible in a few years to determine, to the satisfaction of all parties concerned, the limits of truth in the laws propounded by Galton or Mendel, or any other; and only by such unbiased experiments, not by controversial publications, can the actual facts be reached. It is admitted on all sides that we stand here, in the matter of variation and heredity, on the eve of wide-reaching discoveries, and only an institute such as we have sketched can really conduct the extensive and secular experiments which are needed for truly authoritative answers‚Äù (88-9)\n\n\n‚ÄúThe crowning study of man is man; the highest science is that which deals with human races, and sees the causes which lead to their progression and relative dominance. This science, applied to national life, is statecraft,‚Äìthe art of seeing what makes for national health and for national fitness. Every nation, however, is an agglomeration of classes and castes, of the mentally and physically healthy, and of the mentally and physically unsound. Man, if the highest of living forms, is still one of them; and it is easy to test whether the general laws of heredity and relative fertility provided for the lower forms hold, with or without modification, for him. Problems as to the reproductive dominance of better stocks, as to effective national fertility, as to progressive physical and mental development in man, will be vital problems for the statesmen of the near future; their solution is closely bound up with a knowledge of the laws of heredity, fertility, and variation in other living forms, which only such an institute as we have just described can effectively study.‚Äù (89-90)\n\na call for science, particularly specialized journals, to be funded more, and for more specialized journals to be created\nThe state should have science counselors and consultants (95)\n\nall about fitting the right man to the right role (96)\n\n\nWe have by one of other process to learn the national importance of science: to realize that science in the broadest sense, as an educator and discoverer, is the mainspring of modern national life; that the future is to the scientifically trained nation which reproduces itself, maintains its health, develops its institutions, controls its production, organizes its distribution, extends its territory, governs its subject races, and prepares its offensive and defensive services with scientific foresight and insight‚Äì\n\nIn the reproof of chance\nLies the true proof of men‚Äî\n\nand, we may add, the true proof of nations (97)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#letters-to-the-editor",
    "href": "06_eugenics.html#letters-to-the-editor",
    "title": "6¬† Regression and Power",
    "section": "6.5 Letters to the Editor",
    "text": "6.5 Letters to the Editor\n‚ÄúCuttings Containing Letters to Editor from Karl Pearson‚Äù (1905)\n\nPearson: ‚ÄúI feel very strongly that this matter is of far greater public importance than any personal controversy between Dr.¬†Donkin and myself. A knowledge of the heredity or non-heredity of character of the tendency to commit anti-social acts must be antecedent to any profitable scheme of criminal reform.‚Äù\n‚ÄúWe have at moment a new and active Home Secretary. May I suggest that one of the most valuable additions that could possible be made to the Prison Department at the present time would be the appointment of a medical man with one or two assistants, whose special occupation should be tracing the family history (chiefly from police records) and environmental conditions in early life of convicted criminals? We should soon have sufficient material on which a definite judgement might be based as to whether crime or the tendency to law-breaking is or is not hereditary.‚Äù\nelder born children are better and live longer somehow (and they may have more ability)\n‚ÄúThe inheritance of ability is so marked that there is every reason to suppose that a man who has won his way to pure ability to the House of Lords will, if he has mated wisely, have children above average in ability. Unfortunately, the House of Lords has too often been recruited by mere plutocrats, by political failures, or by men who have not taken the pains necessary to found or preserve an able stock.‚Äù\nGalton seeks to establish that the natural ability in England springs forth from just a few great families by analyzing 200 survey responses from the Royal Society. In the analysis, he shows some attention to factors other than mere inheritance (i.e., also paying heed to nurture)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#practical-eugenics",
    "href": "06_eugenics.html#practical-eugenics",
    "title": "6¬† Regression and Power",
    "section": "6.6 Practical Eugenics",
    "text": "6.6 Practical Eugenics\nPearson (1909) - a laypersons introduction to the science of Eugenics\n\n‚ÄúWhen we apply this method to various characteristics in man, we find that the degree of resemblance between parent and offspring lies between .4 and .5; betweek offspring and grandparent between .2 and .3; and between any grade of ancestor and the offspring the resemblance diminished in geometrical progression, the factor of reduction lying between .5 and .6‚Äù (4)\n\nthis is a natural law present in humans as in ‚Äúhorses, dogs, and cattle‚Äù\nyou cannot explain albinism, as Pearson documented it, using Mendelian inheritance\nthe characteristics explained by Mendelian genetics are not complex or socially meaningful enough to be useful in a regime of population control, even for the comparatively minor task of selectively breeding some traits into horses\n‚ÄúYou cannot profitably group men into tall and short, into pure blue eyed and non-blue eyed, into albinos and all the rest. The finer grading and statistical observation of how much quantitative differences in the parents or ancestry influence the grade of the off-spring are essential to our judgement of hereditary influence in man.‚Äù (5) hm\nDoesn‚Äôt this proposed natural law correspond almost exactly to the predictions made by Mendelian genetics? NO! it wouldn‚Äôt because of the dominant/recessive thing?\n\n‚ÄúGood home environment was shown to have practically no influence on the intelligence of boys, on girls it was represented by a correlation of .07, hardly 1/7 the intensity of heredity. The relationship between eyesight and home condition was practically insensible, and the effects of employment of mother on the physique of the children, or of the drinking of the parents on the intelligence of the children were practically of no importance compared with the fundamental factor of heredity.‚Äù (6)\n\n‚ÄúHave not the numbers given in the past lectures taught us then a first fundamental principle of practical Eugenics? It is five to ten times as advantageous to improve the condition of the race through parentage as through change of environment.‚Äù (6)\noutlawing child labor is bad actually - because evolution\n\n‚ÄúFormerly a child became at an early age a pecuniary asset. It contributed to the family maintenance by six or eight years of age, and by the number of children the economic prosperity of the home was in a certain sense measured. That a child should be looked upon as a ‚Äòpecuniary asset‚Äô shocks many of us, as it shocked Lord Shaftsbury. But from inquiries I have made, the condition of the child as a ‚Äòpecuniary asset‚Äô was not wholly a bad one; it must be kept in health, because it ceased to have any pecuniary value if it broke down. A Bradford doctor assured me that in the days before the factory acts more care was taken of the children on this very account. No Bradford woman of those days would have replied as to the number of her children:‚ÄùSixteen, but thank the Lord, thirteen of them are in the churchyard.‚Äù The effect of lead-poisoning and the professional abortionist were then practically unknown in the manufacturing centres.\n‚ÄúThe mistake of most legislation is that it is carried by appeal to the sentiment and feelings of relatively small classes‚Äìthe cultured and highly sensitive upper and middle class. The biological and economic bases of life are disregarded, and the result is only manifest some twenty or thirty years later.‚Äù (7-8)\n\n\nanother piece of factory legislation: restrictions on pregnant mothers working: ‚ÄúWe picture the child and the mother toiling in the factory, and we, judging the matter from our own feelings and cultured sentiment, shudder and‚Äìturn them out. We never regard the matter from the economic standpoint, and do not realise that in our well meant action we have taken a great step towards the abolition of both children and motherhood.‚Äù (8)\nallegedly progressive factory legislation removes the possibility of the child being a pecuniary asset until it has reached working age (which was inflated up to 13-15 years)\n\nBy force of economy, this discouraged motherhood\n\nbirthrate computation (10)\n\nthe birthrate is falling (plates 1 and 2, p.¬†13 and 15). why?\n\nis the change in the distribution of the ages of mothers causing the drop in births (this is an effect of mothers living longer, on average, due to the medical innovations)?\n\n‚ÄúI believe this to be largely the source of the fall in birthrate in our Colonies, for only the active younger women emigrated in the early days. Now that emigration is not the chief source of the population, there are many more elderly women, and the birthrate has naturally fallen.‚Äù (10)\nPlates 1 and 2 are the fruits of the endeavor to answer this question. They show (in Cornwall and Bradford) that the birth rate curve for just married women 15-55 has the same slope as the curve for all married women (‚Äúall possibly reproductive wives‚Äù, 11), only it is higher (to account for the fewer births among older women)\n\nwhen did the fall in birth-rates start? Now we are going to look for spurious correlations between factory legislation and birthrates, if I had to guess\n\n‚Äúslight effect in 1867, marked effect in 1877, very marked effect in 1887 and accelerative effect in or about 1892.\nNorfolk, rural district: ‚ÄúWives become more numerous, but mothers fewer‚Äù (12) in like 1887\nNorth Riding of Yorkshire & Middleborough, rural district?: the birthrate decreases more substantially since 1887 when you exclude the part of town where there a lot of young women moved and the birth rate drastically increased.\nYork (plate 2), town with county and trading occupations but ‚Äúno strong manufacturing interest‚Äù (13): reduction since 1887\nmanchester, manufacturing town with other trading interests: decrease\nBradford (plate 2), manufacturing town: there was an extraordinary decrease starting in 1877, and now ‚Äúthe population of Bradford without immigration would hardly be maintaining itself at the present rate‚Äù (13) given an assumption of 30 in 100 infant mortality\nHuddersfield (plate 2), manufacturing town: marked decrease started in 1877, as with Bradford\nBolton (plate 3), textiles manufacturing town: decrease began in 1877 and accelerated in 1892\nLeeds (plate 3), engineering and textile centre: decrease begins in 1877 and accelerates from 1892 onwards\n‚ÄúNow I think it impossible to study such curves as I have put before you and not appreciate the national gravity of the situation. The English population has not yet reached, but is in a fair way to reach in the course of the next fifteen years, the condition of France in which it will not reproduce itself and will depend for maintenance on immigration‚Äù (14)\n\nnot warning the brits they could wind up like the French, who are among the neighbors we are preparing for in war via our education in Pearson (1919) (? right citation)\n\n\ntrends in vital statistics (birthrate) lead to the following questions which seek to clarify the cause via a series of spurious correlations (I think this is it!!):\n\nwhat happened in 1867 that may have explained the decrease in Huddersfield?\n\nWorkshop Regulation Act of 1867: ‚ÄúBy this act no child under eight was to be employed in any handicraft, children from 8 to 13 only to be employed half-time. I believe this to be actually the most important step taken up to that date to destroy the economic value of the child.‚Äù (15) - even though the effect was only in one town in 1867? maybe it took more than a decade to arrive elsewhere?\n\nWhat happened in 1877 that affected the textile and engineering tows?\n\n1874: factory law raises minimum working age to 10\n1878: being super fucking dramatic: ‚ÄúThis act of 1878 was extremely complex and calculated on this very ground to discourage the employment of children. Children defined to be persons under 14 were to be employed for half time only‚Äìin morning or afternoon sets or on alternate days. A child must not be employed for two successive periods of seven days in the same set, whether morning or afternoon, nor on two successive Saturdays, nor on Saturday in any week, if he has already in that week been employed on one day more than five and a half hours. Nor shall a child be employed fully on two successive days, nor on the same day in two successive weeks. Employment of the children at home when work is the same as in the factory or workshop was also regulated. No child under ten was to be employed, and medical certificates were required in the case of all children and young persons under 16‚Äù (15)\n\nmuch more succinct: ‚Äúmost effectively tended to destroy the child as an ‚Äòeconomic asset.‚Äô Children under ten could not be employed at all, children under 14 could only be employed half time and this in a complicated way. All persons under 16 required a medical certificate‚Äù (16)\n\n\nwhat happened in 1887 which had a general effect across towns, including ‚Äúcertain non-urban districts and trading towns‚Äù?\n\nMines Act in 1887: prohibited employment of boys under age 12 underground, and the employment of boys and girls above ground in mining/mineral preparation work. ‚ÄúThis act directly touched interests in Cornwall and North Yorkshire‚Äù (16)\n1891: age of employment raised again, and employment of women immediately after childbirth (16)\n1899: ‚Äúthe Education Act made it unlawful to emply any child under 12 in such a manner as to prevent full attendance at school‚Äù\n‚Äú, and in 1901 the prohibition of the employment of any child under 12 in factory or workshop was made direct and absolute‚Äù (16)\n\nuniversal education reforms were also important in minimizing the economic value of the child\n‚ÄúIf, as I believe, our present precarious condition with regard to the birthrate is a direct effect of the destruction by legislation of the economic value of the child, surely a great lesson may be drawn for practical eugenics? Does it not demonstrate that whatever law affects the economic status of a portion of a community must also be dealt with from its biological aspects?‚Äù (!8)\n\n\nother effects of a lower birthrate\n\nfirst and second born children are ‚Äúof a more nervous and less stable constitution. We find the neurotic, the insane, the tuberculous, and the albinotic are frequent among the elder-born. Dr Goring‚Äôs results for criminality show the same law. The diagram (see plate 4) I put before you will bring this outl; you see in the tuberculous, the insane and criminal stocks that the first few members are weighted.‚Äù (19)\n‚ÄúThe result of this law is remarkable. It means that if you reduce the size of the family you will tend to decrease the relative proportion of the mentally and physically sound in the community.‚Äù (19)\n\n\nNow it seems to me that we have an illustration in this matter of a case‚Äìand it is not an isolated case‚Äìin which legislation intended to promote national progress‚Äìto improve the radical qualities of future generations‚Äìhas directly tended to enfeeble the race. (19)\n\n\nam I advocating that we should repeal the factory laws? ‚ÄúAssuredly not. I wish, however to emphasize two practical points. The first is, legislation intended to increase racial fitness may end by penalising parentage and motherhood. The second is, that the economic value of the child will in the long run govern its production. All legislation which places parents in an economically worse position than the unmarried is radically unsound.‚Äù (20)\n\nyou can solve the problem of practical eugenics by comodifying children and then regulating them like a comodity\n\n‚ÄúThe child is economically a commodity and like any other ware is produced to meet the demand; for the great bulk of the population whose wages extend but little beyond subsistence, the child will be produced or not according as it has economic value. If we can give the child economic value, the birthrate will rise; if we can differentiate between good and bad parentage, if we can make the possession of healthy, sound children a greater economic asset than the possession of feeble offspring, then we have for the mass of people solved the problem of practical eugenics‚Äù (21)\ndid he know that this was immoral at the time. Yes! ‚ÄúI am very fully aware that this fundamental principle that the child is a ware and, in a community which has learnt how to restrict its birth rate, will be produced in proportion to its economic value, will not be a popular doctrine.‚Äù (21)\n\nlegitimately subversive take: of course the rich sentimental people won‚Äôt understand this, ‚Äúwith these classes the child has never been an economic asset; it is a luxury which we know we must pay for, and expect to pay for, until after college and professional training, and, in the case of unmarried daughters, often long after our own lives are concluded‚Äù (21)\nthis upper class logic has no business being extended to lower classes to which it does not apply\n‚ÄúWe can, in the case of these cultured classes, urge great social principles, and ultimately create social sanctions, for the parentage of the fitter and the sterility of the unfitter stocks. This is a moral crusade, and I believe it will be successful, however many are the prejudices and difficulties it will have to encounter.‚Äù (21-2)\n\n\ntwo fundamental problems for practical eugenics:\n\nproducing ‚Äúa sufficient supply of leaders of ability and energy for the community‚Äù\n\nyou can use ‚Äúmoral teaching backed by social sanction‚Äù for these folks, but not for the workers who are living on subsistence wages\n\nproviding ‚Äúintelligent and healthy men and women for the great army of workers‚Äù\n\nthese people follow the rules of the capitalist market\n‚ÄúThere is, I believe, one way, and one way only, of solving this problem: we must reverse the effect of the factory acts which have penalized parentage and handicapped motherhood. Both the reversal must be done in a differential manner, sound parentage and healthy motherhood must be given a substantial economic advantage over not only childlessness, but over unsound parentage and feeble motherhood; the well-born child must again be made a valuable economic asset. This is the central problem of all practical eugenics,‚Äìeugenics as a doctrine of national welfare is a branch of national economy.‚Äù (23)\nwhere is the moral panic? ‚ÄúBefore touching possible directions of reform, I want to point out to you that while a penalisation of parentage is bad,‚Äìfor, given the material, Nature, the first and most thorough practical eugenicist, will play her part in selection‚Äìyet our special penalisation is excessively bad; for it has, owing to municipal and charitable institutions, emphasizes the penalty in the case of the better type of parent.‚Äù (23)\n‚ÄúThe thrifty, provident parents who wish to provide a home life for their offspring not only find themselves penalised as against their childless competitors,but as against the thriftless and improvident who throw the burden of their children on public rates and on private charities. I want to bring this out emphaticaly because it seems to me an essential part of practical eugenic policy to protect and fight against this municipal and charitable method of penalising better parentage.‚Äù (25)\nTable 1 shows that the birthrate for various ‚Äúpathological‚Äù type folks is higher than for ‚Äúnormal‚Äù folks in most cases (and in some cases much higher)\n\nbecause these people are provided for largely by the government, their children do not come with the same economic disadvantage that the normal people are subject to (those being the normal workers who are subject to the child as commodity system; the intellectual elite have the lowest birthrates and are subject to different calculus).\nTable 2 gives a lot of scary-looking correlations\n\n‚ÄúThis table shows in every case (cancer is the only exception we know) a positive correlation between an undesirable social feature and a high birthrate, and a negative correlation between a mark of well-to-do population and the birthrate.‚Äù (26)\n\nthe correlations are getting worse (shown in table 3) - I don‚Äôt get them\n\n\n\n\nAll separate lines of inquiry tend to confirm the view that the districts of a good social character have the lowest birthrates; that the anti-social stocks are at present most prolific, and this whether we measure the gross or net fertility.\n\n\n‚Äúthe child ceasing to be an economic asset, has become a burden, but poor law and charity have largely succeeded in lifting this burden from the shoulders of the degenerate parents. We have not only hindered Nature from weeding out social wastage, but we have made the conditions increasingly more favourable to the multiplication of this degeneracy. Practical eugenists must urgently demand the reversal of all legislation which penalises the parentage of the fit, and the restriction of all charity which favors the parentage of the unfit. we must directly or indirectly produce differential wage for the fit parent; in other words there must be endowment of fit parentage at the expense of the unfit parent and of childless men and women.‚Äù (29)\nhow to do it?\n\ndifferential taxation (Lloyd George‚Äôs proposal will be ineffective - georgism, if I recall?): ‚ÄúIt will be the fault of eugenic workers if the thin end of the wedge thus inserted be not driven home. Taxation must differentiate between the parent and the non-parent in income-tax, settled estate duty and death duties.‚Äù (29)\nother economic means (30), modeled after Germany and the Indian Civil Service: ‚ÄúAt first a very rough standard of differentiation would suffice‚Äìa fairly clean bill of health for both parents, and absence of obvious taint in their immediate stock, a moderate school standard passed, and a minimum wage value in the market to test general ability. Even without this slight test‚Äìwhich at any rate would exclude the epileptic, the deformed, the insane, and the deaf-mute stocks from the benefits of the scheme‚Äìwe should by a simple insurance fund of this kind have removed the present disabilities of parentage which, as I have endeavoured to show, are practically differential with regard to the fitter parentage.‚Äù (31)\n‚ÄúWe see enormous sums annually given for charitable purposes without the least attempt to differentiate between the recipients who spring from fit and those who spring from unfit parentages, between the recipients who are of racial value and those who are mere social wastage. Asylums abound for the imbecile and the cripple, homes for the waifs and strays, orphanages, hospitals, the boast of which is that they receive without selection all sufferers. Do the subsrcribers to these and many other kindred institutions ever consider that they are directly penalising fit parentage by enabling the unfit parent to obtain provision for his deformed of diseased offspring?‚Äù (31-32)\n\n‚ÄúIs it not possible by aid of a little educative propagandism of a eugenic character to divert some of the thousands we see every week willed to indiscriminate charity in a more rational and national channel? Why should they not be ear-marked in some small percentage of cases for the offspring of fit parentage? Cecil Rhodes with the insight of a strong man determined that Rhodes scholars should be selected for ability, physique, and character combined. Such a combination will rarely be found without fit parentage, and probably the best means of securing it would be a study of ancestry.‚Äù (32)\n\n‚ÄúThe whole system of secondary school and university scholarships provided by the educational committees of the County Councils wants at present stable basis. The candidates too often lack the physique and character, without which mere examination ability is worthless. Here again is a wide field for eugenic effort, for the indirect endowment of the fitter parentage.‚Äù (33)\n‚ÄúI am very fully conscious that there are many other direction than those I have advocated to-day wherein the eugenist can work towards racial improvement. But I have chosen the two points‚Äìfactory legislation and modern charity‚Äìbecause I believe they are the sources of our gravest present difficulties. Both of them mark the extreme limit of philanthropic effort‚Äìthe attempt to improve the racial fitness of the nation by purely environmental reforms, the removal of the child and mother from unhealthy surroundings, and the provision for the weak and the suffering. Both have failed in promoting racial efficiency, because they overlooked great all-mastering biological laws. after 60 years of philanthropic effort unparalleled in any European country, we find ourselves as a race confronted with race suicide; we watch with concern the loss of our formal racial stability and national stamina.\n\n‚Äúan artificial birthrate has been created in the fitter classes, which may become habitual, and if so spells ultimate racial destruction.‚Äù (35)\n‚ÄúThis view of human society which has been given in this lecture, will I fear prove unpopular‚Äìthat is not my mind in an argument against its truth. I would not ask you to accept it without much criticism, and without viewing it from every possible side. To some of you who do this it will become a real possession, which will unify your conceptions of our present difficulties as to the apparent incompatibility of the highest forms of civilisation with continuous race progress. Why do we find degeneracy and race suicide arise as human sympathies and emotions are widened? I think the answer lies in the fact that environment appeals directly to our senses, but heredity only to our reasoning. We rush to modify the former, regardless of the laws of the latter. The releif of pain and suffering is so obvious a duty, the penalisation of suffering is so obvious a duty, the penalisation of parentage is so disguised and so distant in its effects. When we say: ‚ÄòYou must protect the child from unhealthy or cruel environment‚Äô the best of the nation is with us with vote and even with purse. When we say ‚ÄòYou must preserve the economic value of the child,‚Äô we evoke no sympathy; none see at once the whole tale of penalised parentage, lowered birthrate, cacogenic reproduction, race degeneracy and the ultimate race suicide involved in the breach of that principle.‚Äù (36)\n\n‚ÄúThe child as comodity whose supply is regulated by economic value may sound a harsh doctrine. But truth‚Äìwhether of natural selection or of social evolution‚Äìis not created by man; he has only to discover it, be palatable or bitter. Social stability depends upon the extent to which we allow even unpalatable truth to guide our legislation and our conduct. Eugenists have before them at present alternative paths, they can follow the easy course of appeal to popular feeling and untutored human emotion, in which case they will create, like philanthropic effort, immediate interest, have their day and their fashions, and leave no progressive impress on racial evolution. Or, they can take the harder road of first ascertaining the laws which regulate the human herd, of creating a science which shall dictate an ultimate eugenic art. In the latter case they will scarcely be popular for I feel sure their truths will be bitter and our generation likes above all things its medicine and mild and toothsome form.‚Äù (37-38)\n\n\nm√ºnecat (2024):\n\nvery long video essay about how bullshit evolutionary psychology is - does a very nice job imo; also very good (and funny) examples and presentation\nmodern evolutionary psychology, unlike eugenics, takes a more reasonable view of the timescale of evolution - at least thousands of years, but always heavily dependent and sometimes taking place on the scale of hundreds and thousands of years.\nmodern evo psych is also bullshit, but for a different reason. The psychologists, knowing nothing about what human culture a hundred thousand years ago, just superimpose their own (gendered) impression of the state of nature.\n\n\n\n\n\n\n\n‚ÄúCuttings Containing Letters to Editor from Karl Pearson.‚Äù 1905.\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nGALTON, FRANCIS. 1901. ‚ÄúBIOMETRY.‚Äù Biometrika 1 (1): 7‚Äì10. https://doi.org/10.1093/biomet/1.1.7.\n\n\n‚Äú(II.) The Spirit of Biometrika.‚Äù 1901. Biometrika 1 (1): 3‚Äì6. https://doi.org/10.1093/biomet/1.1.3.\n\n\nm√ºnecat, dir. 2024. I Debunked Evolutionary Psychology. https://www.youtube.com/watch?v=31e0RcImReY.\n\n\nPearson, Karl. 1909. The Problem of Practical Eugenics / by Karl Pearson. Eugenics Laboratory Lecture Series 5. London: Dulau and Co.\n\n\n‚Äî‚Äî‚Äî. 1919. The Function of Science in the Modern State / by Karl Pearson. 2nd ed. Eugenics Lecture Series 12. Cambridge: University Press.\n\n\n‚Äî‚Äî‚Äî. 1920. The Science of Man: Its Needs and Its Prospects / by Karl Pearson. Questions of the Day and of the Fray ; No. 10. London: Cambridge University Press.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#footnotes",
    "href": "06_eugenics.html#footnotes",
    "title": "6¬† Regression and Power",
    "section": "",
    "text": "The ‚ÄúRule of Three‚Äù is a method for solving linear systems of equations (i.e., it is a method for determining unknown values in a system of variables).‚Ü©Ô∏é\n‚ÄúIf you tell me that we are here trenching on the field of psychology and medicine, I reply: Certainly; you do not suppose that any form of investigation which deals with man‚Äìbody or mind‚Äìis to be omitted from the science of man? If you do you have failed to grasp why anthropology is the queen of the sciences. The University anthropological institute of the future will have attached to it a psychologist, a medical officer, and a biologist. They are essential portions of its requisite staff, but this is a very different matter from lopping off large and important branches of its fitting studies, to lie neglected on the ground, or to be dragged away, as dead wood, to be hewn and sharpen for other purposed by colleagues in other institutes. Remember that I am emphasising the size of anthropology which studies man in the service of the State‚Äìanthropology as a utile science‚Äìand that this is the only ground on which anthropology can appeal for support and sympathy from State, from municipality, and from private donors.‚Äù (Pearson 1920)‚Ü©Ô∏é",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "0R_references.html",
    "href": "0R_references.html",
    "title": "References",
    "section": "",
    "text": "Bakker, Arthur, and Koeno P. E. Gravemeijer. 2006. ‚ÄúAn\nHistorical Phenomenology of Mean and\nMedian.‚Äù Educational Studies in Mathematics\n62 (2): 149‚Äì68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. ‚ÄúA Brief History of\nS.‚Äù In Computational\nStatistics, edited by Peter Dirschedl and R√ºdiger\nOstermann, 81‚Äì110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. ‚ÄúCase\nStudy 1: AT&T\nDivestiture.‚Äù Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\n‚ÄúCuttings Containing Letters to Editor\nfrom Karl Pearson.‚Äù 1905.\n\n\nFoucault, Michel. 1978. The History of\nSexuality. Vol. 1. 3 vols. Random House.\n\n\nFoundation, Free Software. n.d. ‚ÄúWhat Is Free\nSoftware? - GNU Project - Free Software\nFoundation.‚Äù Accessed May 9, 2024. https://www.gnu.org/philosophy/free-sw.html.\n\n\nGALTON, FRANCIS. 1901. ‚ÄúBIOMETRY.‚Äù\nBiometrika 1 (1): 7‚Äì10. https://doi.org/10.1093/biomet/1.1.7.\n\n\nIhaka, Ross, and Robert Gentleman. 1996. ‚ÄúR: A\nLanguage for Data Analysis and\nGraphics.‚Äù Journal of Computational and\nGraphical Statistics 5 (3): 299‚Äì314. https://doi.org/10.2307/1390807.\n\n\n‚Äú(II.) The Spirit of\nBiometrika.‚Äù 1901. Biometrika 1 (1): 3‚Äì6.\nhttps://doi.org/10.1093/biomet/1.1.3.\n\n\nm√ºnecat, dir. 2024. I Debunked Evolutionary\nPsychology. https://www.youtube.com/watch?v=31e0RcImReY.\n\n\nPearson, Karl. 1909. The Problem of Practical Eugenics / by\nKarl Pearson. Eugenics Laboratory Lecture Series 5.\nLondon: Dulau and Co.\n\n\n‚Äî‚Äî‚Äî. 1919. The Function of Science in the Modern State / by\nKarl Pearson. 2nd ed. Eugenics Lecture Series 12.\nCambridge: University Press.\n\n\n‚Äî‚Äî‚Äî. 1920. The Science of Man: Its Needs and Its Prospects / by\nKarl Pearson. Questions of the Day and of the Fray ;\nNo. 10. London: Cambridge University Press.\n\n\nShustek, Leonard J. 2016. ‚ÄúProgramming the ENIAC:\nAn Example of Why Computer History Is\nHard.‚Äù May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. ‚ÄúThe Medicalization of\n‚ÄòHomosexuality‚Äô.‚Äù Honors Capstone\nProjects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTukey, John W. 1972. ‚ÄúData Analysis, Computation and\nMathematics.‚Äù Quarterly of Applied Mathematics 30 (1):\n51‚Äì65. https://doi.org/10.1090/qam/99740.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to\nthe Entscheidungsproblem.‚Äù Journal of Math\n58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023.\n‚ÄúData Visualization.‚Äù In R for Data\nScience, 2nd ed. https://r4ds.hadley.nz/data-visualize.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. ‚ÄúData\nVisualization.‚Äù In R for Data Science, 1st\ned. https://r4ds.had.co.nz/data-visualisation.html.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "0A_resources.html",
    "href": "0A_resources.html",
    "title": "Appendix A ‚Äî Getting Help",
    "section": "",
    "text": "The contents of this appendix are not here!\n\ngetting help in R (? and ??)\nthe two types of help pages, and the structure of each\n\ndataset pages\nfunction pages\n\nCheatsheets\ndocumentation online\n\nrmarkdown\nquarto:\n\nquarto guides\nquarto reference\n\nrstudio user guide\ntidymodels (for machine learning, mostly)\ngoogle and stack exchange\n\nThere is a general guide to getting R help, and it includes a suggestion I will forward: use Google and include the term ‚ÄúR‚Äù in the search",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Getting Help</span>"
    ]
  }
]